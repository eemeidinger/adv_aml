{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "        For all the classes and methods in this assignment you will use the PyTorch library. You should use a double precision data type and the device is either \"cpu\" or \"cuda\".\n",
        "\n",
        "\n",
        "\n",
        "        1. (5 points) Create your own PyTorch class that implements the method of SCAD regularization and variable selection (smoothly clipped absolute deviations) for linear models. Your development should be based on the following references:\n",
        "\n",
        "https://andrewcharlesjones.github.io/journal/scad.html\n",
        "https://www.jstor.org/stable/27640214?seq=1\n",
        "            Test your method one a real data set, and determine a variable selection based on features importance according to SCAD.\n"
      ],
      "metadata": {
        "id": "Ulr2mJpzxf2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#given beta star, do 500 sim of x and y."
      ],
      "metadata": {
        "id": "mgN7s3ypzsrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # we are going to use pytorch instead of numpy because it's much faster.\n",
        "import torch.nn as nn\n",
        "# from ignite.contrib.metrics.regression import R2Score\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import linear_model\n",
        "from scipy.optimize import minimize\n",
        "from scipy.linalg import toeplitz\n",
        "from sklearn.metrics import mean_absolute_error as mae, mean_squared_error as mse, r2_score as R2\n",
        "from sklearn.model_selection import train_test_split as tts"
      ],
      "metadata": {
        "id": "sjwxG0KfgtVO"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")\n",
        "dtype = torch.float64"
      ],
      "metadata": {
        "id": "HmJw1NEYxlSg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1"
      ],
      "metadata": {
        "id": "01L8YIdCxl5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#beta, lambda, alpha\n",
        "#convert this to torch, it looks like all of the methods are there already\n",
        "#initialize beta hat with a sparsity pattern\n",
        "#what is lambda? regularization term. do we get from the dataset or do we initialize ourselves\n",
        "def scad_penalty(beta_hat, lambda_val, a_val):\n",
        "    is_linear = (torch.abs(beta_hat) <= lambda_val)\n",
        "    is_quadratic = torch.logical_and(lambda_val < torch.abs(beta_hat), torch.abs(beta_hat) <= a_val * lambda_val)\n",
        "    is_constant = (a_val * lambda_val) < torch.abs(beta_hat)\n",
        "\n",
        "    linear_part = lambda_val * torch.abs(beta_hat) * is_linear\n",
        "    quadratic_part = (2 * a_val * lambda_val * torch.abs(beta_hat) - beta_hat**2 - lambda_val**2) / (2 * (a_val - 1)) * is_quadratic\n",
        "    constant_part = (lambda_val**2 * (a + 1)) / 2 * is_constant\n",
        "    return linear_part + quadratic_part + constant_part\n",
        "\n",
        "def scad_derivative(beta_hat, lambda_val, a_val):\n",
        "    return lambda_val * ((beta_hat <= lambda_val) + (a_val * lambda_val - beta_hat)*((a_val * lambda_val - beta_hat) > 0) / ((a_val - 1) * lambda_val) * (beta_hat > lambda_val))"
      ],
      "metadata": {
        "id": "jRWMjqe0gyeF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StandardScaler:\n",
        "    def __init__(self):\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "\n",
        "    def fit(self, data):\n",
        "        \"\"\"\n",
        "        Compute the minimum and maximum value of the data for scaling.\n",
        "\n",
        "        Args:\n",
        "        - data (torch.Tensor): Input data tensor.\n",
        "        \"\"\"\n",
        "        self.mean = torch.mean(data, dim=0, keepdim=True)\n",
        "        self.std = torch.std(data, dim=0, keepdim=True)+1e-10\n",
        "\n",
        "    def transform(self, data):\n",
        "        \"\"\"\n",
        "        Scale the data based on the computed minimum and maximum values.\n",
        "\n",
        "        Args:\n",
        "        - data (torch.Tensor): Input data tensor.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Scaled data tensor.\n",
        "        \"\"\"\n",
        "        if self.mean is None or self.std is None:\n",
        "            raise ValueError(\"Scaler has not been fitted yet. Please call 'fit' with appropriate data.\")\n",
        "\n",
        "        scaled_data = (data - self.mean) / (self.std)\n",
        "        return scaled_data\n",
        "\n",
        "    def fit_transform(self, data):\n",
        "        \"\"\"\n",
        "        Fit to data, then transform it.\n",
        "\n",
        "        Args:\n",
        "        - data (torch.Tensor): Input data tensor.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Scaled data tensor.\n",
        "        \"\"\"\n",
        "        self.fit(data)\n",
        "        return self.transform(data)"
      ],
      "metadata": {
        "id": "O9egeXTP3v22"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MinMaxScaler:\n",
        "    def __init__(self):\n",
        "        self.min = None\n",
        "        self.max = None\n",
        "\n",
        "    def fit(self, data):\n",
        "        \"\"\"\n",
        "        Compute the minimum and maximum value of the data for scaling.\n",
        "\n",
        "        Args:\n",
        "        - data (torch.Tensor): Input data tensor.\n",
        "        \"\"\"\n",
        "        self.min = torch.min(data, dim=0, keepdim=True).values\n",
        "        self.max = torch.max(data, dim=0, keepdim=True).values\n",
        "\n",
        "    def transform(self, data):\n",
        "        \"\"\"\n",
        "        Scale the data based on the computed minimum and maximum values.\n",
        "\n",
        "        Args:\n",
        "        - data (torch.Tensor): Input data tensor.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Scaled data tensor.\n",
        "        \"\"\"\n",
        "        if self.min is None or self.max is None:\n",
        "            raise ValueError(\"Scaler has not been fitted yet. Please call 'fit' with appropriate data.\")\n",
        "\n",
        "        scaled_data = (data - self.min) / (self.max - self.min)\n",
        "        return scaled_data\n",
        "\n",
        "    def fit_transform(self, data):\n",
        "        \"\"\"\n",
        "        Fit to data, then transform it.\n",
        "\n",
        "        Args:\n",
        "        - data (torch.Tensor): Input data tensor.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Scaled data tensor.\n",
        "        \"\"\"\n",
        "        self.fit(data)\n",
        "        return self.transform(data)"
      ],
      "metadata": {
        "id": "HbMZ4TwQ3v6K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ElasticNet(nn.Module):\n",
        "    def __init__(self, input_size, alpha=1.0, l1_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the ElasticNet regression model.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Number of input features.\n",
        "            alpha (float): Regularization strength. Higher values of alpha\n",
        "                emphasize L1 regularization, while lower values emphasize L2 regularization.\n",
        "            l1_ratio (float): The ratio of L1 regularization to the total\n",
        "                regularization (L1 + L2). It should be between 0 and 1.\n",
        "\n",
        "        \"\"\"\n",
        "        super(ElasticNet, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.alpha = alpha\n",
        "        self.l1_ratio = l1_ratio\n",
        "\n",
        "        # Define the linear regression layer\n",
        "        self.linear = nn.Linear(input_size, 1).double()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the ElasticNet model.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input data with shape (batch_size, input_size).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Predicted values with shape (batch_size, 1).\n",
        "\n",
        "        \"\"\"\n",
        "        return self.linear(x)\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Compute the ElasticNet loss function.\n",
        "\n",
        "        Args:\n",
        "            y_pred (Tensor): Predicted values with shape (batch_size, 1).\n",
        "            y_true (Tensor): True target values with shape (batch_size, 1).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The ElasticNet loss.\n",
        "\n",
        "        \"\"\"\n",
        "        mse_loss = nn.MSELoss()(y_pred, y_true)\n",
        "        l1_reg = torch.norm(self.linear.weight, p=1)\n",
        "        l2_reg = torch.norm(self.linear.weight, p=2)\n",
        "\n",
        "        penalty_term = self.alpha * (\n",
        "                    self.l1_ratio * l1_reg + (1 - self.l1_ratio) * l2_reg\n",
        "                )\n",
        "        loss = (1/2)*mse_loss + penalty_term\n",
        "        return loss,penalty_term\n",
        "\n",
        "    def fit(self, X, y, num_epochs=100, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        Fit the ElasticNet model to the training data.\n",
        "\n",
        "        Args:\n",
        "            X (Tensor): Input data with shape (num_samples, input_size).\n",
        "            y (Tensor): Target values with shape (num_samples, 1).\n",
        "            num_epochs (int): Number of training epochs.\n",
        "            learning_rate (float): Learning rate for optimization.\n",
        "\n",
        "        \"\"\"\n",
        "        optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            self.train()\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = self(X)\n",
        "            loss, penalty_term = self.loss(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item(), penalty_term}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict target values for input data.\n",
        "\n",
        "        Args:\n",
        "            X (Tensor): Input data with shape (num_samples, input_size).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Predicted values with shape (num_samples, 1).\n",
        "\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred = self(X)\n",
        "        return y_pred\n",
        "    def get_coefficients(self):\n",
        "        \"\"\"\n",
        "        Get the coefficients (weights) of the linear regression layer.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Coefficients with shape (output_size, input_size).\n",
        "\n",
        "        \"\"\"\n",
        "        return self.linear.weight\n"
      ],
      "metadata": {
        "id": "f_vU2rAYyNU_"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class sqrtLasso(nn.Module):\n",
        "    def __init__(self, input_size, alpha=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the  regression model.\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        super(sqrtLasso, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.alpha = alpha\n",
        "\n",
        "\n",
        "        # Define the linear regression layer\n",
        "        self.linear = nn.Linear(input_size, 1).double()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input data with shape (batch_size, input_size).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Predicted values with shape (batch_size, 1).\n",
        "\n",
        "        \"\"\"\n",
        "        return self.linear(x)\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Compute the loss function.\n",
        "\n",
        "        Args:\n",
        "            y_pred (Tensor): Predicted values with shape (batch_size, 1).\n",
        "            y_true (Tensor): True target values with shape (batch_size, 1).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The loss.\n",
        "\n",
        "        \"\"\"\n",
        "        mse_loss = nn.MSELoss()(y_pred, y_true)\n",
        "        l1_reg = torch.norm(self.linear.weight, p=1,dtype=torch.float64)\n",
        "        # l2_reg = torch.norm(self.linear.weight, p=2,dtype=torch.float64)\n",
        "\n",
        "        loss = (len(y_true)*mse_loss)**(1/2) + self.alpha * (l1_reg)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def fit(self, X, y, num_epochs=200, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        Fit the model to the training data.\n",
        "\n",
        "        Args:\n",
        "            X (Tensor): Input data with shape (num_samples, input_size).\n",
        "            y (Tensor): Target values with shape (num_samples, 1).\n",
        "            num_epochs (int): Number of training epochs.\n",
        "            learning_rate (float): Learning rate for optimization.\n",
        "\n",
        "        \"\"\"\n",
        "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            self.train()\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = self(X)\n",
        "            loss = self.loss(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (epoch + 1) % 100 == 0:\n",
        "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict target values for input data.\n",
        "\n",
        "        Args:\n",
        "            X (Tensor): Input data with shape (num_samples, input_size).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Predicted values with shape (num_samples, 1).\n",
        "\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred = self(X)\n",
        "        return y_pred\n",
        "    def get_coefficients(self):\n",
        "        \"\"\"\n",
        "        Get the coefficients (weights) of the linear regression layer.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Coefficients with shape (output_size, input_size).\n",
        "\n",
        "        \"\"\"\n",
        "        return self.linear.weight"
      ],
      "metadata": {
        "id": "jyD3u0m-yO6n"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scad_penalty(beta_hat, lambda_val, a_val):\n",
        "    is_linear = (torch.abs(beta_hat) <= lambda_val)\n",
        "    is_quadratic = torch.logical_and(lambda_val < torch.abs(beta_hat), torch.abs(beta_hat) <= a_val * lambda_val)\n",
        "    is_constant = (a_val * lambda_val) < torch.abs(beta_hat)\n",
        "\n",
        "    linear_part = lambda_val * torch.abs(beta_hat) * is_linear\n",
        "    quadratic_part = (2 * a_val * lambda_val * torch.abs(beta_hat) - beta_hat**2 - lambda_val**2) / (2 * (a_val - 1)) * is_quadratic\n",
        "    constant_part = (lambda_val**2 * (a_val + 1)) / 2 * is_constant\n",
        "    return linear_part + quadratic_part + constant_part"
      ],
      "metadata": {
        "id": "rUt1k7DV0CIE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "FtRciLpX4lbS",
        "outputId": "4d62cfe3-ec5a-4076-82a9-f8efc77989b6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-18994828-5e7e-4348-a3b7-b8e596fe3e28\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-18994828-5e7e-4348-a3b7-b8e596fe3e28\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving concrete.csv to concrete.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conc = pd.read_csv('concrete.csv')\n",
        "conc.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2bqKxaK6alt",
        "outputId": "4c473306-968a-45bf-9ff4-1445afc5b1d0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1030, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = conc.iloc[:,-1].values\n",
        "x = conc.iloc[:, :-1].values"
      ],
      "metadata": {
        "id": "Luk28ebgKAcz"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss = StandardScaler()"
      ],
      "metadata": {
        "id": "QjDAWHd6gXsr"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain, xtest, ytrain, ytest = tts(x,y,test_size=0.2,shuffle=True,random_state=123)\n",
        "xtrain = ss.fit_transform(xtrain)\n",
        "xtest = ss.transform(xtest)"
      ],
      "metadata": {
        "id": "ATqD9YKMf_CH"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain = torch.tensor(xtrain)\n",
        "xtest = torch.tensor(xtest)\n",
        "ytrain = torch.tensor(ytrain)\n",
        "ytest = torch.tensor(ytest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WDnbSD-KxdQ",
        "outputId": "5f1546af-fd4f-411c-fb3d-e36f7dd065ef"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-96-b80ef10eae68>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  xtrain = torch.tensor(xtrain)\n",
            "<ipython-input-96-b80ef10eae68>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  xtest = torch.tensor(xtest)\n",
            "<ipython-input-96-b80ef10eae68>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  ytrain = torch.tensor(ytrain)\n",
            "<ipython-input-96-b80ef10eae68>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  ytest = torch.tensor(ytest)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SCAD(nn.Module):\n",
        "    def __init__(self, input_size, alpha=0.1, lambda_val = .95):\n",
        "        \"\"\"\n",
        "        Initialize the  regression model.\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        super(SCAD, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.alpha = alpha\n",
        "        self.lambda_val = lambda_val\n",
        "\n",
        "        # Define the linear regression layer\n",
        "        self.linear = nn.Linear(input_size, 1).double()\n",
        "\n",
        "    def scad_penalty(self, beta_hat, lambda_val, a_val):\n",
        "        is_linear = (torch.abs(beta_hat) <= lambda_val)\n",
        "        is_quadratic = torch.logical_and(lambda_val < torch.abs(beta_hat), torch.abs(beta_hat) <= a_val * lambda_val)\n",
        "        is_constant = (a_val * lambda_val) < torch.abs(beta_hat)\n",
        "\n",
        "        linear_part = lambda_val * torch.abs(beta_hat) * is_linear\n",
        "        quadratic_part = (2 * a_val * lambda_val * torch.abs(beta_hat) - beta_hat**2 - lambda_val**2) / (2 * (a_val - 1)) * is_quadratic\n",
        "        constant_part = (lambda_val**2 * (a_val + 1)) / 2 * is_constant\n",
        "        return linear_part + quadratic_part + constant_part\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input data with shape (batch_size, input_size).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Predicted values with shape (batch_size, 1).\n",
        "\n",
        "        \"\"\"\n",
        "        return self.linear(x)\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Compute the loss function.\n",
        "\n",
        "        Args:\n",
        "            y_pred (Tensor): Predicted values with shape (batch_size, 1).\n",
        "            y_true (Tensor): True target values with shape (batch_size, 1).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The loss.\n",
        "\n",
        "        \"\"\"\n",
        "        mse_loss = nn.MSELoss()(y_pred, y_true)\n",
        "\n",
        "        beta =torch.tensor([-1,2,2,0,0,0,2,-1])\n",
        "        beta = beta.reshape(-1,1)\n",
        "\n",
        "        num_zeros = self.input_size - len(beta)\n",
        "\n",
        "        # Create a tensor of zeros with the required shape\n",
        "        additional_zeros = torch.zeros((num_zeros, 1))\n",
        "        #betas = torch.concat([beta,torch.repeat(0,self.input_size-len(beta)).reshape(-1,1)],axis=0)\n",
        "        betas = torch.cat((beta, additional_zeros), dim=0)\n",
        "\n",
        "        scad_pen_term = scad_penalty(betas,self.lambda_val,self.alpha)\n",
        "\n",
        "        loss = (1/2)*mse_loss + scad_pen_term\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def fit(self, X, y, num_epochs=200, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        Fit the model to the training data.\n",
        "\n",
        "        Args:\n",
        "            X (Tensor): Input data with shape (num_samples, input_size).\n",
        "            y (Tensor): Target values with shape (num_samples, 1).\n",
        "            num_epochs (int): Number of training epochs.\n",
        "            learning_rate (float): Learning rate for optimization.\n",
        "\n",
        "        \"\"\"\n",
        "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            self.train()\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = self(X)\n",
        "            loss = self.loss(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (epoch + 1) % 100 == 0:\n",
        "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict target values for input data.\n",
        "\n",
        "        Args:\n",
        "            X (Tensor): Input data with shape (num_samples, input_size).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Predicted values with shape (num_samples, 1).\n",
        "\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred = self(X)\n",
        "        return y_pred\n",
        "    def get_coefficients(self):\n",
        "        \"\"\"\n",
        "        Get the coefficients (weights) of the linear regression layer.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Coefficients with shape (output_size, input_size).\n",
        "\n",
        "        \"\"\"\n",
        "        return self.linear.weight"
      ],
      "metadata": {
        "id": "Wt1gy2lKz6Gx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SCAD(nn.Module):\n",
        "    def __init__(self, input_size, alpha=0.01, lambda_val=0.78):\n",
        "        super(SCAD, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.a_val = alpha\n",
        "        self.lambda_val = lambda_val\n",
        "        self.linear = nn.Linear(input_size, 1).double()\n",
        "\n",
        "    def scad_penalty(self, beta_hat, lambda_val, a_val):\n",
        "        is_linear = (torch.abs(beta_hat) <= lambda_val)\n",
        "        is_quadratic = torch.logical_and(lambda_val < torch.abs(beta_hat), torch.abs(beta_hat) <= a_val * lambda_val)\n",
        "        is_constant = (a_val * lambda_val) < torch.abs(beta_hat)\n",
        "\n",
        "        linear_part = lambda_val * torch.abs(beta_hat) * is_linear\n",
        "        quadratic_part = (2 * a_val * lambda_val * torch.abs(beta_hat) - beta_hat**2 - lambda_val**2) / (2 * (a_val - 1)) * is_quadratic\n",
        "        constant_part = (lambda_val**2 * (a_val + 1)) / 2 * is_constant\n",
        "        return linear_part + quadratic_part + constant_part\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        mse_loss = nn.MSELoss()(y_pred, y_true)\n",
        "        beta_hat = self.linear.weight\n",
        "        scad_pen_term = self.scad_penalty(beta_hat,self.lambda_val,self.a_val)\n",
        "        scad_pen_term = torch.mean(scad_pen_term)\n",
        "        loss = (1 / 2) * mse_loss + scad_pen_term\n",
        "        return loss,scad_pen_term\n",
        "\n",
        "    def fit(self, X, y, num_epochs=200, learning_rate=0.01):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        for epoch in range(num_epochs):\n",
        "            self.train()\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = self(X)\n",
        "            y_true = y.view(-1, 1)\n",
        "            loss,scad_pen_term = self.loss(y_pred, y_true)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (epoch + 1) % 100 == 0:\n",
        "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item(),scad_pen_term}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred = self(X)\n",
        "        return y_pred\n",
        "\n",
        "    def get_coefficients(self):\n",
        "        return self.linear.weight"
      ],
      "metadata": {
        "id": "kZ1bLbVlQtOA"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SCAD(input_size=xtrain.shape[1])"
      ],
      "metadata": {
        "id": "1H2IhYu4-EPr"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(xtrain,ytrain,num_epochs=20000,learning_rate=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jigief1Wz0Le",
        "outputId": "2e4ba345-0f3d-4edf-e396-4acc6c72843e"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/20000], Loss: (718.5927987275438, tensor(0.4762, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [200/20000], Loss: (665.4216087842962, tensor(0.3232, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [300/20000], Loss: (620.321765121233, tensor(0.3232, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [400/20000], Loss: (581.000314521797, tensor(0.3232, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [500/20000], Loss: (546.0279416489645, tensor(0.3232, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [600/20000], Loss: (514.3266978674164, tensor(0.3232, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [700/20000], Loss: (485.10980125515744, tensor(0.3232, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [800/20000], Loss: (457.90884473267056, tensor(0.3958, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [900/20000], Loss: (432.1893236412573, tensor(0.3683, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1000/20000], Loss: (407.84648887418484, tensor(0.3448, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1100/20000], Loss: (384.7049567486792, tensor(0.2831, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1200/20000], Loss: (362.8571868791066, tensor(0.3281, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1300/20000], Loss: (342.07320165368645, tensor(0.3489, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1400/20000], Loss: (322.31831060460024, tensor(0.3704, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1500/20000], Loss: (303.5510871001993, tensor(0.3916, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1600/20000], Loss: (285.6330596095678, tensor(0.3232, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1700/20000], Loss: (268.6955905176956, tensor(0.3232, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1800/20000], Loss: (252.64973269590465, tensor(0.3232, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1900/20000], Loss: (237.4649103082344, tensor(0.3232, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2000/20000], Loss: (223.11188638895288, tensor(0.3232, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2100/20000], Loss: (209.56245401906475, tensor(0.3232, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2200/20000], Loss: (196.789343853528, tensor(0.3232, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2300/20000], Loss: (184.76614484720167, tensor(0.3232, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2400/20000], Loss: (173.4672163435439, tensor(0.3232, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2500/20000], Loss: (162.86758878149698, tensor(0.3232, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2600/20000], Loss: (153.00465998071343, tensor(0.3867, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2700/20000], Loss: (143.70348673565167, tensor(0.3700, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2800/20000], Loss: (135.0964726971842, tensor(0.4222, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2900/20000], Loss: (126.98820938398768, tensor(0.3756, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3000/20000], Loss: (119.43281806006563, tensor(0.3014, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3100/20000], Loss: (112.55747715155057, tensor(0.3265, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3200/20000], Loss: (106.16560493855864, tensor(0.2915, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3300/20000], Loss: (100.34351077086347, tensor(0.2964, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3400/20000], Loss: (95.05290758151028, tensor(0.3438, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3500/20000], Loss: (90.18346446310761, tensor(0.3529, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3600/20000], Loss: (85.75222436767505, tensor(0.3618, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3700/20000], Loss: (81.73617670886303, tensor(0.3705, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3800/20000], Loss: (78.11232420922467, tensor(0.3787, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3900/20000], Loss: (74.8576899548944, tensor(0.3866, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4000/20000], Loss: (71.9493434313142, tensor(0.3940, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4100/20000], Loss: (69.36444137706928, tensor(0.4009, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4200/20000], Loss: (67.08028217420868, tensor(0.4074, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4300/20000], Loss: (65.07437298258645, tensor(0.4134, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4400/20000], Loss: (63.32450891571928, tensor(0.4189, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4500/20000], Loss: (61.808863378482265, tensor(0.4239, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4600/20000], Loss: (60.506088309990595, tensor(0.4284, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4700/20000], Loss: (59.39542255534018, tensor(0.4324, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4800/20000], Loss: (58.45680597974302, tensor(0.4360, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4900/20000], Loss: (57.67099629133637, tensor(0.4392, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5000/20000], Loss: (57.01968491429775, tensor(0.4420, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5100/20000], Loss: (56.485607720310206, tensor(0.4443, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5200/20000], Loss: (56.05264605952463, tensor(0.4464, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5300/20000], Loss: (55.70591340889231, tensor(0.4481, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5400/20000], Loss: (55.431823144551224, tensor(0.4496, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5500/20000], Loss: (55.21813349338911, tensor(0.4508, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5600/20000], Loss: (55.05396664047029, tensor(0.4517, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5700/20000], Loss: (54.92980023241136, tensor(0.4525, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5800/20000], Loss: (54.83743104057488, tensor(0.4531, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5900/20000], Loss: (54.76991220356612, tensor(0.4536, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6000/20000], Loss: (54.72146709297828, tensor(0.4540, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6100/20000], Loss: (54.68738426383003, tensor(0.4543, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6200/20000], Loss: (54.66389900028169, tensor(0.4545, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6300/20000], Loss: (54.64806752847897, tensor(0.4547, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6400/20000], Loss: (54.63763998571378, tensor(0.4548, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6500/20000], Loss: (54.630937726422815, tensor(0.4548, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6600/20000], Loss: (54.626739599304905, tensor(0.4549, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6700/20000], Loss: (54.624180588310836, tensor(0.4549, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6800/20000], Loss: (54.62266484344141, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6900/20000], Loss: (54.62179380338631, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7000/20000], Loss: (54.62130897095134, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7100/20000], Loss: (54.621048037283956, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7200/20000], Loss: (54.62091250154093, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7300/20000], Loss: (54.620844687903926, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7400/20000], Loss: (54.62081207237238, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7500/20000], Loss: (54.620797026134646, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7600/20000], Loss: (54.62079038377274, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7700/20000], Loss: (54.62078758451606, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7800/20000], Loss: (54.62078646131412, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7900/20000], Loss: (54.62078603338929, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8000/20000], Loss: (54.62078587904294, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8100/20000], Loss: (54.62078582650314, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8200/20000], Loss: (54.62078580968022, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8300/20000], Loss: (54.6207858046313, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8400/20000], Loss: (54.620785803216336, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8500/20000], Loss: (54.62078580284754, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8600/20000], Loss: (54.62078580275853, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8700/20000], Loss: (54.620785802738716, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8800/20000], Loss: (54.62078580273467, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8900/20000], Loss: (54.62078580273393, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9000/20000], Loss: (54.6207858027338, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9100/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9200/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9300/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9400/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9500/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9600/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9700/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9800/20000], Loss: (54.62078580273377, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9900/20000], Loss: (54.62078580273377, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10000/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10100/20000], Loss: (54.62078580273377, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10200/20000], Loss: (54.62078580273377, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10300/20000], Loss: (54.62078580273377, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10400/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10500/20000], Loss: (54.62078580273377, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10600/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10700/20000], Loss: (54.62078580273377, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10800/20000], Loss: (54.62078580273377, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10900/20000], Loss: (54.62078580273377, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11000/20000], Loss: (54.620785802733764, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11100/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11200/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11300/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11400/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11500/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11600/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11700/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11800/20000], Loss: (54.62078580273379, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11900/20000], Loss: (54.62078580273377, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12000/20000], Loss: (54.62078580273379, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12100/20000], Loss: (54.62078580273379, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12200/20000], Loss: (54.62078580273379, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12300/20000], Loss: (54.62078580273379, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12400/20000], Loss: (54.62078580273379, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12500/20000], Loss: (54.62078580273377, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12600/20000], Loss: (54.62078580273379, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12700/20000], Loss: (54.62078580273378, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12800/20000], Loss: (54.62078580273378, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12900/20000], Loss: (54.62078580273379, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13000/20000], Loss: (54.62078580273377, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13100/20000], Loss: (54.62078580273378, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13200/20000], Loss: (54.62078580273377, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13300/20000], Loss: (54.62078580273378, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13400/20000], Loss: (54.62078580273378, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13500/20000], Loss: (54.62078580273378, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13600/20000], Loss: (54.62078580273379, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13700/20000], Loss: (54.62078580273379, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13800/20000], Loss: (54.62078580273379, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13900/20000], Loss: (54.62078580273378, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14000/20000], Loss: (54.62078580273378, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14100/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14200/20000], Loss: (54.620786129914535, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14300/20000], Loss: (54.62078580275388, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14400/20000], Loss: (54.62078580273378, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14500/20000], Loss: (54.62078580273378, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14600/20000], Loss: (54.62078580273378, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14700/20000], Loss: (54.62078587759755, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14800/20000], Loss: (54.62078580275257, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14900/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15000/20000], Loss: (54.62078580273378, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15100/20000], Loss: (54.62078580273378, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15200/20000], Loss: (54.620785805406626, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15300/20000], Loss: (54.62078580273383, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15400/20000], Loss: (54.62078580273378, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15500/20000], Loss: (54.62078580273378, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15600/20000], Loss: (54.6207880468383, tensor(0.4551, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15700/20000], Loss: (54.620785802801656, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15800/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15900/20000], Loss: (54.620785802733785, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16000/20000], Loss: (54.62078580406365, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16100/20000], Loss: (54.620785803750834, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16200/20000], Loss: (54.62078580273381, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16300/20000], Loss: (54.62078580273378, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16400/20000], Loss: (54.62078580276284, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16500/20000], Loss: (54.620785802783224, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16600/20000], Loss: (54.620785802733806, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16700/20000], Loss: (54.62078580273377, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16800/20000], Loss: (54.62078696491097, tensor(0.4549, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16900/20000], Loss: (54.62078580273823, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17000/20000], Loss: (54.62078580273378, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17100/20000], Loss: (54.620785802735995, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17200/20000], Loss: (54.620785803418116, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17300/20000], Loss: (54.62078580273395, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17400/20000], Loss: (54.620785802780965, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17500/20000], Loss: (54.62078797003458, tensor(0.4551, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17600/20000], Loss: (54.62078644158247, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17700/20000], Loss: (54.620785803084914, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17800/20000], Loss: (54.62078580612403, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17900/20000], Loss: (54.62078580302928, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18000/20000], Loss: (54.62078588703094, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18100/20000], Loss: (54.620785804659164, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18200/20000], Loss: (54.62078583132843, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18300/20000], Loss: (54.62078580383025, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18400/20000], Loss: (54.62078581151996, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18500/20000], Loss: (54.62078580929848, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18600/20000], Loss: (54.620785819123654, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18700/20000], Loss: (54.62078675725899, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18800/20000], Loss: (54.620785802860304, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18900/20000], Loss: (54.62078581181946, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19000/20000], Loss: (54.620785828185745, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19100/20000], Loss: (54.620785805192156, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19200/20000], Loss: (54.62078585558714, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19300/20000], Loss: (54.62078793792924, tensor(0.4551, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19400/20000], Loss: (54.62078580381853, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19500/20000], Loss: (54.620785802735, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19600/20000], Loss: (54.620785821290994, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19700/20000], Loss: (54.62078583763261, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19800/20000], Loss: (54.62078749378689, tensor(0.4551, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19900/20000], Loss: (54.620785803942915, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [20000/20000], Loss: (54.62078580603005, tensor(0.4550, dtype=torch.float64, grad_fn=<MeanBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse(model.predict(xtest),ytest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TztpSwkbhdlV",
        "outputId": "06e2f25a-7b04-4568-df15-d3df620e91ec"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "104.2929170816051"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_coefficients()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ePLY7vePCwE",
        "outputId": "51a649d0-90aa-4718-983f-0b971ec3a76e"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[11.5211,  7.9807,  4.7786, -3.6877,  1.9179,  0.6645,  0.6535,  7.1890]],\n",
              "       dtype=torch.float64, requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = sqrtLasso(input_size=xtrain.shape[1])"
      ],
      "metadata": {
        "id": "kw70kjs7Sxtn"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.fit(xtrain,ytrain, num_epochs = 20000,learning_rate = .01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B2SlVipUTZ_",
        "outputId": "7dec9b5c-6736-44f7-92ca-e9fd9db1ee7a"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([824])) that is different to the input size (torch.Size([824, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/20000], Loss: 1101.1778712341327\n",
            "Epoch [200/20000], Loss: 1075.5073496126747\n",
            "Epoch [300/20000], Loss: 1050.061087838795\n",
            "Epoch [400/20000], Loss: 1024.8547351912694\n",
            "Epoch [500/20000], Loss: 999.9046351470631\n",
            "Epoch [600/20000], Loss: 975.227477592403\n",
            "Epoch [700/20000], Loss: 950.8425891913314\n",
            "Epoch [800/20000], Loss: 926.7707714303557\n",
            "Epoch [900/20000], Loss: 903.0333785883208\n",
            "Epoch [1000/20000], Loss: 879.6546560832107\n",
            "Epoch [1100/20000], Loss: 856.6606169196449\n",
            "Epoch [1200/20000], Loss: 834.0780221710646\n",
            "Epoch [1300/20000], Loss: 811.9368169185057\n",
            "Epoch [1400/20000], Loss: 790.2688108696594\n",
            "Epoch [1500/20000], Loss: 769.1066081038941\n",
            "Epoch [1600/20000], Loss: 748.4858760286678\n",
            "Epoch [1700/20000], Loss: 728.4438321742853\n",
            "Epoch [1800/20000], Loss: 709.0179905831\n",
            "Epoch [1900/20000], Loss: 690.2481981232718\n",
            "Epoch [2000/20000], Loss: 672.1747941328301\n",
            "Epoch [2100/20000], Loss: 654.8369557044236\n",
            "Epoch [2200/20000], Loss: 638.2743479930638\n",
            "Epoch [2300/20000], Loss: 622.5248119154489\n",
            "Epoch [2400/20000], Loss: 607.6223572541842\n",
            "Epoch [2500/20000], Loss: 593.5984430678321\n",
            "Epoch [2600/20000], Loss: 580.4793775717071\n",
            "Epoch [2700/20000], Loss: 568.2840736358694\n",
            "Epoch [2800/20000], Loss: 557.0253518545728\n",
            "Epoch [2900/20000], Loss: 546.7074996520489\n",
            "Epoch [3000/20000], Loss: 537.3245694672866\n",
            "Epoch [3100/20000], Loss: 528.862391155529\n",
            "Epoch [3200/20000], Loss: 521.297083013059\n",
            "Epoch [3300/20000], Loss: 514.5943643536349\n",
            "Epoch [3400/20000], Loss: 508.7127597436695\n",
            "Epoch [3500/20000], Loss: 503.6032092987854\n",
            "Epoch [3600/20000], Loss: 499.2095141112568\n",
            "Epoch [3700/20000], Loss: 495.47237199688203\n",
            "Epoch [3800/20000], Loss: 492.3295987833877\n",
            "Epoch [3900/20000], Loss: 489.7167682007599\n",
            "Epoch [4000/20000], Loss: 487.5712531533612\n",
            "Epoch [4100/20000], Loss: 485.83212686891807\n",
            "Epoch [4200/20000], Loss: 484.4404192177187\n",
            "Epoch [4300/20000], Loss: 483.34251467108317\n",
            "Epoch [4400/20000], Loss: 482.4893800408639\n",
            "Epoch [4500/20000], Loss: 481.8359759485913\n",
            "Epoch [4600/20000], Loss: 481.34392825866746\n",
            "Epoch [4700/20000], Loss: 480.98006126502526\n",
            "Epoch [4800/20000], Loss: 480.7151358292709\n",
            "Epoch [4900/20000], Loss: 480.5261099008159\n",
            "Epoch [5000/20000], Loss: 480.3943545084479\n",
            "Epoch [5100/20000], Loss: 480.3038585392386\n",
            "Epoch [5200/20000], Loss: 480.24327979406763\n",
            "Epoch [5300/20000], Loss: 480.2040708823477\n",
            "Epoch [5400/20000], Loss: 480.17874011142595\n",
            "Epoch [5500/20000], Loss: 480.1629468060112\n",
            "Epoch [5600/20000], Loss: 480.1537798208667\n",
            "Epoch [5700/20000], Loss: 480.1480997400367\n",
            "Epoch [5800/20000], Loss: 480.1448340857808\n",
            "Epoch [5900/20000], Loss: 480.1434030781238\n",
            "Epoch [6000/20000], Loss: 480.14233418616107\n",
            "Epoch [6100/20000], Loss: 480.1417020240605\n",
            "Epoch [6200/20000], Loss: 480.1417410778914\n",
            "Epoch [6300/20000], Loss: 480.14148527618136\n",
            "Epoch [6400/20000], Loss: 480.1413160030052\n",
            "Epoch [6500/20000], Loss: 480.1415946142035\n",
            "Epoch [6600/20000], Loss: 480.1414374453899\n",
            "Epoch [6700/20000], Loss: 480.1412738495198\n",
            "Epoch [6800/20000], Loss: 480.1415445265158\n",
            "Epoch [6900/20000], Loss: 480.14139878306\n",
            "Epoch [7000/20000], Loss: 480.14127954682596\n",
            "Epoch [7100/20000], Loss: 480.1415798633795\n",
            "Epoch [7200/20000], Loss: 480.1414317658008\n",
            "Epoch [7300/20000], Loss: 480.141271740558\n",
            "Epoch [7400/20000], Loss: 480.14154371887315\n",
            "Epoch [7500/20000], Loss: 480.14139848521194\n",
            "Epoch [7600/20000], Loss: 480.14127941312057\n",
            "Epoch [7700/20000], Loss: 480.14157975739386\n",
            "Epoch [7800/20000], Loss: 480.14143170475216\n",
            "Epoch [7900/20000], Loss: 480.14127169300235\n",
            "Epoch [8000/20000], Loss: 480.14154365805774\n",
            "Epoch [8100/20000], Loss: 480.14139844602255\n",
            "Epoch [8200/20000], Loss: 480.1412793782054\n",
            "Epoch [8300/20000], Loss: 480.14157971123535\n",
            "Epoch [8400/20000], Loss: 480.14143167479824\n",
            "Epoch [8500/20000], Loss: 480.1412716682365\n",
            "Epoch [8600/20000], Loss: 480.1415436257313\n",
            "Epoch [8700/20000], Loss: 480.14139842502635\n",
            "Epoch [8800/20000], Loss: 480.141279359521\n",
            "Epoch [8900/20000], Loss: 480.1415796865798\n",
            "Epoch [9000/20000], Loss: 480.1414316587249\n",
            "Epoch [9100/20000], Loss: 480.1412716549745\n",
            "Epoch [9200/20000], Loss: 480.1415436084582\n",
            "Epoch [9300/20000], Loss: 480.1413984137551\n",
            "Epoch [9400/20000], Loss: 480.1412793495155\n",
            "Epoch [9500/20000], Loss: 480.1415796734054\n",
            "Epoch [9600/20000], Loss: 480.1414316500972\n",
            "Epoch [9700/20000], Loss: 480.14127164787084\n",
            "Epoch [9800/20000], Loss: 480.1415435992262\n",
            "Epoch [9900/20000], Loss: 480.14139840770304\n",
            "Epoch [10000/20000], Loss: 480.14127934415643\n",
            "Epoch [10100/20000], Loss: 480.14157966636424\n",
            "Epoch [10200/20000], Loss: 480.14143164546516\n",
            "Epoch [10300/20000], Loss: 480.141271644065\n",
            "Epoch [10400/20000], Loss: 480.1415435942911\n",
            "Epoch [10500/20000], Loss: 480.1413984044527\n",
            "Epoch [10600/20000], Loss: 480.1412793412855\n",
            "Epoch [10700/20000], Loss: 480.14157966260035\n",
            "Epoch [10800/20000], Loss: 480.141431642978\n",
            "Epoch [10900/20000], Loss: 480.14127164202574\n",
            "Epoch [11000/20000], Loss: 480.1415435916526\n",
            "Epoch [11100/20000], Loss: 480.1413984027068\n",
            "Epoch [11200/20000], Loss: 480.1412793397473\n",
            "Epoch [11300/20000], Loss: 480.1415796605881\n",
            "Epoch [11400/20000], Loss: 480.1414316416421\n",
            "Epoch [11500/20000], Loss: 480.14127164093287\n",
            "Epoch [11600/20000], Loss: 480.14154359024155\n",
            "Epoch [11700/20000], Loss: 480.14139840176887\n",
            "Epoch [11800/20000], Loss: 480.141279338923\n",
            "Epoch [11900/20000], Loss: 480.1415796595122\n",
            "Epoch [12000/20000], Loss: 480.1414316409246\n",
            "Epoch [12100/20000], Loss: 480.1412716403471\n",
            "Epoch [12200/20000], Loss: 480.14154358948696\n",
            "Epoch [12300/20000], Loss: 480.14139840126495\n",
            "Epoch [12400/20000], Loss: 480.1412793384812\n",
            "Epoch [12500/20000], Loss: 480.14157965893685\n",
            "Epoch [12600/20000], Loss: 480.14143164053917\n",
            "Epoch [12700/20000], Loss: 480.14127164003315\n",
            "Epoch [12800/20000], Loss: 480.14154358908337\n",
            "Epoch [12900/20000], Loss: 480.14139840099415\n",
            "Epoch [13000/20000], Loss: 480.1412793382445\n",
            "Epoch [13100/20000], Loss: 480.1415796586291\n",
            "Epoch [13200/20000], Loss: 480.14143164033214\n",
            "Epoch [13300/20000], Loss: 480.14127163986484\n",
            "Epoch [13400/20000], Loss: 480.14154358886753\n",
            "Epoch [13500/20000], Loss: 480.1413984008486\n",
            "Epoch [13600/20000], Loss: 480.1412793381175\n",
            "Epoch [13700/20000], Loss: 480.1415796584644\n",
            "Epoch [13800/20000], Loss: 480.14143164022084\n",
            "Epoch [13900/20000], Loss: 480.1412716397745\n",
            "Epoch [14000/20000], Loss: 480.141543588752\n",
            "Epoch [14100/20000], Loss: 480.1413984007705\n",
            "Epoch [14200/20000], Loss: 480.14127933804946\n",
            "Epoch [14300/20000], Loss: 480.14157965837643\n",
            "Epoch [14400/20000], Loss: 480.14143164016104\n",
            "Epoch [14500/20000], Loss: 480.14127163972614\n",
            "Epoch [14600/20000], Loss: 480.1415435886902\n",
            "Epoch [14700/20000], Loss: 480.14139840072835\n",
            "Epoch [14800/20000], Loss: 480.14127933801296\n",
            "Epoch [14900/20000], Loss: 480.14157965832936\n",
            "Epoch [15000/20000], Loss: 480.1414316401289\n",
            "Epoch [15100/20000], Loss: 480.14127163970016\n",
            "Epoch [15200/20000], Loss: 480.14154358865716\n",
            "Epoch [15300/20000], Loss: 480.1413984007058\n",
            "Epoch [15400/20000], Loss: 480.1412793379934\n",
            "Epoch [15500/20000], Loss: 480.14157965830407\n",
            "Epoch [15600/20000], Loss: 480.1414316401116\n",
            "Epoch [15700/20000], Loss: 480.14127163968624\n",
            "Epoch [15800/20000], Loss: 480.1415435886394\n",
            "Epoch [15900/20000], Loss: 480.1413984006937\n",
            "Epoch [16000/20000], Loss: 480.1412793379829\n",
            "Epoch [16100/20000], Loss: 480.1415796582906\n",
            "Epoch [16200/20000], Loss: 480.1414316401024\n",
            "Epoch [16300/20000], Loss: 480.1412716396788\n",
            "Epoch [16400/20000], Loss: 480.14154358862993\n",
            "Epoch [16500/20000], Loss: 480.14139840068714\n",
            "Epoch [16600/20000], Loss: 480.14127933797727\n",
            "Epoch [16700/20000], Loss: 480.14157965828343\n",
            "Epoch [16800/20000], Loss: 480.1414316400973\n",
            "Epoch [16900/20000], Loss: 480.1412716396748\n",
            "Epoch [17000/20000], Loss: 480.14154358862487\n",
            "Epoch [17100/20000], Loss: 480.14139840068356\n",
            "Epoch [17200/20000], Loss: 480.1412793379743\n",
            "Epoch [17300/20000], Loss: 480.1415796582795\n",
            "Epoch [17400/20000], Loss: 480.14143164009465\n",
            "Epoch [17500/20000], Loss: 480.1412716396726\n",
            "Epoch [17600/20000], Loss: 480.14154358862214\n",
            "Epoch [17700/20000], Loss: 480.14139840068174\n",
            "Epoch [17800/20000], Loss: 480.14127933797266\n",
            "Epoch [17900/20000], Loss: 480.14157965827746\n",
            "Epoch [18000/20000], Loss: 480.14143164009323\n",
            "Epoch [18100/20000], Loss: 480.14127163967146\n",
            "Epoch [18200/20000], Loss: 480.14154358862066\n",
            "Epoch [18300/20000], Loss: 480.14139840068066\n",
            "Epoch [18400/20000], Loss: 480.1412793379718\n",
            "Epoch [18500/20000], Loss: 480.1415796582764\n",
            "Epoch [18600/20000], Loss: 480.1414316400925\n",
            "Epoch [18700/20000], Loss: 480.14127163967083\n",
            "Epoch [18800/20000], Loss: 480.1415435886199\n",
            "Epoch [18900/20000], Loss: 480.14139840068015\n",
            "Epoch [19000/20000], Loss: 480.1412793379713\n",
            "Epoch [19100/20000], Loss: 480.14157965827576\n",
            "Epoch [19200/20000], Loss: 480.14143164009204\n",
            "Epoch [19300/20000], Loss: 480.14127163967055\n",
            "Epoch [19400/20000], Loss: 480.1415435886195\n",
            "Epoch [19500/20000], Loss: 480.14139840067986\n",
            "Epoch [19600/20000], Loss: 480.14127933797107\n",
            "Epoch [19700/20000], Loss: 480.1415796582755\n",
            "Epoch [19800/20000], Loss: 480.14143164009187\n",
            "Epoch [19900/20000], Loss: 480.1412716396704\n",
            "Epoch [20000/20000], Loss: 480.1415435886193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse(model1.predict(xtest),ytest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y57ODeMmjBSw",
        "outputId": "120efd3b-b045-4398-e310-b547b36313b8"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "275.09446915315567"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.get_coefficients()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCdwrqDLjyJd",
        "outputId": "f6b4debc-5950-4ba7-c326-1c1889539840"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0012, -0.0012,  0.0020, -0.0012,  0.0007, -0.0017, -0.0008, -0.0007]],\n",
              "       dtype=torch.float64, requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = ElasticNet(input_size=xtrain.shape[1],alpha=0.01,l1_ratio=0.5)"
      ],
      "metadata": {
        "id": "pOVH_262sf5C"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.fit(xtrain,ytrain, num_epochs = 20000,learning_rate = .01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq2L0TsWsknN",
        "outputId": "2dfa67fd-e9c2-438f-d4fe-ce57aca1302a"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/20000], Loss: (660.640437261402, tensor(0.0063, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [20/20000], Loss: (565.809773785003, tensor(0.0061, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [30/20000], Loss: (488.24991689396484, tensor(0.0059, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [40/20000], Loss: (424.81483359827274, tensor(0.0057, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [50/20000], Loss: (372.9318436775245, tensor(0.0056, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [60/20000], Loss: (330.4969942689609, tensor(0.0054, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [70/20000], Loss: (295.7895942836346, tensor(0.0053, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [80/20000], Loss: (267.40237134594105, tensor(0.0052, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [90/20000], Loss: (244.18436635023937, tensor(0.0050, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [100/20000], Loss: (225.1942390072803, tensor(0.0049, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [110/20000], Loss: (209.66208404233973, tensor(0.0048, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [120/20000], Loss: (196.9582054149216, tensor(0.0047, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [130/20000], Loss: (186.56757971375632, tensor(0.0045, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [140/20000], Loss: (178.0689716022643, tensor(0.0044, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [150/20000], Loss: (171.1178534700203, tensor(0.0043, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [160/20000], Loss: (165.4324361031275, tensor(0.0042, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [170/20000], Loss: (160.78224358252814, tensor(0.0041, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [180/20000], Loss: (156.9787689366453, tensor(0.0040, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [190/20000], Loss: (153.86783153886972, tensor(0.0039, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [200/20000], Loss: (151.32332629895072, tensor(0.0038, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [210/20000], Loss: (149.2421111648233, tensor(0.0037, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [220/20000], Loss: (147.5398256264215, tensor(0.0036, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [230/20000], Loss: (146.14747067349145, tensor(0.0035, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [240/20000], Loss: (145.008611539907, tensor(0.0034, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [250/20000], Loss: (144.07708982179622, tensor(0.0033, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [260/20000], Loss: (143.3151522112426, tensor(0.0032, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [270/20000], Loss: (142.69191997972183, tensor(0.0031, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [280/20000], Loss: (142.18213716119604, tensor(0.0030, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [290/20000], Loss: (141.76514668438065, tensor(0.0029, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [300/20000], Loss: (141.42405294545844, tensor(0.0028, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [310/20000], Loss: (141.1450368712466, tensor(0.0028, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [320/20000], Loss: (140.916795705062, tensor(0.0027, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [330/20000], Loss: (140.73008480395524, tensor(0.0026, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [340/20000], Loss: (140.57734287163316, tensor(0.0025, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [350/20000], Loss: (140.4523854339353, tensor(0.0025, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [360/20000], Loss: (140.35015413032374, tensor(0.0024, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [370/20000], Loss: (140.26651165764906, tensor(0.0023, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [380/20000], Loss: (140.19807405321404, tensor(0.0022, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [390/20000], Loss: (140.14207351789778, tensor(0.0022, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [400/20000], Loss: (140.09624621820254, tensor(0.0021, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [410/20000], Loss: (140.05874051873195, tensor(0.0020, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [420/20000], Loss: (140.02804192486053, tensor(0.0020, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [430/20000], Loss: (140.00291169278412, tensor(0.0019, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [440/20000], Loss: (139.98233661821766, tensor(0.0018, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [450/20000], Loss: (139.96548796818698, tensor(0.0018, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [460/20000], Loss: (139.95168789102146, tensor(0.0017, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [470/20000], Loss: (139.94038194282192, tensor(0.0017, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [480/20000], Loss: (139.93111661663573, tensor(0.0016, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [490/20000], Loss: (139.9235209633841, tensor(0.0016, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [500/20000], Loss: (139.91729155946175, tensor(0.0015, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [510/20000], Loss: (139.91218021160608, tensor(0.0014, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [520/20000], Loss: (139.90798390059828, tensor(0.0014, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [530/20000], Loss: (139.9045365561228, tensor(0.0013, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [540/20000], Loss: (139.90170232934523, tensor(0.0013, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [550/20000], Loss: (139.89937009048572, tensor(0.0012, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [560/20000], Loss: (139.8974489283265, tensor(0.0012, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [570/20000], Loss: (139.8958644692099, tensor(0.0011, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [580/20000], Loss: (139.8945558663046, tensor(0.0011, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [590/20000], Loss: (139.89347333709097, tensor(0.0010, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [600/20000], Loss: (139.89257614924063, tensor(0.0010, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [610/20000], Loss: (139.89183097324266, tensor(0.0009, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [620/20000], Loss: (139.89121053499687, tensor(0.0009, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [630/20000], Loss: (139.8906925137549, tensor(0.0009, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [640/20000], Loss: (139.89025864073645, tensor(0.0008, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [650/20000], Loss: (139.88989396188217, tensor(0.0008, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [660/20000], Loss: (139.8895864400131, tensor(0.0007, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [670/20000], Loss: (139.8893260522894, tensor(0.0007, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [680/20000], Loss: (139.88910443800847, tensor(0.0007, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [690/20000], Loss: (139.8889149283905, tensor(0.0006, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [700/20000], Loss: (139.8887518662338, tensor(0.0006, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [710/20000], Loss: (139.88861068827907, tensor(0.0005, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [720/20000], Loss: (139.88848793424592, tensor(0.0005, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [730/20000], Loss: (139.88838161210413, tensor(0.0005, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [740/20000], Loss: (139.8882899454206, tensor(0.0004, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [750/20000], Loss: (139.88820934513743, tensor(0.0004, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [760/20000], Loss: (139.88813720191007, tensor(0.0004, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [770/20000], Loss: (139.8880732531894, tensor(0.0004, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [780/20000], Loss: (139.8880149508409, tensor(0.0003, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [790/20000], Loss: (139.88796247129068, tensor(0.0003, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [800/20000], Loss: (139.887914522559, tensor(0.0003, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [810/20000], Loss: (139.88787017704286, tensor(0.0003, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [820/20000], Loss: (139.8878294079586, tensor(0.0002, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [830/20000], Loss: (139.887792552205, tensor(0.0002, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [840/20000], Loss: (139.88775838091942, tensor(0.0002, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [850/20000], Loss: (139.88772680396767, tensor(0.0002, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [860/20000], Loss: (139.8876976688036, tensor(0.0001, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [870/20000], Loss: (139.88767014684765, tensor(0.0001, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [880/20000], Loss: (139.88764456857166, tensor(9.5412e-05, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [890/20000], Loss: (139.8876203212949, tensor(7.5714e-05, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [900/20000], Loss: (139.8875978599037, tensor(5.6886e-05, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [910/20000], Loss: (139.88757665350792, tensor(3.8539e-05, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [920/20000], Loss: (139.88755840618245, tensor(2.2688e-05, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [930/20000], Loss: (139.88754198806, tensor(7.8787e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [940/20000], Loss: (139.8875353503712, tensor(2.1652e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [950/20000], Loss: (139.88753458292234, tensor(2.1199e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [960/20000], Loss: (139.88753395349082, tensor(2.0810e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [970/20000], Loss: (139.88753343711033, tensor(2.0477e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [980/20000], Loss: (139.8875330134154, tensor(2.0191e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [990/20000], Loss: (139.8875326657182, tensor(1.9946e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1000/20000], Loss: (139.88753238034315, tensor(1.9735e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1010/20000], Loss: (139.887532146082, tensor(1.9554e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1020/20000], Loss: (139.88753195374773, tensor(1.9399e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1030/20000], Loss: (139.8875317958091, tensor(1.9266e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1040/20000], Loss: (139.8875316660918, tensor(1.9152e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1050/20000], Loss: (139.88753155953316, tensor(1.9054e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1060/20000], Loss: (139.88753147198167, tensor(1.8970e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1070/20000], Loss: (139.8875314000324, tensor(1.8898e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1080/20000], Loss: (139.8875313408926, tensor(1.8836e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1090/20000], Loss: (139.88753129227112, tensor(1.8783e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1100/20000], Loss: (139.88753125228823, tensor(1.8738e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1110/20000], Loss: (139.88753121940138, tensor(1.8699e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1120/20000], Loss: (139.88753119234462, tensor(1.8665e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1130/20000], Loss: (139.8875311700788, tensor(1.8636e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1140/20000], Loss: (139.8875311517508, tensor(1.8611e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1150/20000], Loss: (139.88753113666013, tensor(1.8590e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1160/20000], Loss: (139.88753112423143, tensor(1.8572e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1170/20000], Loss: (139.88753111399228, tensor(1.8556e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1180/20000], Loss: (139.88753110555433, tensor(1.8543e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1190/20000], Loss: (139.8875310985986, tensor(1.8532e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1200/20000], Loss: (139.88753109286287, tensor(1.8522e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1210/20000], Loss: (139.88753108813162, tensor(1.8513e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1220/20000], Loss: (139.88753108422762, tensor(1.8506e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1230/20000], Loss: (139.88753108100508, tensor(1.8500e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1240/20000], Loss: (139.88753107834404, tensor(1.8494e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1250/20000], Loss: (139.88753107614596, tensor(1.8490e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1260/20000], Loss: (139.88753107432947, tensor(1.8486e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1270/20000], Loss: (139.88753107282784, tensor(1.8482e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1280/20000], Loss: (139.88753107158593, tensor(1.8480e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1290/20000], Loss: (139.8875310705584, tensor(1.8477e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1300/20000], Loss: (139.88753106970793, tensor(1.8475e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1310/20000], Loss: (139.88753106900361, tensor(1.8473e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1320/20000], Loss: (139.8875310684202, tensor(1.8472e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1330/20000], Loss: (139.8875310679366, tensor(1.8470e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1340/20000], Loss: (139.88753106753558, tensor(1.8469e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1350/20000], Loss: (139.88753106720293, tensor(1.8468e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1360/20000], Loss: (139.88753106692684, tensor(1.8467e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1370/20000], Loss: (139.88753106669753, tensor(1.8466e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1380/20000], Loss: (139.88753106650702, tensor(1.8466e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1390/20000], Loss: (139.88753106634869, tensor(1.8465e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1400/20000], Loss: (139.88753106621698, tensor(1.8465e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1410/20000], Loss: (139.88753106610739, tensor(1.8464e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1420/20000], Loss: (139.8875310660161, tensor(1.8464e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1430/20000], Loss: (139.8875310659401, tensor(1.8464e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1440/20000], Loss: (139.88753106587677, tensor(1.8464e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1450/20000], Loss: (139.88753106582394, tensor(1.8463e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1460/20000], Loss: (139.88753106577985, tensor(1.8463e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1470/20000], Loss: (139.88753106574302, tensor(1.8463e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1480/20000], Loss: (139.88753106571227, tensor(1.8463e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1490/20000], Loss: (139.88753106568657, tensor(1.8463e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1500/20000], Loss: (139.88753106566512, tensor(1.8463e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1510/20000], Loss: (139.8875310656471, tensor(1.8463e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1520/20000], Loss: (139.88753106563206, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1530/20000], Loss: (139.88753106561944, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1540/20000], Loss: (139.88753106560887, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1550/20000], Loss: (139.8875310656, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1560/20000], Loss: (139.88753106559258, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1570/20000], Loss: (139.8875310655863, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1580/20000], Loss: (139.88753106558104, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1590/20000], Loss: (139.88753106557664, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1600/20000], Loss: (139.88753106557294, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1610/20000], Loss: (139.88753106556982, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1620/20000], Loss: (139.88753106556717, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1630/20000], Loss: (139.88753106556496, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1640/20000], Loss: (139.88753106556308, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1650/20000], Loss: (139.88753106556155, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1660/20000], Loss: (139.88753106556015, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1670/20000], Loss: (139.88753106555907, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1680/20000], Loss: (139.88753106555814, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1690/20000], Loss: (139.8875310655573, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1700/20000], Loss: (139.88753106555666, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1710/20000], Loss: (139.8875310655561, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1720/20000], Loss: (139.8875310655556, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1730/20000], Loss: (139.88753106555515, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1740/20000], Loss: (139.8875310655548, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1750/20000], Loss: (139.88753106555453, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1760/20000], Loss: (139.88753106555427, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1770/20000], Loss: (139.88753106555404, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1780/20000], Loss: (139.88753106555387, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1790/20000], Loss: (139.88753106555373, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1800/20000], Loss: (139.88753106555362, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1810/20000], Loss: (139.8875310655535, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1820/20000], Loss: (139.88753106555342, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1830/20000], Loss: (139.88753106555333, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1840/20000], Loss: (139.88753106555325, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1850/20000], Loss: (139.88753106555316, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1860/20000], Loss: (139.88753106555316, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1870/20000], Loss: (139.88753106555308, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1880/20000], Loss: (139.88753106555308, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1890/20000], Loss: (139.88753106555305, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1900/20000], Loss: (139.88753106555302, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1910/20000], Loss: (139.887531065553, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1920/20000], Loss: (139.887531065553, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1930/20000], Loss: (139.88753106555296, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1940/20000], Loss: (139.88753106555296, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1950/20000], Loss: (139.88753106555293, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1960/20000], Loss: (139.88753106555293, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1970/20000], Loss: (139.88753106555293, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1980/20000], Loss: (139.8875310655529, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1990/20000], Loss: (139.8875310655529, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2000/20000], Loss: (139.8875310655529, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2010/20000], Loss: (139.8875310655529, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2020/20000], Loss: (139.8875310655529, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2030/20000], Loss: (139.8875310655529, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2040/20000], Loss: (139.8875310655529, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2050/20000], Loss: (139.8875310655529, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2060/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2070/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2080/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2090/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2100/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2110/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2120/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2130/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2140/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2150/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2160/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2170/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2180/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2190/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2200/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2210/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2220/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2230/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2240/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2250/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2260/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2270/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2280/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2290/20000], Loss: (139.88753106555285, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2300/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2310/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2320/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2330/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2340/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2350/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2360/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2370/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2380/20000], Loss: (139.88753106555285, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2390/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2400/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2410/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2420/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2430/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2440/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2450/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2460/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2470/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2480/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2490/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2500/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2510/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2520/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2530/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2540/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2550/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2560/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2570/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2580/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2590/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2600/20000], Loss: (139.88753106555285, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2610/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2620/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2630/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2640/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2650/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2660/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2670/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2680/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2690/20000], Loss: (139.88753106555285, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2700/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2710/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2720/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2730/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2740/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2750/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2760/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2770/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2780/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2790/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2800/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2810/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2820/20000], Loss: (139.88753106555285, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2830/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2840/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2850/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2860/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2870/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2880/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2890/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2900/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2910/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2920/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2930/20000], Loss: (139.88753106555285, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2940/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2950/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2960/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2970/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2980/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2990/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3000/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3010/20000], Loss: (139.88753106555285, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3020/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3030/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3040/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3050/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3060/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3070/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3080/20000], Loss: (139.88753106555285, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3090/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3100/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3110/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3120/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3130/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3140/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3150/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3160/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3170/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3180/20000], Loss: (139.88753106555285, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3190/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3200/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3210/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3220/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3230/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3240/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3250/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3260/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3270/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3280/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3290/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3300/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3310/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3320/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3330/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3340/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3350/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3360/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3370/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3380/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3390/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3400/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3410/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3420/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3430/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3440/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3450/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3460/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3470/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3480/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3490/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3500/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3510/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3520/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3530/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3540/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3550/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3560/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3570/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3580/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3590/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3600/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3610/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3620/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3630/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3640/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3650/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3660/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3670/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3680/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3690/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3700/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3710/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3720/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3730/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3740/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3750/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3760/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3770/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3780/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3790/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3800/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3810/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3820/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3830/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3840/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3850/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3860/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3870/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3880/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3890/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3900/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3910/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3920/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3930/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3940/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3950/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3960/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3970/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3980/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3990/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4000/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4010/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4020/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4030/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4040/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4050/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4060/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4070/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4080/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4090/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4100/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4110/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4120/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4130/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4140/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4150/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4160/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4170/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4180/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4190/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4200/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4210/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4220/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4230/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4240/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4250/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4260/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4270/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4280/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4290/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4300/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4310/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4320/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4330/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4340/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4350/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4360/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4370/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4380/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4390/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4400/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4410/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4420/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4430/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4440/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4450/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4460/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4470/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4480/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4490/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4500/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4510/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4520/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4530/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4540/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4550/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4560/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4570/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4580/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4590/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4600/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4610/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4620/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4630/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4640/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4650/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4660/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4670/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4680/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4690/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4700/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4710/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4720/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4730/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4740/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4750/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4760/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4770/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4780/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4790/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4800/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4810/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4820/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4830/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4840/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4850/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4860/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4870/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4880/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4890/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4900/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4910/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4920/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4930/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4940/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4950/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4960/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4970/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4980/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4990/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5000/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5010/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5020/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5030/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5040/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5050/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5060/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5070/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5080/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5090/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5100/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5110/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5120/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5130/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5140/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5150/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5160/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5170/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5180/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5190/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5200/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5210/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5220/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5230/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5240/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5250/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5260/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5270/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5280/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5290/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5300/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5310/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5320/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5330/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5340/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5350/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5360/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5370/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5380/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5390/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5400/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5410/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5420/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5430/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5440/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5450/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5460/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5470/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5480/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5490/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5500/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5510/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5520/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5530/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5540/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5550/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5560/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5570/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5580/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5590/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5600/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5610/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5620/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5630/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5640/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5650/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5660/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5670/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5680/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5690/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5700/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5710/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5720/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5730/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5740/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5750/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5760/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5770/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5780/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5790/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5800/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5810/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5820/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5830/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5840/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5850/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5860/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5870/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5880/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5890/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5900/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5910/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5920/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5930/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5940/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5950/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5960/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5970/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5980/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5990/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6000/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6010/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6020/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6030/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6040/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6050/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6060/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6070/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6080/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6090/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6100/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6110/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6120/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6130/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6140/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6150/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6160/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6170/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6180/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6190/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6200/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6210/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6220/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6230/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6240/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6250/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6260/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6270/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6280/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6290/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6300/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6310/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6320/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6330/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6340/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6350/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6360/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6370/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6380/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6390/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6400/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6410/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6420/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6430/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6440/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6450/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6460/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6470/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6480/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6490/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6500/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6510/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6520/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6530/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6540/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6550/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6560/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6570/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6580/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6590/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6600/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6610/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6620/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6630/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6640/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6650/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6660/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6670/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6680/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6690/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6700/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6710/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6720/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6730/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6740/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6750/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6760/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6770/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6780/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6790/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6800/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6810/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6820/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6830/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6840/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6850/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6860/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6870/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6880/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6890/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6900/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6910/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6920/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6930/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6940/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6950/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6960/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6970/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6980/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6990/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7000/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7010/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7020/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7030/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7040/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7050/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7060/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7070/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7080/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7090/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7100/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7110/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7120/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7130/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7140/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7150/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7160/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7170/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7180/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7190/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7200/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7210/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7220/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7230/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7240/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7250/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7260/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7270/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7280/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7290/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7300/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7310/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7320/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7330/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7340/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7350/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7360/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7370/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7380/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7390/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7400/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7410/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7420/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7430/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7440/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7450/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7460/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7470/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7480/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7490/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7500/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7510/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7520/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7530/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7540/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7550/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7560/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7570/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7580/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7590/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7600/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7610/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7620/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7630/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7640/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7650/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7660/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7670/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7680/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7690/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7700/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7710/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7720/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7730/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7740/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7750/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7760/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7770/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7780/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7790/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7800/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7810/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7820/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7830/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7840/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7850/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7860/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7870/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7880/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7890/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7900/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7910/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7920/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7930/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7940/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7950/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7960/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7970/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7980/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7990/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8000/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8010/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8020/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8030/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8040/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8050/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8060/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8070/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8080/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8090/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8100/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8110/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8120/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8130/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8140/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8150/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8160/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8170/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8180/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8190/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8200/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8210/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8220/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8230/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8240/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8250/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8260/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8270/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8280/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8290/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8300/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8310/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8320/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8330/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8340/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8350/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8360/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8370/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8380/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8390/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8400/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8410/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8420/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8430/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8440/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8450/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8460/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8470/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8480/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8490/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8500/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8510/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8520/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8530/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8540/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8550/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8560/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8570/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8580/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8590/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8600/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8610/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8620/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8630/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8640/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8650/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8660/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8670/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8680/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8690/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8700/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8710/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8720/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8730/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8740/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8750/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8760/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8770/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8780/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8790/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8800/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8810/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8820/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8830/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8840/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8850/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8860/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8870/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8880/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8890/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8900/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8910/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8920/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8930/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8940/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8950/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8960/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8970/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8980/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8990/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9000/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9010/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9020/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9030/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9040/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9050/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9060/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9070/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9080/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9090/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9100/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9110/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9120/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9130/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9140/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9150/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9160/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9170/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9180/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9190/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9200/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9210/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9220/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9230/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9240/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9250/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9260/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9270/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9280/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9290/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9300/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9310/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9320/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9330/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9340/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9350/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9360/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9370/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9380/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9390/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9400/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9410/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9420/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9430/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9440/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9450/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9460/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9470/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9480/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9490/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9500/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9510/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9520/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9530/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9540/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9550/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9560/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9570/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9580/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9590/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9600/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9610/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9620/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9630/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9640/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9650/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9660/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9670/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9680/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9690/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9700/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9710/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9720/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9730/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9740/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9750/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9760/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9770/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9780/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9790/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9800/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9810/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9820/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9830/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9840/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9850/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9860/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9870/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9880/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9890/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9900/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9910/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9920/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9930/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9940/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9950/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9960/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9970/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9980/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9990/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10000/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10010/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10020/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10030/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10040/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10050/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10060/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10070/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10080/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10090/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10100/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10110/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10120/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10130/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10140/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10150/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10160/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10170/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10180/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10190/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10200/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10210/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10220/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10230/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10240/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10250/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10260/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10270/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10280/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10290/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10300/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10310/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10320/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10330/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10340/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10350/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10360/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10370/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10380/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10390/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10400/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10410/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10420/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10430/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10440/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10450/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10460/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10470/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10480/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10490/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10500/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10510/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10520/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10530/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10540/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10550/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10560/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10570/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10580/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10590/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10600/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10610/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10620/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10630/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10640/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10650/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10660/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10670/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10680/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10690/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10700/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10710/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10720/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10730/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10740/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10750/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10760/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10770/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10780/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10790/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10800/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10810/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10820/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10830/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10840/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10850/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10860/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10870/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10880/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10890/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10900/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10910/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10920/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10930/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10940/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10950/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10960/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10970/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10980/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10990/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11000/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11010/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11020/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11030/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11040/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11050/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11060/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11070/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11080/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11090/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11100/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11110/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11120/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11130/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11140/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11150/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11160/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11170/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11180/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11190/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11200/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11210/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11220/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11230/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11240/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11250/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11260/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11270/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11280/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11290/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11300/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11310/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11320/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11330/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11340/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11350/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11360/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11370/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11380/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11390/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11400/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11410/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11420/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11430/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11440/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11450/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11460/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11470/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11480/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11490/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11500/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11510/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11520/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11530/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11540/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11550/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11560/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11570/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11580/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11590/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11600/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11610/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11620/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11630/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11640/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11650/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11660/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11670/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11680/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11690/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11700/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11710/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11720/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11730/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11740/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11750/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11760/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11770/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11780/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11790/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11800/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11810/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11820/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11830/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11840/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11850/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11860/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11870/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11880/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11890/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11900/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11910/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11920/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11930/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11940/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11950/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11960/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11970/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11980/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11990/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12000/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12010/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12020/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12030/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12040/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12050/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12060/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12070/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12080/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12090/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12100/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12110/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12120/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12130/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12140/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12150/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12160/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12170/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12180/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12190/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12200/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12210/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12220/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12230/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12240/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12250/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12260/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12270/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12280/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12290/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12300/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12310/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12320/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12330/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12340/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12350/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12360/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12370/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12380/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12390/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12400/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12410/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12420/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12430/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12440/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12450/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12460/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12470/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12480/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12490/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12500/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12510/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12520/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12530/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12540/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12550/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12560/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12570/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12580/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12590/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12600/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12610/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12620/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12630/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12640/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12650/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12660/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12670/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12680/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12690/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12700/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12710/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12720/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12730/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12740/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12750/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12760/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12770/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12780/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12790/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12800/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12810/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12820/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12830/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12840/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12850/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12860/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12870/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12880/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12890/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12900/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12910/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12920/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12930/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12940/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12950/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12960/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12970/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12980/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12990/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13000/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13010/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13020/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13030/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13040/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13050/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13060/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13070/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13080/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13090/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13100/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13110/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13120/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13130/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13140/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13150/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13160/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13170/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13180/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13190/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13200/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13210/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13220/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13230/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13240/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13250/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13260/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13270/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13280/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13290/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13300/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13310/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13320/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13330/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13340/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13350/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13360/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13370/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13380/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13390/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13400/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13410/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13420/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13430/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13440/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13450/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13460/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13470/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13480/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13490/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13500/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13510/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13520/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13530/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13540/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13550/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13560/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13570/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13580/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13590/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13600/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13610/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13620/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13630/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13640/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13650/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13660/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13670/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13680/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13690/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13700/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13710/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13720/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13730/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13740/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13750/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13760/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13770/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13780/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13790/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13800/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13810/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13820/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13830/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13840/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13850/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13860/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13870/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13880/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13890/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13900/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13910/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13920/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13930/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13940/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13950/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13960/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13970/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13980/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13990/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14000/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14010/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14020/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14030/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14040/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14050/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14060/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14070/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14080/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14090/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14100/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14110/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14120/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14130/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14140/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14150/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14160/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14170/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14180/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14190/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14200/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14210/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14220/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14230/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14240/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14250/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14260/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14270/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14280/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14290/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14300/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14310/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14320/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14330/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14340/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14350/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14360/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14370/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14380/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14390/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14400/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14410/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14420/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14430/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14440/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14450/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14460/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14470/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14480/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14490/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14500/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14510/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14520/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14530/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14540/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14550/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14560/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14570/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14580/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14590/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14600/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14610/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14620/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14630/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14640/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14650/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14660/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14670/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14680/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14690/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14700/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14710/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14720/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14730/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14740/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14750/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14760/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14770/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14780/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14790/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14800/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14810/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14820/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14830/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14840/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14850/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14860/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14870/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14880/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14890/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14900/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14910/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14920/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14930/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14940/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14950/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14960/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14970/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14980/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14990/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15000/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15010/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15020/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15030/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15040/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15050/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15060/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15070/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15080/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15090/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15100/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15110/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15120/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15130/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15140/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15150/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15160/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15170/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15180/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15190/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15200/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15210/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15220/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15230/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15240/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15250/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15260/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15270/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15280/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15290/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15300/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15310/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15320/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15330/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15340/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15350/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15360/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15370/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15380/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15390/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15400/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15410/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15420/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15430/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15440/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15450/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15460/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15470/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15480/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15490/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15500/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15510/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15520/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15530/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15540/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15550/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15560/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15570/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15580/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15590/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15600/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15610/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15620/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15630/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15640/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15650/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15660/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15670/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15680/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15690/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15700/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15710/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15720/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15730/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15740/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15750/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15760/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15770/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15780/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15790/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15800/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15810/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15820/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15830/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15840/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15850/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15860/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15870/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15880/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15890/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15900/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15910/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15920/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15930/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15940/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15950/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15960/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15970/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15980/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15990/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16000/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16010/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16020/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16030/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16040/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16050/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16060/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16070/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16080/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16090/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16100/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16110/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16120/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16130/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16140/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16150/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16160/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16170/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16180/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16190/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16200/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16210/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16220/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16230/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16240/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16250/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16260/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16270/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16280/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16290/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16300/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16310/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16320/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16330/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16340/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16350/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16360/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16370/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16380/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16390/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16400/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16410/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16420/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16430/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16440/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16450/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16460/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16470/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16480/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16490/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16500/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16510/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16520/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16530/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16540/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16550/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16560/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16570/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16580/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16590/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16600/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16610/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16620/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16630/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16640/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16650/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16660/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16670/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16680/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16690/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16700/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16710/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16720/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16730/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16740/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16750/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16760/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16770/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16780/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16790/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16800/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16810/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16820/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16830/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16840/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16850/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16860/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16870/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16880/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16890/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16900/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16910/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16920/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16930/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16940/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16950/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16960/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16970/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16980/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16990/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17000/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17010/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17020/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17030/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17040/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17050/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17060/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17070/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17080/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17090/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17100/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17110/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17120/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17130/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17140/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17150/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17160/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17170/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17180/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17190/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17200/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17210/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17220/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17230/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17240/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17250/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17260/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17270/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17280/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17290/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17300/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17310/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17320/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17330/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17340/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17350/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17360/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17370/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17380/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17390/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17400/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17410/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17420/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17430/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17440/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17450/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17460/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17470/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17480/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17490/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17500/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17510/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17520/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17530/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17540/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17550/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17560/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17570/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17580/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17590/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17600/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17610/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17620/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17630/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17640/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17650/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17660/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17670/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17680/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17690/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17700/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17710/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17720/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17730/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17740/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17750/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17760/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17770/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17780/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17790/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17800/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17810/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17820/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17830/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17840/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17850/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17860/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17870/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17880/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17890/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17900/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17910/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17920/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17930/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17940/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17950/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17960/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17970/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17980/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17990/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18000/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18010/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18020/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18030/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18040/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18050/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18060/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18070/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18080/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18090/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18100/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18110/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18120/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18130/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18140/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18150/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18160/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18170/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18180/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18190/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18200/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18210/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18220/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18230/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18240/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18250/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18260/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18270/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18280/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18290/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18300/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18310/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18320/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18330/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18340/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18350/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18360/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18370/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18380/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18390/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18400/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18410/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18420/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18430/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18440/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18450/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18460/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18470/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18480/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18490/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18500/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18510/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18520/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18530/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18540/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18550/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18560/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18570/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18580/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18590/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18600/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18610/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18620/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18630/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18640/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18650/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18660/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18670/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18680/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18690/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18700/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18710/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18720/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18730/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18740/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18750/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18760/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18770/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18780/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18790/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18800/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18810/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18820/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18830/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18840/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18850/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18860/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18870/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18880/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18890/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18900/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18910/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18920/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18930/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18940/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18950/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18960/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18970/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18980/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18990/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19000/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19010/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19020/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19030/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19040/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19050/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19060/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19070/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19080/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19090/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19100/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19110/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19120/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19130/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19140/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19150/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19160/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19170/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19180/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19190/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19200/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19210/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19220/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19230/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19240/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19250/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19260/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19270/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19280/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19290/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19300/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19310/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19320/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19330/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19340/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19350/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19360/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19370/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19380/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19390/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19400/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19410/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19420/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19430/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19440/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19450/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19460/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19470/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19480/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19490/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19500/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19510/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19520/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19530/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19540/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19550/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19560/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19570/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19580/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19590/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19600/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19610/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19620/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19630/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19640/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19650/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19660/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19670/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19680/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19690/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19700/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19710/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19720/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19730/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19740/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19750/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19760/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19770/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19780/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19790/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19800/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19810/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19820/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19830/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19840/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19850/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19860/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19870/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19880/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19890/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19900/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19910/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19920/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19930/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19940/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19950/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19960/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19970/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19980/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19990/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [20000/20000], Loss: (139.88753106555288, tensor(1.8462e-06, dtype=torch.float64, grad_fn=<MulBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse(model2.predict(xtest),ytest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuM9Mk_ss7Hl",
        "outputId": "07b7137a-7fc7-4750-8218-72fc3ec88c3d"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "275.0824991880445"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conc.iloc[:, :-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "s_MXnWTKUUUj",
        "outputId": "0e426e51-5a1d-4a0d-d636-f6b2d83d3054"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      cement   slag    ash  water  superplastic  coarseagg  fineagg  age\n",
              "0      540.0    0.0    0.0  162.0           2.5     1040.0    676.0   28\n",
              "1      540.0    0.0    0.0  162.0           2.5     1055.0    676.0   28\n",
              "2      332.5  142.5    0.0  228.0           0.0      932.0    594.0  270\n",
              "3      332.5  142.5    0.0  228.0           0.0      932.0    594.0  365\n",
              "4      198.6  132.4    0.0  192.0           0.0      978.4    825.5  360\n",
              "...      ...    ...    ...    ...           ...        ...      ...  ...\n",
              "1025   276.4  116.0   90.3  179.6           8.9      870.1    768.3   28\n",
              "1026   322.2    0.0  115.6  196.0          10.4      817.9    813.4   28\n",
              "1027   148.5  139.4  108.6  192.7           6.1      892.4    780.0   28\n",
              "1028   159.1  186.7    0.0  175.6          11.3      989.6    788.9   28\n",
              "1029   260.9  100.5   78.3  200.6           8.6      864.5    761.5   28\n",
              "\n",
              "[1030 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3d9ea6bb-b635-4d70-8d42-cec018a04bbe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cement</th>\n",
              "      <th>slag</th>\n",
              "      <th>ash</th>\n",
              "      <th>water</th>\n",
              "      <th>superplastic</th>\n",
              "      <th>coarseagg</th>\n",
              "      <th>fineagg</th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1055.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>198.6</td>\n",
              "      <td>132.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>978.4</td>\n",
              "      <td>825.5</td>\n",
              "      <td>360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1025</th>\n",
              "      <td>276.4</td>\n",
              "      <td>116.0</td>\n",
              "      <td>90.3</td>\n",
              "      <td>179.6</td>\n",
              "      <td>8.9</td>\n",
              "      <td>870.1</td>\n",
              "      <td>768.3</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1026</th>\n",
              "      <td>322.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>115.6</td>\n",
              "      <td>196.0</td>\n",
              "      <td>10.4</td>\n",
              "      <td>817.9</td>\n",
              "      <td>813.4</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1027</th>\n",
              "      <td>148.5</td>\n",
              "      <td>139.4</td>\n",
              "      <td>108.6</td>\n",
              "      <td>192.7</td>\n",
              "      <td>6.1</td>\n",
              "      <td>892.4</td>\n",
              "      <td>780.0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1028</th>\n",
              "      <td>159.1</td>\n",
              "      <td>186.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>175.6</td>\n",
              "      <td>11.3</td>\n",
              "      <td>989.6</td>\n",
              "      <td>788.9</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1029</th>\n",
              "      <td>260.9</td>\n",
              "      <td>100.5</td>\n",
              "      <td>78.3</td>\n",
              "      <td>200.6</td>\n",
              "      <td>8.6</td>\n",
              "      <td>864.5</td>\n",
              "      <td>761.5</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1030 rows × 8 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d9ea6bb-b635-4d70-8d42-cec018a04bbe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3d9ea6bb-b635-4d70-8d42-cec018a04bbe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3d9ea6bb-b635-4d70-8d42-cec018a04bbe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ef628e13-039b-435b-b1ea-a568b1ca9808\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ef628e13-039b-435b-b1ea-a568b1ca9808')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ef628e13-039b-435b-b1ea-a568b1ca9808 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"conc\",\n  \"rows\": 1030,\n  \"fields\": [\n    {\n      \"column\": \"cement\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 104.50636449481532,\n        \"min\": 102.0,\n        \"max\": 540.0,\n        \"num_unique_values\": 278,\n        \"samples\": [\n          337.9,\n          290.2,\n          262.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 86.27934174810584,\n        \"min\": 0.0,\n        \"max\": 359.4,\n        \"num_unique_values\": 185,\n        \"samples\": [\n          94.7,\n          119.0,\n          136.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ash\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 63.99700415268765,\n        \"min\": 0.0,\n        \"max\": 200.1,\n        \"num_unique_values\": 156,\n        \"samples\": [\n          98.0,\n          142.0,\n          195.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"water\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 21.35421856503247,\n        \"min\": 121.8,\n        \"max\": 247.0,\n        \"num_unique_values\": 195,\n        \"samples\": [\n          195.4,\n          183.8,\n          127.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"superplastic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.97384139248552,\n        \"min\": 0.0,\n        \"max\": 32.2,\n        \"num_unique_values\": 111,\n        \"samples\": [\n          15.0,\n          28.2,\n          16.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"coarseagg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 77.75395396672077,\n        \"min\": 801.0,\n        \"max\": 1145.0,\n        \"num_unique_values\": 284,\n        \"samples\": [\n          852.1,\n          913.9,\n          914.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fineagg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 80.17598014240437,\n        \"min\": 594.0,\n        \"max\": 992.6,\n        \"num_unique_values\": 302,\n        \"samples\": [\n          710.0,\n          695.4,\n          769.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 63,\n        \"min\": 1,\n        \"max\": 365,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          91,\n          100,\n          28\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##By looking at the loss above, we can see that SCAD is providing much better results than sqrtLasso and elastic net. In this example I scaled the data using standard scaler and did a tts\n",
        "\n",
        "# By looking at the results from getting the weights of the model, we can also see which features are likely to be impactful to the output, which are: 1,2,3,4,5, and 8. This is because they are further away from zero."
      ],
      "metadata": {
        "id": "tVw2dqJGVlJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## q2\n",
        "        2. (4 points) Based on the simulation design explained in class, generate 500 data sets where the input features have a strong correlation structure (you may consider a 0.9) and apply ElasticNet, SqrtLasso and SCAD to check which method produces the best approximation of an ideal solution, such as a \"betastar\" you design with a sparsity pattern.\n",
        "\n",
        "\n",
        "\n",
        "         3. (1 point) Host your project in your GitHub space."
      ],
      "metadata": {
        "id": "n6pg3PxMkftg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_correlated_features(num_samples,p,rho):\n",
        "  vcor = []\n",
        "  for i in range(p):\n",
        "    vcor.append(rho**i)\n",
        "  r = toeplitz(vcor)\n",
        "  mu = np.repeat(0,p)\n",
        "  x = np.random.multivariate_normal(mu, r, size=num_samples)\n",
        "  return x"
      ],
      "metadata": {
        "id": "wFY__jaskUNT"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rho =0.9 # closer to one = higher corr\n",
        "p = 20 #features\n",
        "n = 500\n",
        "vcor = []\n",
        "for i in range(p):\n",
        "  vcor.append(rho**i)"
      ],
      "metadata": {
        "id": "UMgSOqUAk69A"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = make_correlated_features(n,p,rho)"
      ],
      "metadata": {
        "id": "YWcChq35lHor"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(x)"
      ],
      "metadata": {
        "id": "MlGO2AO4ohZl"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beta =np.array([-1,2,3,0,0,0,0,2,-1,4])\n",
        "beta = beta.reshape(-1,1)\n",
        "betastar = np.concatenate([beta,np.repeat(0,p-len(beta)).reshape(-1,1)],axis=0)"
      ],
      "metadata": {
        "id": "grYNvrUQlKQO"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = x@betastar + 1.5*np.random.normal(size=(n,1))"
      ],
      "metadata": {
        "id": "ukQJrf9nlMnf"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(x,device=device)\n",
        "y = torch.tensor(y,device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob4H9azCox1u",
        "outputId": "82ea8231-d7a2-44e5-b5df-e4a04767c58b"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-143-6397e7363bf8>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x,device=device)\n",
            "<ipython-input-143-6397e7363bf8>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y = torch.tensor(y,device=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_scad = SCAD(input_size=x.shape[1])"
      ],
      "metadata": {
        "id": "iK0kHgM0mo1Q"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_scad.fit(x,y,num_epochs=20000,learning_rate=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjePd2Dhoaqi",
        "outputId": "44202f22-4e9b-473b-a2cd-4eb5d1939ca1"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/20000], Loss: (3.970210735046212, tensor(0.5940, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [200/20000], Loss: (3.0097428616869437, tensor(0.5631, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [300/20000], Loss: (2.713854042146505, tensor(0.4829, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [400/20000], Loss: (2.498473498005278, tensor(0.4579, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [500/20000], Loss: (2.3261197343149154, tensor(0.4484, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [600/20000], Loss: (2.12067892129961, tensor(0.3818, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [700/20000], Loss: (1.9878870600610798, tensor(0.3644, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [800/20000], Loss: (1.8876137933064547, tensor(0.3544, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [900/20000], Loss: (1.8317304156186325, tensor(0.3764, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1000/20000], Loss: (1.765545973248769, tensor(0.3814, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1100/20000], Loss: (1.6791462111043454, tensor(0.3564, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1200/20000], Loss: (1.6035745209964538, tensor(0.3325, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1300/20000], Loss: (1.5513896009304675, tensor(0.3238, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1400/20000], Loss: (1.4908677699441608, tensor(0.2993, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1500/20000], Loss: (1.4661431566428063, tensor(0.2956, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1600/20000], Loss: (1.4622494993339534, tensor(0.3079, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1700/20000], Loss: (1.446804125805206, tensor(0.3054, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1800/20000], Loss: (1.4038691289917917, tensor(0.2729, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [1900/20000], Loss: (1.3623070891416293, tensor(0.2427, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2000/20000], Loss: (1.3546849612155987, tensor(0.2430, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2100/20000], Loss: (1.3495396518476828, tensor(0.2433, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2200/20000], Loss: (1.3459507830812922, tensor(0.2437, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2300/20000], Loss: (1.3280307230520618, tensor(0.2290, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2400/20000], Loss: (1.3263324330958683, tensor(0.2299, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2500/20000], Loss: (1.325091897781907, tensor(0.2309, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2600/20000], Loss: (1.3242344318643051, tensor(0.2318, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2700/20000], Loss: (1.3235545486445048, tensor(0.2326, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2800/20000], Loss: (1.3231955874231698, tensor(0.2334, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [2900/20000], Loss: (1.3228725030368071, tensor(0.2339, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3000/20000], Loss: (1.3226740806908992, tensor(0.2344, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3100/20000], Loss: (1.3225890463159555, tensor(0.2349, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3200/20000], Loss: (1.3225004853277915, tensor(0.2352, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3300/20000], Loss: (1.3224500269013382, tensor(0.2355, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3400/20000], Loss: (1.3224397129202758, tensor(0.2356, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3500/20000], Loss: (1.2877545723488828, tensor(0.2065, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3600/20000], Loss: (1.2853530837803502, tensor(0.2064, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3700/20000], Loss: (1.2841899950980409, tensor(0.2061, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3800/20000], Loss: (1.2835815903137673, tensor(0.2055, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [3900/20000], Loss: (1.2833329049575533, tensor(0.2054, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4000/20000], Loss: (1.2832032198361478, tensor(0.2051, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4100/20000], Loss: (1.2831287588358904, tensor(0.2050, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4200/20000], Loss: (1.2830993603235412, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4300/20000], Loss: (1.283096452089357, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4400/20000], Loss: (1.2831473758614365, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4500/20000], Loss: (1.2830933138928529, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4600/20000], Loss: (1.2830951504646757, tensor(0.2043, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4700/20000], Loss: (1.2831291401707932, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4800/20000], Loss: (1.2830999695403145, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [4900/20000], Loss: (1.2831986097315364, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5000/20000], Loss: (1.2831115833509907, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5100/20000], Loss: (1.2831426684808647, tensor(0.2043, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5200/20000], Loss: (1.2830944970866494, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5300/20000], Loss: (1.283105800973217, tensor(0.2043, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5400/20000], Loss: (1.2830728895393635, tensor(0.2043, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5500/20000], Loss: (1.2831205833377943, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5600/20000], Loss: (1.2832567697698691, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5700/20000], Loss: (1.2830949482431793, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5800/20000], Loss: (1.2831382386432109, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [5900/20000], Loss: (1.2831345333731619, tensor(0.2042, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6000/20000], Loss: (1.283126682223685, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6100/20000], Loss: (1.2831806219846755, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6200/20000], Loss: (1.283240536730399, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6300/20000], Loss: (1.2830864234828718, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6400/20000], Loss: (1.2831141722836041, tensor(0.2046, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6500/20000], Loss: (1.28317836187361, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6600/20000], Loss: (1.2831853898104768, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6700/20000], Loss: (1.2832812842166381, tensor(0.2049, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6800/20000], Loss: (1.2833099829736452, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [6900/20000], Loss: (1.2831283334409656, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7000/20000], Loss: (1.2831128859873386, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7100/20000], Loss: (1.2831484536054336, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7200/20000], Loss: (1.2831202370507169, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7300/20000], Loss: (1.2832134249574492, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7400/20000], Loss: (1.2832669189171981, tensor(0.2049, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7500/20000], Loss: (1.28320318940627, tensor(0.2041, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7600/20000], Loss: (1.283261061587805, tensor(0.2048, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7700/20000], Loss: (1.2832528538174792, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7800/20000], Loss: (1.2831531892708858, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [7900/20000], Loss: (1.283266100113004, tensor(0.2049, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8000/20000], Loss: (1.283171270382518, tensor(0.2043, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8100/20000], Loss: (1.2834490449622578, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8200/20000], Loss: (1.2832226112365435, tensor(0.2040, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8300/20000], Loss: (1.2832304306459525, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8400/20000], Loss: (1.28315418195321, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8500/20000], Loss: (1.2833204654647106, tensor(0.2051, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8600/20000], Loss: (1.2986278546746965, tensor(0.2202, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8700/20000], Loss: (1.2987109322059311, tensor(0.2201, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8800/20000], Loss: (1.2832559182306416, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [8900/20000], Loss: (1.2832206675413473, tensor(0.2046, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9000/20000], Loss: (1.2831734949224043, tensor(0.2046, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9100/20000], Loss: (1.283207794581795, tensor(0.2041, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9200/20000], Loss: (1.2833258833267798, tensor(0.2049, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9300/20000], Loss: (1.2833830328356595, tensor(0.2049, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9400/20000], Loss: (1.283347139615469, tensor(0.2040, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9500/20000], Loss: (1.2832967238289235, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9600/20000], Loss: (1.2832112084567995, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9700/20000], Loss: (1.2833370517037868, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9800/20000], Loss: (1.2986514027527551, tensor(0.2200, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [9900/20000], Loss: (1.2832332050998767, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10000/20000], Loss: (1.2832455015131654, tensor(0.2048, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10100/20000], Loss: (1.2833367456979778, tensor(0.2050, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10200/20000], Loss: (1.2831960990073392, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10300/20000], Loss: (1.2985846939963182, tensor(0.2201, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10400/20000], Loss: (1.2987322581108585, tensor(0.2206, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10500/20000], Loss: (1.283359158670285, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10600/20000], Loss: (1.2987551025647175, tensor(0.2207, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10700/20000], Loss: (1.2986613439169585, tensor(0.2198, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10800/20000], Loss: (1.2988162329834798, tensor(0.2202, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [10900/20000], Loss: (1.2831857850790576, tensor(0.2043, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11000/20000], Loss: (1.2834047149237715, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11100/20000], Loss: (1.2832629362108812, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11200/20000], Loss: (1.2832195062212421, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11300/20000], Loss: (1.2833528840258386, tensor(0.2048, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11400/20000], Loss: (1.2833767819117008, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11500/20000], Loss: (1.2833925199790046, tensor(0.2051, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11600/20000], Loss: (1.2832084539715767, tensor(0.2042, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11700/20000], Loss: (1.298647071399085, tensor(0.2201, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11800/20000], Loss: (1.2832351246750768, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [11900/20000], Loss: (1.2833731463928502, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12000/20000], Loss: (1.298565344193221, tensor(0.2201, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12100/20000], Loss: (1.2833855167447243, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12200/20000], Loss: (1.2832676337501634, tensor(0.2049, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12300/20000], Loss: (1.2832126620993622, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12400/20000], Loss: (1.2831978492255547, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12500/20000], Loss: (1.2833472699724275, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12600/20000], Loss: (1.31410248834009, tensor(0.2358, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12700/20000], Loss: (1.2832242417467692, tensor(0.2048, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12800/20000], Loss: (1.2832207485242273, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [12900/20000], Loss: (1.2985413375035666, tensor(0.2196, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13000/20000], Loss: (1.283288861780495, tensor(0.2042, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13100/20000], Loss: (1.2831876876097676, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13200/20000], Loss: (1.2985278754801184, tensor(0.2200, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13300/20000], Loss: (1.2831763273756582, tensor(0.2048, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13400/20000], Loss: (1.2832212026208174, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13500/20000], Loss: (1.2988902208795563, tensor(0.2201, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13600/20000], Loss: (1.2832176982348962, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13700/20000], Loss: (1.2833220624363, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13800/20000], Loss: (1.2985651892692673, tensor(0.2199, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [13900/20000], Loss: (1.2831923184207925, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14000/20000], Loss: (1.2831019813927151, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14100/20000], Loss: (1.2985625958810587, tensor(0.2203, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14200/20000], Loss: (1.283195640305248, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14300/20000], Loss: (1.283221995193903, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14400/20000], Loss: (1.2986318843732703, tensor(0.2203, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14500/20000], Loss: (1.2833537072290324, tensor(0.2051, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14600/20000], Loss: (1.2833284792334578, tensor(0.2042, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14700/20000], Loss: (1.298740408405859, tensor(0.2201, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14800/20000], Loss: (1.2832344966972082, tensor(0.2048, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [14900/20000], Loss: (1.2832030121049, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15000/20000], Loss: (1.2985843799426768, tensor(0.2204, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15100/20000], Loss: (1.2832533952857583, tensor(0.2048, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15200/20000], Loss: (1.2832855554519864, tensor(0.2042, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15300/20000], Loss: (1.283198057520585, tensor(0.2042, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15400/20000], Loss: (1.2984457797675302, tensor(0.2200, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15500/20000], Loss: (1.2833255435963336, tensor(0.2053, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15600/20000], Loss: (1.283265633695448, tensor(0.2048, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15700/20000], Loss: (1.2832750505398676, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15800/20000], Loss: (1.283204326894092, tensor(0.2042, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [15900/20000], Loss: (1.2833431383107765, tensor(0.2046, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16000/20000], Loss: (1.298637083490496, tensor(0.2200, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16100/20000], Loss: (1.2833153727479645, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16200/20000], Loss: (1.283314520645151, tensor(0.2046, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16300/20000], Loss: (1.2985727317921276, tensor(0.2203, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16400/20000], Loss: (1.2832739015957841, tensor(0.2048, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16500/20000], Loss: (1.283345661096848, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16600/20000], Loss: (1.283248357744627, tensor(0.2045, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16700/20000], Loss: (1.2985672491296325, tensor(0.2198, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16800/20000], Loss: (1.2832224076464074, tensor(0.2042, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [16900/20000], Loss: (1.283220224437302, tensor(0.2046, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17000/20000], Loss: (1.2987416395915146, tensor(0.2201, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17100/20000], Loss: (1.2833539051360503, tensor(0.2040, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17200/20000], Loss: (1.2833138406042084, tensor(0.2048, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17300/20000], Loss: (1.2986321823057256, tensor(0.2200, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17400/20000], Loss: (1.298682012590601, tensor(0.2200, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17500/20000], Loss: (1.2831463116084252, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17600/20000], Loss: (1.2831912639699048, tensor(0.2048, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17700/20000], Loss: (1.2986004884942648, tensor(0.2208, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17800/20000], Loss: (1.28337398799878, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [17900/20000], Loss: (1.2832322392786815, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18000/20000], Loss: (1.2986813243503343, tensor(0.2204, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18100/20000], Loss: (1.2832578363679874, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18200/20000], Loss: (1.2833488536448892, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18300/20000], Loss: (1.2985678396520044, tensor(0.2198, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18400/20000], Loss: (1.2833069653967668, tensor(0.2046, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18500/20000], Loss: (1.2987347589704712, tensor(0.2201, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18600/20000], Loss: (1.2984859039392904, tensor(0.2198, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18700/20000], Loss: (1.2832812110320162, tensor(0.2047, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18800/20000], Loss: (1.2832562493576032, tensor(0.2046, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [18900/20000], Loss: (1.298615831085366, tensor(0.2202, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19000/20000], Loss: (1.2834702820065509, tensor(0.2049, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19100/20000], Loss: (1.2987924619858409, tensor(0.2204, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19200/20000], Loss: (1.2832881683497612, tensor(0.2046, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19300/20000], Loss: (1.2831634639761955, tensor(0.2043, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19400/20000], Loss: (1.2833148636657508, tensor(0.2050, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19500/20000], Loss: (1.2986024624230352, tensor(0.2204, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19600/20000], Loss: (1.2832974435366775, tensor(0.2044, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19700/20000], Loss: (1.2832316324490562, tensor(0.2048, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19800/20000], Loss: (1.314068228883102, tensor(0.2359, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [19900/20000], Loss: (1.2833615799256466, tensor(0.2042, dtype=torch.float64, grad_fn=<MeanBackward0>))\n",
            "Epoch [20000/20000], Loss: (1.2832063189251, tensor(0.2043, dtype=torch.float64, grad_fn=<MeanBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_en = ElasticNet(input_size=x.shape[1],alpha=0.01,l1_ratio=0.5)"
      ],
      "metadata": {
        "id": "ucO_-3tnlRXN"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_en.fit(x,y,num_epochs=20000,learning_rate=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRe_i1NRoJsg",
        "outputId": "687d2978-a641-4406-c272-72536463d610"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/20000], Loss: (1.2036475423759185, tensor(0.0859, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [20/20000], Loss: (1.203112475424171, tensor(0.0860, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [30/20000], Loss: (1.2025848792632357, tensor(0.0860, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [40/20000], Loss: (1.202064636856501, tensor(0.0861, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [50/20000], Loss: (1.201551455084848, tensor(0.0862, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [60/20000], Loss: (1.201045579623426, tensor(0.0863, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [70/20000], Loss: (1.200546724573865, tensor(0.0864, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [80/20000], Loss: (1.2000547817022766, tensor(0.0865, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [90/20000], Loss: (1.1995694677752309, tensor(0.0866, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [100/20000], Loss: (1.199091029975385, tensor(0.0867, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [110/20000], Loss: (1.1986191936214312, tensor(0.0867, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [120/20000], Loss: (1.1981536835911648, tensor(0.0868, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [130/20000], Loss: (1.1976947422462245, tensor(0.0869, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [140/20000], Loss: (1.197242104573453, tensor(0.0870, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [150/20000], Loss: (1.1967955324893436, tensor(0.0871, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [160/20000], Loss: (1.1963551895065192, tensor(0.0871, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [170/20000], Loss: (1.1959208941371258, tensor(0.0872, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [180/20000], Loss: (1.195492366033923, tensor(0.0873, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [190/20000], Loss: (1.1950698425075748, tensor(0.0874, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [200/20000], Loss: (1.1946530745226343, tensor(0.0875, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [210/20000], Loss: (1.1942418084414672, tensor(0.0875, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [220/20000], Loss: (1.193836290719925, tensor(0.0876, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [230/20000], Loss: (1.1934361433451275, tensor(0.0877, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [240/20000], Loss: (1.193041509265231, tensor(0.0878, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [250/20000], Loss: (1.1926522467305554, tensor(0.0878, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [260/20000], Loss: (1.192268080623175, tensor(0.0879, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [270/20000], Loss: (1.1918892547359206, tensor(0.0880, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [280/20000], Loss: (1.1915153753358434, tensor(0.0881, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [290/20000], Loss: (1.1911466797402295, tensor(0.0881, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [300/20000], Loss: (1.1907828106249538, tensor(0.0882, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [310/20000], Loss: (1.1904239248094952, tensor(0.0883, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [320/20000], Loss: (1.1900698315824676, tensor(0.0883, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [330/20000], Loss: (1.1897204098464662, tensor(0.0884, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [340/20000], Loss: (1.189375770883216, tensor(0.0885, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [350/20000], Loss: (1.1890355762871914, tensor(0.0885, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [360/20000], Loss: (1.1887000733775688, tensor(0.0886, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [370/20000], Loss: (1.1883688826051795, tensor(0.0887, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [380/20000], Loss: (1.188042250158018, tensor(0.0888, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [390/20000], Loss: (1.1877198035411203, tensor(0.0888, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [400/20000], Loss: (1.1874017870297826, tensor(0.0889, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [410/20000], Loss: (1.1870878323718912, tensor(0.0889, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [420/20000], Loss: (1.1867781841661322, tensor(0.0890, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [430/20000], Loss: (1.1864724777471152, tensor(0.0891, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [440/20000], Loss: (1.1861709592868526, tensor(0.0891, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [450/20000], Loss: (1.1858732646335093, tensor(0.0892, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [460/20000], Loss: (1.185579618327461, tensor(0.0893, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [470/20000], Loss: (1.185289732566535, tensor(0.0893, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [480/20000], Loss: (1.185003698853011, tensor(0.0894, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [490/20000], Loss: (1.1847214358566491, tensor(0.0895, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [500/20000], Loss: (1.1844427930960102, tensor(0.0895, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [510/20000], Loss: (1.1841679423757001, tensor(0.0896, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [520/20000], Loss: (1.1838965427168502, tensor(0.0896, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [530/20000], Loss: (1.1836288337515597, tensor(0.0897, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [540/20000], Loss: (1.1833644735709876, tensor(0.0898, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [550/20000], Loss: (1.1831037062091156, tensor(0.0898, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [560/20000], Loss: (1.1828461868074458, tensor(0.0899, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [570/20000], Loss: (1.1825921453106596, tensor(0.0899, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [580/20000], Loss: (1.18234129567673, tensor(0.0900, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [590/20000], Loss: (1.1820936853189647, tensor(0.0900, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [600/20000], Loss: (1.1818494233403574, tensor(0.0901, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [610/20000], Loss: (1.1816081846336615, tensor(0.0902, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [620/20000], Loss: (1.1813702093910747, tensor(0.0902, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [630/20000], Loss: (1.1811351660430076, tensor(0.0903, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [640/20000], Loss: (1.1809033019112296, tensor(0.0903, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [650/20000], Loss: (1.1806742815130855, tensor(0.0904, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [660/20000], Loss: (1.1804482453296092, tensor(0.0904, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [670/20000], Loss: (1.1802251950437423, tensor(0.0905, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [680/20000], Loss: (1.180004902784629, tensor(0.0905, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [690/20000], Loss: (1.1797875809393814, tensor(0.0906, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [700/20000], Loss: (1.1795729044580174, tensor(0.0906, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [710/20000], Loss: (1.1793611234138572, tensor(0.0907, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [720/20000], Loss: (1.1791519081429005, tensor(0.0907, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [730/20000], Loss: (1.17894537140721, tensor(0.0908, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [740/20000], Loss: (1.1787416143622196, tensor(0.0908, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [750/20000], Loss: (1.1785403160915537, tensor(0.0909, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [760/20000], Loss: (1.1783417280739115, tensor(0.0909, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [770/20000], Loss: (1.1781455247503119, tensor(0.0910, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [780/20000], Loss: (1.177951896080343, tensor(0.0910, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [790/20000], Loss: (1.1777607183937504, tensor(0.0911, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [800/20000], Loss: (1.17757190377118, tensor(0.0911, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [810/20000], Loss: (1.1773856259580713, tensor(0.0912, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [820/20000], Loss: (1.1772015676822953, tensor(0.0912, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [830/20000], Loss: (1.177019984376262, tensor(0.0913, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [840/20000], Loss: (1.1768405535773347, tensor(0.0913, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [850/20000], Loss: (1.176663396839106, tensor(0.0914, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [860/20000], Loss: (1.176488611231584, tensor(0.0914, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [870/20000], Loss: (1.1763158934769944, tensor(0.0915, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [880/20000], Loss: (1.1761454961172295, tensor(0.0915, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [890/20000], Loss: (1.1759770998395933, tensor(0.0916, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [900/20000], Loss: (1.1758109052895924, tensor(0.0916, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [910/20000], Loss: (1.1756467792142251, tensor(0.0916, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [920/20000], Loss: (1.175484654365814, tensor(0.0917, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [930/20000], Loss: (1.1753247051498765, tensor(0.0917, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [940/20000], Loss: (1.1751666191703785, tensor(0.0918, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [950/20000], Loss: (1.1750106576791008, tensor(0.0918, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [960/20000], Loss: (1.1748565027212678, tensor(0.0919, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [970/20000], Loss: (1.1747043059592108, tensor(0.0919, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [980/20000], Loss: (1.1745540944888284, tensor(0.0919, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [990/20000], Loss: (1.1744056456886882, tensor(0.0920, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1000/20000], Loss: (1.1742591900357673, tensor(0.0920, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1010/20000], Loss: (1.1741144167859678, tensor(0.0921, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1020/20000], Loss: (1.1739715908300834, tensor(0.0921, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1030/20000], Loss: (1.1738303957537146, tensor(0.0922, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1040/20000], Loss: (1.1736909698282, tensor(0.0922, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1050/20000], Loss: (1.1735533925163064, tensor(0.0922, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1060/20000], Loss: (1.1734173932415348, tensor(0.0923, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1070/20000], Loss: (1.1732832224476153, tensor(0.0923, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1080/20000], Loss: (1.1731505706441925, tensor(0.0924, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1090/20000], Loss: (1.1730197062038168, tensor(0.0924, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1100/20000], Loss: (1.1728903138626845, tensor(0.0924, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1110/20000], Loss: (1.1727625581665617, tensor(0.0925, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1120/20000], Loss: (1.172636451113672, tensor(0.0925, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1130/20000], Loss: (1.1725117934297404, tensor(0.0925, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1140/20000], Loss: (1.1723888154913014, tensor(0.0926, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1150/20000], Loss: (1.1722672087256498, tensor(0.0926, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1160/20000], Loss: (1.1721472448196715, tensor(0.0927, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1170/20000], Loss: (1.172028609346952, tensor(0.0927, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1180/20000], Loss: (1.1719115355371077, tensor(0.0927, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1190/20000], Loss: (1.1717958398850314, tensor(0.0928, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1200/20000], Loss: (1.171681521968105, tensor(0.0928, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1210/20000], Loss: (1.1715687493025773, tensor(0.0928, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1220/20000], Loss: (1.1714572131061887, tensor(0.0929, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1230/20000], Loss: (1.1713471876998458, tensor(0.0929, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1240/20000], Loss: (1.1712383635218295, tensor(0.0929, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1250/20000], Loss: (1.1711310169567566, tensor(0.0930, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1260/20000], Loss: (1.1710248336131763, tensor(0.0930, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1270/20000], Loss: (1.1709200822637396, tensor(0.0930, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1280/20000], Loss: (1.1708164850599867, tensor(0.0931, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1290/20000], Loss: (1.1707143239816422, tensor(0.0931, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1300/20000], Loss: (1.1706134309808633, tensor(0.0931, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1310/20000], Loss: (1.1705138633275347, tensor(0.0932, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1320/20000], Loss: (1.1704154185950062, tensor(0.0932, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1330/20000], Loss: (1.1703181554566573, tensor(0.0932, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1340/20000], Loss: (1.1702221960310162, tensor(0.0933, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1350/20000], Loss: (1.1701272324341554, tensor(0.0933, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1360/20000], Loss: (1.1700336421984034, tensor(0.0933, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1370/20000], Loss: (1.1699409376384153, tensor(0.0934, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1380/20000], Loss: (1.1698496315745677, tensor(0.0934, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1390/20000], Loss: (1.1697591446503055, tensor(0.0934, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1400/20000], Loss: (1.1696700494135586, tensor(0.0935, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1410/20000], Loss: (1.169581813394818, tensor(0.0935, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1420/20000], Loss: (1.1694947841253456, tensor(0.0935, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1430/20000], Loss: (1.1694087435080869, tensor(0.0936, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1440/20000], Loss: (1.1693237242424117, tensor(0.0936, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1450/20000], Loss: (1.1692397456392498, tensor(0.0936, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1460/20000], Loss: (1.169156785490697, tensor(0.0936, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1470/20000], Loss: (1.1690747955537977, tensor(0.0937, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1480/20000], Loss: (1.1689939688533426, tensor(0.0937, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1490/20000], Loss: (1.1689137930125983, tensor(0.0937, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1500/20000], Loss: (1.1688348961085129, tensor(0.0938, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1510/20000], Loss: (1.1687567996971755, tensor(0.0938, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1520/20000], Loss: (1.1686796230606602, tensor(0.0938, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1530/20000], Loss: (1.1686033868640662, tensor(0.0938, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1540/20000], Loss: (1.1685281643769903, tensor(0.0939, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1550/20000], Loss: (1.1684536336711804, tensor(0.0939, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1560/20000], Loss: (1.1683801503183389, tensor(0.0939, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1570/20000], Loss: (1.1683076131324883, tensor(0.0940, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1580/20000], Loss: (1.1682356670091432, tensor(0.0940, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1590/20000], Loss: (1.168164894659347, tensor(0.0940, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1600/20000], Loss: (1.1680948237053383, tensor(0.0940, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1610/20000], Loss: (1.1680256376619187, tensor(0.0941, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1620/20000], Loss: (1.1679571538581226, tensor(0.0941, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1630/20000], Loss: (1.167889707957703, tensor(0.0941, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1640/20000], Loss: (1.1678229209190045, tensor(0.0941, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1650/20000], Loss: (1.1677569023072576, tensor(0.0942, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1660/20000], Loss: (1.1676916904461332, tensor(0.0942, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1670/20000], Loss: (1.167627411118345, tensor(0.0942, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1680/20000], Loss: (1.1675637414362052, tensor(0.0942, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1690/20000], Loss: (1.1675008222857743, tensor(0.0943, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1700/20000], Loss: (1.1674386371829464, tensor(0.0943, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1710/20000], Loss: (1.1673773612107374, tensor(0.0943, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1720/20000], Loss: (1.1673166631399392, tensor(0.0943, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1730/20000], Loss: (1.1672567776958764, tensor(0.0944, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1740/20000], Loss: (1.1671973827067008, tensor(0.0944, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1750/20000], Loss: (1.167138967751546, tensor(0.0944, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1760/20000], Loss: (1.167081090249177, tensor(0.0944, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1770/20000], Loss: (1.167024032067803, tensor(0.0945, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1780/20000], Loss: (1.1669674766283182, tensor(0.0945, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1790/20000], Loss: (1.1669116230803311, tensor(0.0945, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1800/20000], Loss: (1.1668564646516315, tensor(0.0945, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1810/20000], Loss: (1.1668020325361468, tensor(0.0946, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1820/20000], Loss: (1.1667481847479548, tensor(0.0946, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1830/20000], Loss: (1.1666950137601835, tensor(0.0946, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1840/20000], Loss: (1.1666423318327204, tensor(0.0946, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1850/20000], Loss: (1.1665903287963113, tensor(0.0946, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1860/20000], Loss: (1.1665389928705132, tensor(0.0947, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1870/20000], Loss: (1.1664883527157228, tensor(0.0947, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1880/20000], Loss: (1.1664382770380874, tensor(0.0947, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1890/20000], Loss: (1.16638877588824, tensor(0.0947, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1900/20000], Loss: (1.1663398429946619, tensor(0.0948, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1910/20000], Loss: (1.166291547066773, tensor(0.0948, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1920/20000], Loss: (1.1662436092629969, tensor(0.0948, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1930/20000], Loss: (1.1661965028631696, tensor(0.0948, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1940/20000], Loss: (1.166149800677048, tensor(0.0948, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1950/20000], Loss: (1.1661038039680123, tensor(0.0949, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1960/20000], Loss: (1.1660581912379218, tensor(0.0949, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1970/20000], Loss: (1.1660132691802239, tensor(0.0949, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1980/20000], Loss: (1.1659687026543306, tensor(0.0949, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [1990/20000], Loss: (1.165924783025688, tensor(0.0949, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2000/20000], Loss: (1.1658811631422394, tensor(0.0950, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2010/20000], Loss: (1.1658382704909587, tensor(0.0950, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2020/20000], Loss: (1.165795759033561, tensor(0.0950, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2030/20000], Loss: (1.1657538960932536, tensor(0.0950, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2040/20000], Loss: (1.1657123791655535, tensor(0.0950, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2050/20000], Loss: (1.165671482576385, tensor(0.0951, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2060/20000], Loss: (1.1656310088703896, tensor(0.0951, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2070/20000], Loss: (1.165590974659035, tensor(0.0951, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2080/20000], Loss: (1.1655515033802522, tensor(0.0951, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2090/20000], Loss: (1.1655122579357928, tensor(0.0951, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2100/20000], Loss: (1.1654736298052846, tensor(0.0952, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2110/20000], Loss: (1.1654354257092034, tensor(0.0952, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2120/20000], Loss: (1.1653977540580003, tensor(0.0952, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2130/20000], Loss: (1.1653603633350604, tensor(0.0952, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2140/20000], Loss: (1.1653235637699837, tensor(0.0952, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2150/20000], Loss: (1.1652870884518505, tensor(0.0953, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2160/20000], Loss: (1.1652512088043276, tensor(0.0953, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2170/20000], Loss: (1.1652156769100237, tensor(0.0953, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2180/20000], Loss: (1.1651803944880903, tensor(0.0953, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2190/20000], Loss: (1.1651456818199903, tensor(0.0953, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2200/20000], Loss: (1.1651113027211084, tensor(0.0954, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2210/20000], Loss: (1.1650774253503402, tensor(0.0954, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2220/20000], Loss: (1.165043739478017, tensor(0.0954, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2230/20000], Loss: (1.1650106023425932, tensor(0.0954, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2240/20000], Loss: (1.1649779372703966, tensor(0.0954, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2250/20000], Loss: (1.1649453976575126, tensor(0.0954, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2260/20000], Loss: (1.1649133615514136, tensor(0.0955, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2270/20000], Loss: (1.1648816263312318, tensor(0.0955, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2280/20000], Loss: (1.1648503847079523, tensor(0.0955, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2290/20000], Loss: (1.1648193020449686, tensor(0.0955, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2300/20000], Loss: (1.1647894753980983, tensor(0.0955, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2310/20000], Loss: (1.164761482302545, tensor(0.0955, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2320/20000], Loss: (1.1647339880023786, tensor(0.0956, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2330/20000], Loss: (1.1647069988969303, tensor(0.0956, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2340/20000], Loss: (1.1646802718219282, tensor(0.0956, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2350/20000], Loss: (1.164653673249474, tensor(0.0956, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2360/20000], Loss: (1.164627690055123, tensor(0.0956, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2370/20000], Loss: (1.1646018147253299, tensor(0.0957, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2380/20000], Loss: (1.1645766826580457, tensor(0.0957, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2390/20000], Loss: (1.164551454125996, tensor(0.0957, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2400/20000], Loss: (1.164526600843075, tensor(0.0957, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2410/20000], Loss: (1.1645022372492866, tensor(0.0957, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2420/20000], Loss: (1.1644781847417254, tensor(0.0957, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2430/20000], Loss: (1.1644542547889545, tensor(0.0958, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2440/20000], Loss: (1.1644309906487738, tensor(0.0958, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2450/20000], Loss: (1.1644075575508757, tensor(0.0958, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2460/20000], Loss: (1.1643846370225535, tensor(0.0958, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2470/20000], Loss: (1.1643619680959678, tensor(0.0958, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2480/20000], Loss: (1.1643394789963017, tensor(0.0958, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2490/20000], Loss: (1.1643172760187734, tensor(0.0959, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2500/20000], Loss: (1.1642952635100352, tensor(0.0959, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2510/20000], Loss: (1.1642734188584214, tensor(0.0959, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2520/20000], Loss: (1.1642523250301142, tensor(0.0959, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2530/20000], Loss: (1.1642311377369317, tensor(0.0959, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2540/20000], Loss: (1.1642105085437429, tensor(0.0959, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2550/20000], Loss: (1.1641895748270523, tensor(0.0959, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2560/20000], Loss: (1.1641693961111848, tensor(0.0960, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2570/20000], Loss: (1.1641494693881573, tensor(0.0960, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2580/20000], Loss: (1.1641292675713626, tensor(0.0960, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2590/20000], Loss: (1.164109961327718, tensor(0.0960, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2600/20000], Loss: (1.1640904281330753, tensor(0.0960, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2610/20000], Loss: (1.1640713616011336, tensor(0.0960, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2620/20000], Loss: (1.1640523876076523, tensor(0.0961, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2630/20000], Loss: (1.1640335571986007, tensor(0.0961, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2640/20000], Loss: (1.1640151739926528, tensor(0.0961, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2650/20000], Loss: (1.1639966881956434, tensor(0.0961, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2660/20000], Loss: (1.1639789131250509, tensor(0.0961, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2670/20000], Loss: (1.1639608193752484, tensor(0.0961, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2680/20000], Loss: (1.163943586670053, tensor(0.0961, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2690/20000], Loss: (1.1639258075667875, tensor(0.0962, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2700/20000], Loss: (1.1639087295570443, tensor(0.0962, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2710/20000], Loss: (1.1638916330226103, tensor(0.0962, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2720/20000], Loss: (1.1638746709079226, tensor(0.0962, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2730/20000], Loss: (1.163858203722382, tensor(0.0962, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2740/20000], Loss: (1.163841687058772, tensor(0.0962, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2750/20000], Loss: (1.1638254999168005, tensor(0.0962, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2760/20000], Loss: (1.1638093803310796, tensor(0.0963, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2770/20000], Loss: (1.1637936672465856, tensor(0.0963, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2780/20000], Loss: (1.1637777518934964, tensor(0.0963, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2790/20000], Loss: (1.1637621735894892, tensor(0.0963, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2800/20000], Loss: (1.163747046475335, tensor(0.0963, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2810/20000], Loss: (1.1637318356569541, tensor(0.0963, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2820/20000], Loss: (1.1637169394950932, tensor(0.0963, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2830/20000], Loss: (1.1637019449454524, tensor(0.0963, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2840/20000], Loss: (1.1636874330204818, tensor(0.0964, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2850/20000], Loss: (1.1636730392233126, tensor(0.0964, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2860/20000], Loss: (1.163658693403759, tensor(0.0964, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2870/20000], Loss: (1.1636445605197632, tensor(0.0964, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2880/20000], Loss: (1.1636305116167533, tensor(0.0964, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2890/20000], Loss: (1.1636167172150411, tensor(0.0964, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2900/20000], Loss: (1.163602994337987, tensor(0.0964, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2910/20000], Loss: (1.163589496567792, tensor(0.0964, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2920/20000], Loss: (1.1635762673206371, tensor(0.0965, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2930/20000], Loss: (1.1635630892899016, tensor(0.0965, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2940/20000], Loss: (1.163550002555765, tensor(0.0965, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2950/20000], Loss: (1.1635372379563778, tensor(0.0965, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2960/20000], Loss: (1.1635244548362398, tensor(0.0965, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2970/20000], Loss: (1.163511893950268, tensor(0.0965, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2980/20000], Loss: (1.163499415943448, tensor(0.0965, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [2990/20000], Loss: (1.1634869149271074, tensor(0.0965, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3000/20000], Loss: (1.1634749543729221, tensor(0.0966, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3010/20000], Loss: (1.1634627878686825, tensor(0.0966, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3020/20000], Loss: (1.1634508859116548, tensor(0.0966, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3030/20000], Loss: (1.1634392885836775, tensor(0.0966, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3040/20000], Loss: (1.1634274903288309, tensor(0.0966, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3050/20000], Loss: (1.16341611390222, tensor(0.0966, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3060/20000], Loss: (1.1634047740862945, tensor(0.0966, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3070/20000], Loss: (1.1633935390859547, tensor(0.0966, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3080/20000], Loss: (1.163382520579779, tensor(0.0966, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3090/20000], Loss: (1.1633714419624652, tensor(0.0967, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3100/20000], Loss: (1.163360517310249, tensor(0.0967, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3110/20000], Loss: (1.163349763442388, tensor(0.0967, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3120/20000], Loss: (1.163339267577247, tensor(0.0967, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3130/20000], Loss: (1.163328671176987, tensor(0.0967, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3140/20000], Loss: (1.1633183748783784, tensor(0.0967, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3150/20000], Loss: (1.1633082778809194, tensor(0.0967, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3160/20000], Loss: (1.1632980008593115, tensor(0.0967, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3170/20000], Loss: (1.1632879313202138, tensor(0.0967, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3180/20000], Loss: (1.163277954810457, tensor(0.0968, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3190/20000], Loss: (1.163268225252873, tensor(0.0968, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3200/20000], Loss: (1.1632586528389726, tensor(0.0968, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3210/20000], Loss: (1.1632492075850906, tensor(0.0968, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3220/20000], Loss: (1.1632397912644363, tensor(0.0968, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3230/20000], Loss: (1.1632302382052822, tensor(0.0968, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3240/20000], Loss: (1.16322091394115, tensor(0.0968, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3250/20000], Loss: (1.163211858058357, tensor(0.0968, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3260/20000], Loss: (1.163202769894553, tensor(0.0968, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3270/20000], Loss: (1.1631938766869547, tensor(0.0968, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3280/20000], Loss: (1.1631850297623787, tensor(0.0969, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3290/20000], Loss: (1.1631762794244236, tensor(0.0969, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3300/20000], Loss: (1.16316765986044, tensor(0.0969, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3310/20000], Loss: (1.1631591422303045, tensor(0.0969, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3320/20000], Loss: (1.1631509447826864, tensor(0.0969, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3330/20000], Loss: (1.163142327840431, tensor(0.0969, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3340/20000], Loss: (1.1631340366344765, tensor(0.0969, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3350/20000], Loss: (1.1631257718793, tensor(0.0969, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3360/20000], Loss: (1.1631179344744544, tensor(0.0969, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3370/20000], Loss: (1.1631098898867012, tensor(0.0969, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3380/20000], Loss: (1.1631019512944611, tensor(0.0969, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3390/20000], Loss: (1.1630942793448866, tensor(0.0970, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3400/20000], Loss: (1.1630865412175382, tensor(0.0970, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3410/20000], Loss: (1.1630788630599915, tensor(0.0970, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3420/20000], Loss: (1.1630713669034012, tensor(0.0970, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3430/20000], Loss: (1.1630638421471533, tensor(0.0970, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3440/20000], Loss: (1.163056374170298, tensor(0.0970, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3450/20000], Loss: (1.1630491243650622, tensor(0.0970, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3460/20000], Loss: (1.1630421942108067, tensor(0.0970, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3470/20000], Loss: (1.1630350190627825, tensor(0.0970, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3480/20000], Loss: (1.163027750089387, tensor(0.0970, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3490/20000], Loss: (1.1630208657846204, tensor(0.0970, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3500/20000], Loss: (1.1630139901074823, tensor(0.0970, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3510/20000], Loss: (1.163007116196487, tensor(0.0971, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3520/20000], Loss: (1.1630004602313502, tensor(0.0971, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3530/20000], Loss: (1.1629939187380274, tensor(0.0971, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3540/20000], Loss: (1.16298718456064, tensor(0.0971, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3550/20000], Loss: (1.1629806188264786, tensor(0.0971, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3560/20000], Loss: (1.162974383316029, tensor(0.0971, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3570/20000], Loss: (1.162967837806233, tensor(0.0971, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3580/20000], Loss: (1.1629615641375874, tensor(0.0971, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3590/20000], Loss: (1.1629554756518226, tensor(0.0971, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3600/20000], Loss: (1.1629491622766126, tensor(0.0971, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3610/20000], Loss: (1.1629431734672468, tensor(0.0971, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3620/20000], Loss: (1.1629370681888669, tensor(0.0971, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3630/20000], Loss: (1.162931191195537, tensor(0.0972, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3640/20000], Loss: (1.1629254106820315, tensor(0.0972, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3650/20000], Loss: (1.1629194222176136, tensor(0.0972, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3660/20000], Loss: (1.1629138362001818, tensor(0.0972, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3670/20000], Loss: (1.1629080301161636, tensor(0.0972, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3680/20000], Loss: (1.1629023044537998, tensor(0.0972, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3690/20000], Loss: (1.1628969318763294, tensor(0.0972, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3700/20000], Loss: (1.1628912619944398, tensor(0.0972, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3710/20000], Loss: (1.1628861209349546, tensor(0.0972, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3720/20000], Loss: (1.1628806581596725, tensor(0.0972, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3730/20000], Loss: (1.1628752376542248, tensor(0.0972, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3740/20000], Loss: (1.162870093382399, tensor(0.0972, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3750/20000], Loss: (1.1628645845543284, tensor(0.0972, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3760/20000], Loss: (1.1628596837204657, tensor(0.0973, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3770/20000], Loss: (1.1628545272822668, tensor(0.0973, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3780/20000], Loss: (1.1628495481962255, tensor(0.0973, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3790/20000], Loss: (1.1628447605415793, tensor(0.0973, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3800/20000], Loss: (1.1628396114619073, tensor(0.0973, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3810/20000], Loss: (1.162834940536962, tensor(0.0973, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3820/20000], Loss: (1.1628300492290917, tensor(0.0973, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3830/20000], Loss: (1.162825347382545, tensor(0.0973, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3840/20000], Loss: (1.1628205657246495, tensor(0.0973, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3850/20000], Loss: (1.1628158268925068, tensor(0.0973, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3860/20000], Loss: (1.1628113201583197, tensor(0.0973, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3870/20000], Loss: (1.1628068107767793, tensor(0.0973, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3880/20000], Loss: (1.162802353139897, tensor(0.0973, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3890/20000], Loss: (1.1627977927712048, tensor(0.0973, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3900/20000], Loss: (1.1627933849499916, tensor(0.0973, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3910/20000], Loss: (1.1627890585303142, tensor(0.0974, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3920/20000], Loss: (1.1627850186882638, tensor(0.0974, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3930/20000], Loss: (1.1627805999369825, tensor(0.0974, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3940/20000], Loss: (1.1627765393340264, tensor(0.0974, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3950/20000], Loss: (1.1627721021278128, tensor(0.0974, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3960/20000], Loss: (1.162768265791406, tensor(0.0974, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3970/20000], Loss: (1.1627641889685918, tensor(0.0974, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3980/20000], Loss: (1.162760107727689, tensor(0.0974, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [3990/20000], Loss: (1.162756125042037, tensor(0.0974, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4000/20000], Loss: (1.1627520849241904, tensor(0.0974, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4010/20000], Loss: (1.162748422876587, tensor(0.0974, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4020/20000], Loss: (1.162744549760697, tensor(0.0974, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4030/20000], Loss: (1.1627406572631986, tensor(0.0974, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4040/20000], Loss: (1.1627368690755415, tensor(0.0974, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4050/20000], Loss: (1.162733204573377, tensor(0.0974, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4060/20000], Loss: (1.1627295667282338, tensor(0.0974, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4070/20000], Loss: (1.1627259922272677, tensor(0.0975, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4080/20000], Loss: (1.1627222046803127, tensor(0.0975, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4090/20000], Loss: (1.1627187787724318, tensor(0.0975, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4100/20000], Loss: (1.1627152319189986, tensor(0.0975, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4110/20000], Loss: (1.162711800302881, tensor(0.0975, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4120/20000], Loss: (1.1627083547945554, tensor(0.0975, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4130/20000], Loss: (1.162704827707008, tensor(0.0975, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4140/20000], Loss: (1.162701694187686, tensor(0.0975, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4150/20000], Loss: (1.1626982284186589, tensor(0.0975, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4160/20000], Loss: (1.1626949573453602, tensor(0.0975, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4170/20000], Loss: (1.1626916798388687, tensor(0.0975, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4180/20000], Loss: (1.162688594396127, tensor(0.0975, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4190/20000], Loss: (1.162685348556187, tensor(0.0975, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4200/20000], Loss: (1.1626822154402123, tensor(0.0975, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4210/20000], Loss: (1.1626789450282442, tensor(0.0975, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4220/20000], Loss: (1.1626762370259454, tensor(0.0975, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4230/20000], Loss: (1.1626729453118447, tensor(0.0975, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4240/20000], Loss: (1.1626700849759661, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4250/20000], Loss: (1.162666950863441, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4260/20000], Loss: (1.1626642846899826, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4270/20000], Loss: (1.162661290349501, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4280/20000], Loss: (1.1626583836018392, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4290/20000], Loss: (1.1626554588506002, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4300/20000], Loss: (1.1626528866714456, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4310/20000], Loss: (1.1626499905174659, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4320/20000], Loss: (1.1626471945031809, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4330/20000], Loss: (1.162644445871193, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4340/20000], Loss: (1.1626418678966655, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4350/20000], Loss: (1.162639341896169, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4360/20000], Loss: (1.162636396316209, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4370/20000], Loss: (1.1626339931522045, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4380/20000], Loss: (1.1626312499895077, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4390/20000], Loss: (1.162628744945303, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4400/20000], Loss: (1.1626262419806979, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4410/20000], Loss: (1.162623709883411, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4420/20000], Loss: (1.162621429280661, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4430/20000], Loss: (1.1626187961291843, tensor(0.0976, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4440/20000], Loss: (1.1626165063887772, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4450/20000], Loss: (1.1626142644274922, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4460/20000], Loss: (1.162611832640119, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4470/20000], Loss: (1.162609486227237, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4480/20000], Loss: (1.1626071881525004, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4490/20000], Loss: (1.1626050532190069, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4500/20000], Loss: (1.1626026978438362, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4510/20000], Loss: (1.1626003891504832, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4520/20000], Loss: (1.1625982434669053, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4530/20000], Loss: (1.1625960971317963, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4540/20000], Loss: (1.1625938289013233, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4550/20000], Loss: (1.1625917330649314, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4560/20000], Loss: (1.162589741325468, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4570/20000], Loss: (1.1625875767702791, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4580/20000], Loss: (1.1625856278988946, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4590/20000], Loss: (1.162583632928692, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4600/20000], Loss: (1.1625815535486013, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4610/20000], Loss: (1.1625795521375393, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4620/20000], Loss: (1.162577524094063, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4630/20000], Loss: (1.1625757333320221, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4640/20000], Loss: (1.1625735862324653, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4650/20000], Loss: (1.162571682015661, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4660/20000], Loss: (1.162570041446436, tensor(0.0977, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4670/20000], Loss: (1.1625680142787176, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4680/20000], Loss: (1.1625663142483065, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4690/20000], Loss: (1.1625645098509547, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4700/20000], Loss: (1.1625625772147423, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4710/20000], Loss: (1.1625608370699239, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4720/20000], Loss: (1.1625590122678477, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4730/20000], Loss: (1.1625573919300325, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4740/20000], Loss: (1.162555516524299, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4750/20000], Loss: (1.1625538517072873, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4760/20000], Loss: (1.1625524531063465, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4770/20000], Loss: (1.1625505456510898, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4780/20000], Loss: (1.1625488948184395, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4790/20000], Loss: (1.1625472812092326, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4800/20000], Loss: (1.1625455587236893, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4810/20000], Loss: (1.1625441201688431, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4820/20000], Loss: (1.1625425547153774, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4830/20000], Loss: (1.1625409220227527, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4840/20000], Loss: (1.1625394822246342, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4850/20000], Loss: (1.162537972858285, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4860/20000], Loss: (1.1625363557508173, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4870/20000], Loss: (1.1625347732130742, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4880/20000], Loss: (1.1625334593542487, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4890/20000], Loss: (1.1625321332576244, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4900/20000], Loss: (1.1625304918984163, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4910/20000], Loss: (1.1625292077081844, tensor(0.0978, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4920/20000], Loss: (1.1625277239760188, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4930/20000], Loss: (1.1625261778195068, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4940/20000], Loss: (1.1625249541288483, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4950/20000], Loss: (1.1625236011064297, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4960/20000], Loss: (1.1625221875057241, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4970/20000], Loss: (1.162520963956847, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4980/20000], Loss: (1.162519632698387, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [4990/20000], Loss: (1.1625181502449233, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5000/20000], Loss: (1.1625169973508294, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5010/20000], Loss: (1.1625157519397782, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5020/20000], Loss: (1.162514365681718, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5030/20000], Loss: (1.1625133762087778, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5040/20000], Loss: (1.1625120084180167, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5050/20000], Loss: (1.16251048523866, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5060/20000], Loss: (1.1625096617244512, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5070/20000], Loss: (1.1625084241164017, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5080/20000], Loss: (1.1625070253406147, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5090/20000], Loss: (1.1625062414199876, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5100/20000], Loss: (1.162504688497234, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5110/20000], Loss: (1.1625035165438042, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5120/20000], Loss: (1.1625027582765306, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5130/20000], Loss: (1.1625014113740575, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5140/20000], Loss: (1.162500315379665, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5150/20000], Loss: (1.1624993120172866, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5160/20000], Loss: (1.162498066659999, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5170/20000], Loss: (1.1624970313103278, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5180/20000], Loss: (1.1624961251348511, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5190/20000], Loss: (1.1624950138254344, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5200/20000], Loss: (1.162493917820538, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5210/20000], Loss: (1.162492962220704, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5220/20000], Loss: (1.162491886845078, tensor(0.0979, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5230/20000], Loss: (1.1624908852063203, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5240/20000], Loss: (1.1624900819779689, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5250/20000], Loss: (1.1624889231746625, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5260/20000], Loss: (1.1624879554321164, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5270/20000], Loss: (1.1624871244575972, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5280/20000], Loss: (1.162485961859632, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5290/20000], Loss: (1.1624852732668964, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5300/20000], Loss: (1.162484283778374, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5310/20000], Loss: (1.1624831244185845, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5320/20000], Loss: (1.162482448185668, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5330/20000], Loss: (1.1624815798981027, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5340/20000], Loss: (1.1624806393714324, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5350/20000], Loss: (1.162479705952561, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5360/20000], Loss: (1.1624788361858631, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5370/20000], Loss: (1.1624781018935681, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5380/20000], Loss: (1.1624772166080246, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5390/20000], Loss: (1.1624763116161685, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5400/20000], Loss: (1.16247559734342, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5410/20000], Loss: (1.1624745561962595, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5420/20000], Loss: (1.1624739472590278, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5430/20000], Loss: (1.1624732784305736, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5440/20000], Loss: (1.1624721069385409, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5450/20000], Loss: (1.1624715904695762, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5460/20000], Loss: (1.162470752961088, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5470/20000], Loss: (1.1624699186970535, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5480/20000], Loss: (1.1624692841345519, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5490/20000], Loss: (1.162468424754777, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5500/20000], Loss: (1.1624676377306855, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5510/20000], Loss: (1.1624670279573543, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5520/20000], Loss: (1.162466243367492, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5530/20000], Loss: (1.1624655585961448, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5540/20000], Loss: (1.1624647213942036, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5550/20000], Loss: (1.1624641936067828, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5560/20000], Loss: (1.162463552454647, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5570/20000], Loss: (1.1624626023219944, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5580/20000], Loss: (1.1624620953022708, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5590/20000], Loss: (1.162461449817521, tensor(0.0980, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5600/20000], Loss: (1.1624606633864103, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5610/20000], Loss: (1.1624601267293742, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5620/20000], Loss: (1.1624594442389462, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5630/20000], Loss: (1.162458746748305, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5640/20000], Loss: (1.1624580726468212, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5650/20000], Loss: (1.162457507467999, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5660/20000], Loss: (1.1624569646031002, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5670/20000], Loss: (1.162456239519229, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5680/20000], Loss: (1.1624556527087704, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5690/20000], Loss: (1.162455114214014, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5700/20000], Loss: (1.162454404779291, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5710/20000], Loss: (1.1624538848453307, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5720/20000], Loss: (1.16245333621422, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5730/20000], Loss: (1.1624526333681375, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5740/20000], Loss: (1.1624521973599593, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5750/20000], Loss: (1.1624516761935495, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5760/20000], Loss: (1.16245098514339, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5770/20000], Loss: (1.1624503965721322, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5780/20000], Loss: (1.1624498550981022, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5790/20000], Loss: (1.1624496735936256, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5800/20000], Loss: (1.1624487586771302, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5810/20000], Loss: (1.1624482115862083, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5820/20000], Loss: (1.1624478921684844, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5830/20000], Loss: (1.1624473140408202, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5840/20000], Loss: (1.1624468442630684, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5850/20000], Loss: (1.1624461777383464, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5860/20000], Loss: (1.1624458444932935, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5870/20000], Loss: (1.1624452524720936, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5880/20000], Loss: (1.162444637790578, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5890/20000], Loss: (1.1624445176036526, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5900/20000], Loss: (1.1624438857719392, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5910/20000], Loss: (1.162443232867927, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5920/20000], Loss: (1.1624428313167627, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5930/20000], Loss: (1.1624424671364688, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5940/20000], Loss: (1.1624421343780633, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5950/20000], Loss: (1.1624413677277792, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5960/20000], Loss: (1.1624410533153169, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5970/20000], Loss: (1.1624408026927315, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5980/20000], Loss: (1.1624400288001864, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [5990/20000], Loss: (1.1624397912599291, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6000/20000], Loss: (1.1624393579019965, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6010/20000], Loss: (1.1624389583292982, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6020/20000], Loss: (1.1624385050700667, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6030/20000], Loss: (1.1624379028381977, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6040/20000], Loss: (1.1624378994206908, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6050/20000], Loss: (1.162437109725428, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6060/20000], Loss: (1.1624368389347584, tensor(0.0981, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6070/20000], Loss: (1.16243656507141, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6080/20000], Loss: (1.1624360040684942, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6090/20000], Loss: (1.1624357208837561, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6100/20000], Loss: (1.16243515006573, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6110/20000], Loss: (1.162435121368204, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6120/20000], Loss: (1.1624345788771209, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6130/20000], Loss: (1.1624340830936681, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6140/20000], Loss: (1.1624338275166668, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6150/20000], Loss: (1.1624333672772083, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6160/20000], Loss: (1.1624331462524053, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6170/20000], Loss: (1.1624327460226698, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6180/20000], Loss: (1.1624322958991475, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6190/20000], Loss: (1.1624320894536924, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6200/20000], Loss: (1.1624316547758433, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6210/20000], Loss: (1.1624313993869184, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6220/20000], Loss: (1.1624308953399038, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6230/20000], Loss: (1.1624307726744907, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6240/20000], Loss: (1.1624303969113179, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6250/20000], Loss: (1.1624299573611983, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6260/20000], Loss: (1.162429653520676, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6270/20000], Loss: (1.1624294316059594, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6280/20000], Loss: (1.1624290093469718, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6290/20000], Loss: (1.1624286266499566, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6300/20000], Loss: (1.162428627240319, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6310/20000], Loss: (1.162428161509683, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6320/20000], Loss: (1.162427826115306, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6330/20000], Loss: (1.1624274552906713, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6340/20000], Loss: (1.1624272248191354, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6350/20000], Loss: (1.1624270848970173, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6360/20000], Loss: (1.1624265848268884, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6370/20000], Loss: (1.1624265046811588, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6380/20000], Loss: (1.1624259841497833, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6390/20000], Loss: (1.1624258600800443, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6400/20000], Loss: (1.1624256718174182, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6410/20000], Loss: (1.1624250186375868, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6420/20000], Loss: (1.1624250908256557, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6430/20000], Loss: (1.16242474010677, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6440/20000], Loss: (1.1624244715463532, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6450/20000], Loss: (1.162424168560228, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6460/20000], Loss: (1.1624239762388036, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6470/20000], Loss: (1.162423705430175, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6480/20000], Loss: (1.1624233234814048, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6490/20000], Loss: (1.1624231975654546, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6500/20000], Loss: (1.1624228684889464, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6510/20000], Loss: (1.1624227592274643, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6520/20000], Loss: (1.1624224692535507, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6530/20000], Loss: (1.1624221922051272, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6540/20000], Loss: (1.162421895911392, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6550/20000], Loss: (1.1624218435664344, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6560/20000], Loss: (1.162421483913307, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6570/20000], Loss: (1.1624211420236275, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6580/20000], Loss: (1.1624211141947582, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6590/20000], Loss: (1.1624208252197845, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6600/20000], Loss: (1.1624207268919216, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6610/20000], Loss: (1.162420303488685, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6620/20000], Loss: (1.1624200989640083, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6630/20000], Loss: (1.1624200677210974, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6640/20000], Loss: (1.1624197056971868, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6650/20000], Loss: (1.1624195645476618, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6660/20000], Loss: (1.1624190900453522, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6670/20000], Loss: (1.1624192857229394, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6680/20000], Loss: (1.1624189640787685, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6690/20000], Loss: (1.162418739403598, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6700/20000], Loss: (1.162418501777551, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6710/20000], Loss: (1.16241824990532, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6720/20000], Loss: (1.1624181675900769, tensor(0.0982, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6730/20000], Loss: (1.1624179645717614, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6740/20000], Loss: (1.1624176480248911, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6750/20000], Loss: (1.162417454455687, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6760/20000], Loss: (1.1624174712562063, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6770/20000], Loss: (1.162417091732666, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6780/20000], Loss: (1.1624171030202737, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6790/20000], Loss: (1.1624167198271256, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6800/20000], Loss: (1.162416719167745, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6810/20000], Loss: (1.1624164259626466, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6820/20000], Loss: (1.162416263575554, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6830/20000], Loss: (1.1624160768458494, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6840/20000], Loss: (1.1624158561221305, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6850/20000], Loss: (1.1624159666283207, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6860/20000], Loss: (1.1624154375110791, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6870/20000], Loss: (1.162415469230573, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6880/20000], Loss: (1.1624153360296638, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6890/20000], Loss: (1.1624151863121628, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6900/20000], Loss: (1.1624150065512389, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6910/20000], Loss: (1.1624147581786495, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6920/20000], Loss: (1.1624145314380814, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6930/20000], Loss: (1.1624145688016043, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6940/20000], Loss: (1.162414381904567, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6950/20000], Loss: (1.1624140960919296, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6960/20000], Loss: (1.1624140165196049, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6970/20000], Loss: (1.1624137651518887, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6980/20000], Loss: (1.1624139251134307, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [6990/20000], Loss: (1.1624135731782277, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7000/20000], Loss: (1.1624134458884146, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7010/20000], Loss: (1.162413161463325, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7020/20000], Loss: (1.1624132933424485, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7030/20000], Loss: (1.1624130843043636, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7040/20000], Loss: (1.1624127920888396, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7050/20000], Loss: (1.1624128173487107, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7060/20000], Loss: (1.1624124325091916, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7070/20000], Loss: (1.1624126170006426, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7080/20000], Loss: (1.1624123845583543, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7090/20000], Loss: (1.1624121974640258, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7100/20000], Loss: (1.1624120307240198, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7110/20000], Loss: (1.1624120317467435, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7120/20000], Loss: (1.1624118412653102, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7130/20000], Loss: (1.1624118555200613, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7140/20000], Loss: (1.1624114410378552, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7150/20000], Loss: (1.1624114463184079, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7160/20000], Loss: (1.1624114606647646, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7170/20000], Loss: (1.1624111400584405, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7180/20000], Loss: (1.1624111996310313, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7190/20000], Loss: (1.1624108275043041, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7200/20000], Loss: (1.1624109757200771, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7210/20000], Loss: (1.1624107507221886, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7220/20000], Loss: (1.1624106686716549, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7230/20000], Loss: (1.1624105054748715, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7240/20000], Loss: (1.162410469357342, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7250/20000], Loss: (1.1624103517312856, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7260/20000], Loss: (1.1624102706172237, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7270/20000], Loss: (1.1624101505190911, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7280/20000], Loss: (1.162409971141833, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7290/20000], Loss: (1.162409873664987, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7300/20000], Loss: (1.1624098398326461, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7310/20000], Loss: (1.162409671790924, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7320/20000], Loss: (1.1624095154254652, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7330/20000], Loss: (1.1624094620099983, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7340/20000], Loss: (1.1624093098379633, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7350/20000], Loss: (1.1624094991597786, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7360/20000], Loss: (1.16240905358243, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7370/20000], Loss: (1.1624091531509673, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7380/20000], Loss: (1.162408937516073, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7390/20000], Loss: (1.162409013798995, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7400/20000], Loss: (1.1624088978801566, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7410/20000], Loss: (1.1624086786022976, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7420/20000], Loss: (1.1624085698935231, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7430/20000], Loss: (1.162408641108781, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7440/20000], Loss: (1.162408463003736, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7450/20000], Loss: (1.1624084271697555, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7460/20000], Loss: (1.1624082136502603, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7470/20000], Loss: (1.1624080557709475, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7480/20000], Loss: (1.1624082177070818, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7490/20000], Loss: (1.1624079180718694, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7500/20000], Loss: (1.1624080482521353, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7510/20000], Loss: (1.162407769116651, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7520/20000], Loss: (1.1624077381497366, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7530/20000], Loss: (1.162407744355818, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7540/20000], Loss: (1.1624076949780722, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7550/20000], Loss: (1.1624075307102162, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7560/20000], Loss: (1.1624075073771138, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7570/20000], Loss: (1.1624072270922647, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7580/20000], Loss: (1.1624075063704948, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7590/20000], Loss: (1.1624072957011191, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7600/20000], Loss: (1.1624072270656787, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7610/20000], Loss: (1.1624070192305147, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7620/20000], Loss: (1.1624068838783392, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7630/20000], Loss: (1.1624070416499157, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7640/20000], Loss: (1.1624069281274712, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7650/20000], Loss: (1.1624068190561854, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7660/20000], Loss: (1.1624065976769868, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7670/20000], Loss: (1.1624066033417428, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7680/20000], Loss: (1.1624065790584563, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7690/20000], Loss: (1.1624066345101725, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7700/20000], Loss: (1.1624063717047735, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7710/20000], Loss: (1.1624063596794623, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7720/20000], Loss: (1.1624062336549443, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7730/20000], Loss: (1.1624063747127724, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7740/20000], Loss: (1.1624062024653703, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7750/20000], Loss: (1.1624061945711994, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7760/20000], Loss: (1.1624059321509796, tensor(0.0983, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7770/20000], Loss: (1.162406087571095, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7780/20000], Loss: (1.1624059220254797, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7790/20000], Loss: (1.162406035603627, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7800/20000], Loss: (1.162405809839436, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7810/20000], Loss: (1.162405744878571, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7820/20000], Loss: (1.1624056868536292, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7830/20000], Loss: (1.1624056703506322, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7840/20000], Loss: (1.1624056427404676, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7850/20000], Loss: (1.1624055900642691, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7860/20000], Loss: (1.1624053865223627, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7870/20000], Loss: (1.1624053948318238, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7880/20000], Loss: (1.162405323292692, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7890/20000], Loss: (1.1624053342038072, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7900/20000], Loss: (1.1624053747869938, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7910/20000], Loss: (1.162405050685938, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7920/20000], Loss: (1.1624051756025358, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7930/20000], Loss: (1.1624049167405672, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7940/20000], Loss: (1.1624051475533361, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7950/20000], Loss: (1.162405044866163, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7960/20000], Loss: (1.1624049085219057, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7970/20000], Loss: (1.1624048257971291, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7980/20000], Loss: (1.1624047732221665, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [7990/20000], Loss: (1.1624048045823778, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8000/20000], Loss: (1.162404977161217, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8010/20000], Loss: (1.1624045666031628, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8020/20000], Loss: (1.162404738150844, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8030/20000], Loss: (1.1624043846984333, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8040/20000], Loss: (1.162404719887934, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8050/20000], Loss: (1.162404630829428, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8060/20000], Loss: (1.1624044873555162, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8070/20000], Loss: (1.1624044068937502, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8080/20000], Loss: (1.1624042793706124, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8090/20000], Loss: (1.1624043900001126, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8100/20000], Loss: (1.162404530671558, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8110/20000], Loss: (1.1624041709831376, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8120/20000], Loss: (1.1624043089539726, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8130/20000], Loss: (1.1624039522886698, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8140/20000], Loss: (1.1624042304663686, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8150/20000], Loss: (1.1624041938131988, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8160/20000], Loss: (1.1624040302216392, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8170/20000], Loss: (1.1624040555098518, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8180/20000], Loss: (1.1624037885538336, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8190/20000], Loss: (1.1624039560883228, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8200/20000], Loss: (1.1624039739482108, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8210/20000], Loss: (1.1624038338929346, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8220/20000], Loss: (1.1624038487931538, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8230/20000], Loss: (1.162403619630081, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8240/20000], Loss: (1.1624036833442184, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8250/20000], Loss: (1.1624038012630693, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8260/20000], Loss: (1.1624036078382713, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8270/20000], Loss: (1.1624037381704175, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8280/20000], Loss: (1.1624033825923108, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8290/20000], Loss: (1.1624035346464412, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8300/20000], Loss: (1.162403483649708, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8310/20000], Loss: (1.162403536964898, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8320/20000], Loss: (1.1624034742054543, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8330/20000], Loss: (1.1624033303693173, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8340/20000], Loss: (1.1624032675103368, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8350/20000], Loss: (1.162403359045018, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8360/20000], Loss: (1.1624032869994843, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8370/20000], Loss: (1.1624034224634519, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8380/20000], Loss: (1.162403088026222, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8390/20000], Loss: (1.1624032802425566, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8400/20000], Loss: (1.1624030039491984, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8410/20000], Loss: (1.1624032930219665, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8420/20000], Loss: (1.1624031475422338, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8430/20000], Loss: (1.1624030961786114, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8440/20000], Loss: (1.1624030372668406, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8450/20000], Loss: (1.1624029118774288, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8460/20000], Loss: (1.162403003258389, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8470/20000], Loss: (1.1624031213333872, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8480/20000], Loss: (1.1624028642188466, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8490/20000], Loss: (1.1624030594041725, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8500/20000], Loss: (1.1624026708502613, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8510/20000], Loss: (1.162402948355252, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8520/20000], Loss: (1.1624028540679776, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8530/20000], Loss: (1.162402891548981, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8540/20000], Loss: (1.1624028311341004, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8550/20000], Loss: (1.1624027019055592, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8560/20000], Loss: (1.1624026415565707, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8570/20000], Loss: (1.1624027833252155, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8580/20000], Loss: (1.1624026732853412, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8590/20000], Loss: (1.1624028295687208, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8600/20000], Loss: (1.162402487495984, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8610/20000], Loss: (1.1624026873402602, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8620/20000], Loss: (1.1624024195023028, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8630/20000], Loss: (1.162402715125866, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8640/20000], Loss: (1.1624025728007374, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8650/20000], Loss: (1.1624025322846496, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8660/20000], Loss: (1.1624024792903862, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8670/20000], Loss: (1.1624023555932663, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8680/20000], Loss: (1.1624024777162538, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8690/20000], Loss: (1.162402540997888, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8700/20000], Loss: (1.1624023335159155, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8710/20000], Loss: (1.1624025377089626, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8720/20000], Loss: (1.1624021546574117, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8730/20000], Loss: (1.1624024388627072, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8740/20000], Loss: (1.1624022487267132, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8750/20000], Loss: (1.1624023953166547, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8760/20000], Loss: (1.1624023346541952, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8770/20000], Loss: (1.1624022202453375, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8780/20000], Loss: (1.162402166558101, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8790/20000], Loss: (1.1624022021007137, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8800/20000], Loss: (1.162402208438024, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8810/20000], Loss: (1.162402311459228, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8820/20000], Loss: (1.1624020364245613, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8830/20000], Loss: (1.1624022437868844, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8840/20000], Loss: (1.1624019088733593, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8850/20000], Loss: (1.1624022856028464, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8860/20000], Loss: (1.1624020000176312, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8870/20000], Loss: (1.1624021147699832, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8880/20000], Loss: (1.1624020673331188, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8890/20000], Loss: (1.162401944697932, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8900/20000], Loss: (1.162402013554222, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8910/20000], Loss: (1.1624019973111959, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8920/20000], Loss: (1.1624019440179423, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8930/20000], Loss: (1.1624020848396648, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8940/20000], Loss: (1.1624017768808792, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8950/20000], Loss: (1.1624019944407165, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8960/20000], Loss: (1.162401788274649, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8970/20000], Loss: (1.1624020332084277, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8980/20000], Loss: (1.1624017742424593, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [8990/20000], Loss: (1.162401869239407, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9000/20000], Loss: (1.162401807063482, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9010/20000], Loss: (1.1624017645263032, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9020/20000], Loss: (1.1624018759095585, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9030/20000], Loss: (1.162401810762335, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9040/20000], Loss: (1.1624017110662657, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9050/20000], Loss: (1.1624017949166825, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9060/20000], Loss: (1.1624015506118592, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9070/20000], Loss: (1.1624018831089193, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9080/20000], Loss: (1.1624016120339258, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9090/20000], Loss: (1.1624018162715548, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9100/20000], Loss: (1.162401611619867, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9110/20000], Loss: (1.1624016560895158, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9120/20000], Loss: (1.1624015295896073, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9130/20000], Loss: (1.1624016512814201, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9140/20000], Loss: (1.1624016692521892, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9150/20000], Loss: (1.1624016627991571, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9160/20000], Loss: (1.162401512460994, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9170/20000], Loss: (1.1624015642563537, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9180/20000], Loss: (1.16240137539452, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9190/20000], Loss: (1.1624017185259927, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9200/20000], Loss: (1.1624014671752232, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9210/20000], Loss: (1.1624016293104045, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9220/20000], Loss: (1.1624014261847553, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9230/20000], Loss: (1.1624014739947586, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9240/20000], Loss: (1.1624013288683082, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9250/20000], Loss: (1.1624015372685188, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9260/20000], Loss: (1.162401478412665, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9270/20000], Loss: (1.1624015459832977, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9280/20000], Loss: (1.1624013402003535, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9290/20000], Loss: (1.1624013919957863, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9300/20000], Loss: (1.1624012875571132, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9310/20000], Loss: (1.1624014685065192, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9320/20000], Loss: (1.1624013487834768, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9330/20000], Loss: (1.1624014656284518, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9340/20000], Loss: (1.1624012641711794, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9350/20000], Loss: (1.1624013175975005, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9360/20000], Loss: (1.1624012376852668, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9370/20000], Loss: (1.1624014144738037, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9380/20000], Loss: (1.1624012051120935, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9390/20000], Loss: (1.162401397329352, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9400/20000], Loss: (1.1624011940736925, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9410/20000], Loss: (1.1624012484838175, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9420/20000], Loss: (1.1624011980742512, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9430/20000], Loss: (1.162401316739587, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9440/20000], Loss: (1.1624012537205481, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9450/20000], Loss: (1.162401187325309, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9460/20000], Loss: (1.1624011301467962, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9470/20000], Loss: (1.162401191707417, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9480/20000], Loss: (1.1624011496704576, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9490/20000], Loss: (1.1624013198178966, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9500/20000], Loss: (1.1624010539769825, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9510/20000], Loss: (1.1624012726886384, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9520/20000], Loss: (1.16240093150805, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9530/20000], Loss: (1.1624011448003229, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9540/20000], Loss: (1.1624011172606132, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9550/20000], Loss: (1.1624011961184633, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9560/20000], Loss: (1.1624011596521064, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9570/20000], Loss: (1.1624010533210745, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9580/20000], Loss: (1.162401014346736, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9590/20000], Loss: (1.1624009532480102, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9600/20000], Loss: (1.1624010677950294, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9610/20000], Loss: (1.1624012388718223, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9620/20000], Loss: (1.1624009473621215, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9630/20000], Loss: (1.162401168407236, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9640/20000], Loss: (1.1624008079054475, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9650/20000], Loss: (1.16240105498337, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9660/20000], Loss: (1.1624009432848206, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9670/20000], Loss: (1.1624010869168417, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9680/20000], Loss: (1.1624010847067452, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9690/20000], Loss: (1.162400945731999, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9700/20000], Loss: (1.1624009827578146, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9710/20000], Loss: (1.1624008489730902, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9720/20000], Loss: (1.1624010790924297, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9730/20000], Loss: (1.1624010149854906, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9740/20000], Loss: (1.1624009553809285, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9750/20000], Loss: (1.1624010058980014, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9760/20000], Loss: (1.162400832099119, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9770/20000], Loss: (1.1624009856898685, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9780/20000], Loss: (1.1624009819924643, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9790/20000], Loss: (1.162400995648138, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9800/20000], Loss: (1.1624009599913405, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9810/20000], Loss: (1.1624008579585756, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9820/20000], Loss: (1.1624008995931143, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9830/20000], Loss: (1.16240077298065, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9840/20000], Loss: (1.1624010879306612, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9850/20000], Loss: (1.1624009523902654, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9860/20000], Loss: (1.1624009662091226, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9870/20000], Loss: (1.1624008254346958, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9880/20000], Loss: (1.1624008422532892, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9890/20000], Loss: (1.162400773651107, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9900/20000], Loss: (1.1624010165987961, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9910/20000], Loss: (1.1624009266397906, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9920/20000], Loss: (1.1624009760931362, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9930/20000], Loss: (1.1624007936093397, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9940/20000], Loss: (1.1624008540004462, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9950/20000], Loss: (1.1624007014467863, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9960/20000], Loss: (1.1624009194106948, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9970/20000], Loss: (1.1624008923245466, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9980/20000], Loss: (1.162400912680558, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [9990/20000], Loss: (1.162400754493172, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10000/20000], Loss: (1.1624008179294847, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10010/20000], Loss: (1.1624006542931988, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10020/20000], Loss: (1.1624009773924948, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10030/20000], Loss: (1.1624007038932382, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10040/20000], Loss: (1.1624009246832128, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10050/20000], Loss: (1.1624006352418395, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10060/20000], Loss: (1.1624007899455626, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10070/20000], Loss: (1.1624006326792273, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10080/20000], Loss: (1.1624008512181507, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10090/20000], Loss: (1.162400836983474, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10100/20000], Loss: (1.162400738587776, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10110/20000], Loss: (1.1624007039364874, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10120/20000], Loss: (1.1624006146969932, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10130/20000], Loss: (1.1624005831887705, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10140/20000], Loss: (1.1624008812819373, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10150/20000], Loss: (1.1624006572201966, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10160/20000], Loss: (1.1624008831373127, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10170/20000], Loss: (1.1624005269915303, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10180/20000], Loss: (1.1624007432377375, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10190/20000], Loss: (1.1624004494816125, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10200/20000], Loss: (1.1624007578522522, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10210/20000], Loss: (1.1624007761420527, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10220/20000], Loss: (1.1624006894887804, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10230/20000], Loss: (1.1624007442788034, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10240/20000], Loss: (1.1624005593521354, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10250/20000], Loss: (1.1624006407972494, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10260/20000], Loss: (1.1624006659690314, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10270/20000], Loss: (1.1624007500829656, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10280/20000], Loss: (1.162400678718218, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10290/20000], Loss: (1.1624006438382894, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10300/20000], Loss: (1.1624006718571798, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10310/20000], Loss: (1.162400546280653, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10320/20000], Loss: (1.1624006994244678, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10330/20000], Loss: (1.1624008032321407, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10340/20000], Loss: (1.1624006587521587, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10350/20000], Loss: (1.1624006896548909, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10360/20000], Loss: (1.1624005306264782, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10370/20000], Loss: (1.1624006309282904, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10380/20000], Loss: (1.1624005965996753, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10390/20000], Loss: (1.162400837691887, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10400/20000], Loss: (1.1624006445501092, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10410/20000], Loss: (1.1624007050754521, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10420/20000], Loss: (1.1624005126376438, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10430/20000], Loss: (1.1624005928010048, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10440/20000], Loss: (1.1624004446119027, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10450/20000], Loss: (1.1624008234616452, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10460/20000], Loss: (1.1624005756485813, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10470/20000], Loss: (1.1624006928299908, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10480/20000], Loss: (1.1624005024371633, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10490/20000], Loss: (1.1624005659779464, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10500/20000], Loss: (1.1624005060967206, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10510/20000], Loss: (1.1624006383526122, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10520/20000], Loss: (1.162400625080933, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10530/20000], Loss: (1.1624005338977264, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10540/20000], Loss: (1.162400496946833, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10550/20000], Loss: (1.1624005298962934, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10560/20000], Loss: (1.1624003725254104, tensor(0.0984, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10570/20000], Loss: (1.162400782103228, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10580/20000], Loss: (1.1624004839222137, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10590/20000], Loss: (1.1624006742262885, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10600/20000], Loss: (1.1624003863316352, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10610/20000], Loss: (1.1624005444487007, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10620/20000], Loss: (1.1624004108293922, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10630/20000], Loss: (1.1624005509394433, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10640/20000], Loss: (1.162400718305012, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10650/20000], Loss: (1.1624005087262768, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10660/20000], Loss: (1.1624006214327334, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10670/20000], Loss: (1.1624003839781363, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10680/20000], Loss: (1.1624005230862522, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10690/20000], Loss: (1.1624005131962691, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10700/20000], Loss: (1.1624006439492558, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10710/20000], Loss: (1.1624005728190892, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10720/20000], Loss: (1.1624005395369497, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10730/20000], Loss: (1.1624005426641115, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10740/20000], Loss: (1.1624005049129755, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10750/20000], Loss: (1.1624004737651448, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10760/20000], Loss: (1.1624006961927784, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10770/20000], Loss: (1.1624005005816715, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10780/20000], Loss: (1.1624005684217833, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10790/20000], Loss: (1.1624003756412413, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10800/20000], Loss: (1.1624005177479897, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10810/20000], Loss: (1.1624004088496294, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10820/20000], Loss: (1.1624006606495327, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10830/20000], Loss: (1.1624005042523522, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10840/20000], Loss: (1.162400567917486, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10850/20000], Loss: (1.1624003793276885, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10860/20000], Loss: (1.1624004884925152, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10870/20000], Loss: (1.162400255184327, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10880/20000], Loss: (1.1624006220264596, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10890/20000], Loss: (1.1624004322233426, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10900/20000], Loss: (1.1624005722112565, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10910/20000], Loss: (1.1624003815632105, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10920/20000], Loss: (1.1624004430958157, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10930/20000], Loss: (1.1624004231989018, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10940/20000], Loss: (1.1624004012320852, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10950/20000], Loss: (1.1624005787562697, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10960/20000], Loss: (1.1624004096895848, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10970/20000], Loss: (1.1624004859933281, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10980/20000], Loss: (1.1624004091233129, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [10990/20000], Loss: (1.1624003929576086, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11000/20000], Loss: (1.1624005315358539, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11010/20000], Loss: (1.1624004998873685, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11020/20000], Loss: (1.162400572440933, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11030/20000], Loss: (1.162400420937973, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11040/20000], Loss: (1.1624004488000972, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11050/20000], Loss: (1.1624004209610694, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11060/20000], Loss: (1.16240032693518, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11070/20000], Loss: (1.1624006236564848, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11080/20000], Loss: (1.1624004221207052, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11090/20000], Loss: (1.1624006445074846, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11100/20000], Loss: (1.162400309009782, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11110/20000], Loss: (1.1624005168488745, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11120/20000], Loss: (1.1624003297515186, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11130/20000], Loss: (1.1624004973436675, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11140/20000], Loss: (1.162400424143846, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11150/20000], Loss: (1.1624004866553541, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11160/20000], Loss: (1.1624003818692707, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11170/20000], Loss: (1.1624004412289015, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11180/20000], Loss: (1.1624003352775367, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11190/20000], Loss: (1.1624004597983486, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11200/20000], Loss: (1.1624004276716002, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11210/20000], Loss: (1.1624004996199158, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11220/20000], Loss: (1.1624003096326887, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11230/20000], Loss: (1.1624003781511958, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11240/20000], Loss: (1.1624003641961471, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11250/20000], Loss: (1.162400322544243, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11260/20000], Loss: (1.1624004380580315, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11270/20000], Loss: (1.162400499630082, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11280/20000], Loss: (1.1624004201251297, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11290/20000], Loss: (1.1624004223819417, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11300/20000], Loss: (1.162400331657816, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11310/20000], Loss: (1.1624004210420458, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11320/20000], Loss: (1.162400344352036, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11330/20000], Loss: (1.162400510928216, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11340/20000], Loss: (1.162400427701968, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11350/20000], Loss: (1.1624003907195117, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11360/20000], Loss: (1.162400522321502, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11370/20000], Loss: (1.162400271014015, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11380/20000], Loss: (1.1624005507026032, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11390/20000], Loss: (1.1624003366644728, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11400/20000], Loss: (1.1624005848422132, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11410/20000], Loss: (1.1624002586425082, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11420/20000], Loss: (1.1624004621475397, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11430/20000], Loss: (1.1624003086983141, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11440/20000], Loss: (1.1624003423425993, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11450/20000], Loss: (1.1624004374054995, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11460/20000], Loss: (1.1624004382782536, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11470/20000], Loss: (1.162400407824241, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11480/20000], Loss: (1.1624003961954896, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11490/20000], Loss: (1.1624002871651968, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11500/20000], Loss: (1.162400361911642, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11510/20000], Loss: (1.1624002587982636, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11520/20000], Loss: (1.1624004587436787, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11530/20000], Loss: (1.1624002949218255, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11540/20000], Loss: (1.1624004447512304, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11550/20000], Loss: (1.1624003600771493, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11560/20000], Loss: (1.1624003653183599, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11570/20000], Loss: (1.162400311011035, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11580/20000], Loss: (1.1624004256022242, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11590/20000], Loss: (1.1624004203761662, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11600/20000], Loss: (1.1624003793689754, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11610/20000], Loss: (1.162400329659032, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11620/20000], Loss: (1.1624003822454303, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11630/20000], Loss: (1.1624003123296416, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11640/20000], Loss: (1.1624003793544717, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11650/20000], Loss: (1.1624005260105952, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11660/20000], Loss: (1.1624003581966, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11670/20000], Loss: (1.1624005202784653, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11680/20000], Loss: (1.162400231981925, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11690/20000], Loss: (1.162400464624627, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11700/20000], Loss: (1.1624001609563315, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11710/20000], Loss: (1.1624005470409304, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11720/20000], Loss: (1.1624002713647532, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11730/20000], Loss: (1.1624004335357723, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11740/20000], Loss: (1.1624003937110732, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11750/20000], Loss: (1.1624003144741328, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11760/20000], Loss: (1.162400288151507, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11770/20000], Loss: (1.162400339940843, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11780/20000], Loss: (1.1624003822011515, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11790/20000], Loss: (1.1624003614992295, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11800/20000], Loss: (1.1624002678810739, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11810/20000], Loss: (1.1624003395043943, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11820/20000], Loss: (1.1624001990437862, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11830/20000], Loss: (1.1624003995245153, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11840/20000], Loss: (1.16240034077268, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11850/20000], Loss: (1.1624004613049232, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11860/20000], Loss: (1.1624003832890557, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11870/20000], Loss: (1.1624003421885798, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11880/20000], Loss: (1.162400326153404, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11890/20000], Loss: (1.1624002352151903, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11900/20000], Loss: (1.1624004069116487, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11910/20000], Loss: (1.162400351891043, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11920/20000], Loss: (1.162400435137681, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11930/20000], Loss: (1.162400366856286, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11940/20000], Loss: (1.1624004324935453, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11950/20000], Loss: (1.1624002408916447, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11960/20000], Loss: (1.1624004119518734, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11970/20000], Loss: (1.1624003377830434, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11980/20000], Loss: (1.1624004896040603, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [11990/20000], Loss: (1.1624002188509615, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12000/20000], Loss: (1.1624004484837611, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12010/20000], Loss: (1.162400121009228, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12020/20000], Loss: (1.1624003607631193, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12030/20000], Loss: (1.1624002935888327, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12040/20000], Loss: (1.1624004248020814, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12050/20000], Loss: (1.1624003777408738, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12060/20000], Loss: (1.1624003089426558, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12070/20000], Loss: (1.1624002929998696, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12080/20000], Loss: (1.1624001846291305, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12090/20000], Loss: (1.1624003357426385, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12100/20000], Loss: (1.1624003258023818, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12110/20000], Loss: (1.1624003625681607, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12120/20000], Loss: (1.1624003970518018, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12130/20000], Loss: (1.162400276460669, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12140/20000], Loss: (1.16240036443392, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12150/20000], Loss: (1.162400239782415, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12160/20000], Loss: (1.1624004478087258, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12170/20000], Loss: (1.1624004241756722, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12180/20000], Loss: (1.1624003416092596, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12190/20000], Loss: (1.1624003997258365, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12200/20000], Loss: (1.1624002221469507, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12210/20000], Loss: (1.162400313692834, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12220/20000], Loss: (1.1624002219031768, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12230/20000], Loss: (1.162400516777259, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12240/20000], Loss: (1.1624003488740728, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12250/20000], Loss: (1.1624004276150193, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12260/20000], Loss: (1.1624002430588987, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12270/20000], Loss: (1.1624003105171898, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12280/20000], Loss: (1.162400174375009, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12290/20000], Loss: (1.1624004386356508, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12300/20000], Loss: (1.162400226319469, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12310/20000], Loss: (1.1624004540295068, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12320/20000], Loss: (1.1624001669296662, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12330/20000], Loss: (1.16240033102517, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12340/20000], Loss: (1.162400189675827, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12350/20000], Loss: (1.162400333463358, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12360/20000], Loss: (1.162400424381883, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12370/20000], Loss: (1.1624003070616473, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12380/20000], Loss: (1.1624004095381124, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12390/20000], Loss: (1.162400190480663, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12400/20000], Loss: (1.162400328225755, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12410/20000], Loss: (1.1624001362005356, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12420/20000], Loss: (1.1624004575020006, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12430/20000], Loss: (1.1624004304550797, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12440/20000], Loss: (1.1624003835352328, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12450/20000], Loss: (1.162400378442736, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12460/20000], Loss: (1.1624002831433702, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12470/20000], Loss: (1.1624002544341658, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12480/20000], Loss: (1.1624003347262235, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12490/20000], Loss: (1.1624003484567094, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12500/20000], Loss: (1.1624004225034978, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12510/20000], Loss: (1.1624002311218065, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12520/20000], Loss: (1.1624003424728648, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12530/20000], Loss: (1.1624001163226843, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12540/20000], Loss: (1.1624003818295696, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12550/20000], Loss: (1.1624002967115725, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12560/20000], Loss: (1.1624004429230514, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12570/20000], Loss: (1.162400261078892, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12580/20000], Loss: (1.1624003240212046, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12590/20000], Loss: (1.1624001805165936, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12600/20000], Loss: (1.1624002041064538, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12610/20000], Loss: (1.1624002351316671, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12620/20000], Loss: (1.1624004488264312, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12630/20000], Loss: (1.1624003202636692, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12640/20000], Loss: (1.1624003450461606, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12650/20000], Loss: (1.162400321289584, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12660/20000], Loss: (1.1624002285348018, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12670/20000], Loss: (1.1624003297168464, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12680/20000], Loss: (1.1624003137095116, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12690/20000], Loss: (1.1624005366954882, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12700/20000], Loss: (1.162400214269444, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12710/20000], Loss: (1.1624004405411668, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12720/20000], Loss: (1.1624001035743676, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12730/20000], Loss: (1.1624003176117799, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12740/20000], Loss: (1.162400295652159, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12750/20000], Loss: (1.1624004145898374, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12760/20000], Loss: (1.1624003909896115, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12770/20000], Loss: (1.1624002990465283, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12780/20000], Loss: (1.1624002740542074, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12790/20000], Loss: (1.162400184458988, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12800/20000], Loss: (1.1624002062482186, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12810/20000], Loss: (1.1624004059539335, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12820/20000], Loss: (1.1624002569890233, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12830/20000], Loss: (1.1624003648039964, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12840/20000], Loss: (1.1624001891276434, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12850/20000], Loss: (1.162400337376523, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12860/20000], Loss: (1.1624001491445792, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12870/20000], Loss: (1.1624003715577371, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12880/20000], Loss: (1.1624004163686503, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12890/20000], Loss: (1.1624003424259046, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12900/20000], Loss: (1.1624003379283092, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12910/20000], Loss: (1.1624002289445725, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12920/20000], Loss: (1.162400256176434, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12930/20000], Loss: (1.162400228726668, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12940/20000], Loss: (1.1624004213991992, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12950/20000], Loss: (1.1624003726652372, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12960/20000], Loss: (1.162400436237339, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12970/20000], Loss: (1.1624002503155373, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12980/20000], Loss: (1.1624003202891833, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [12990/20000], Loss: (1.1624001332834029, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13000/20000], Loss: (1.162400443439894, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13010/20000], Loss: (1.1624002362077728, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13020/20000], Loss: (1.1624004636756038, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13030/20000], Loss: (1.1624001208051107, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13040/20000], Loss: (1.1624003475284261, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13050/20000], Loss: (1.1624000961949916, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13060/20000], Loss: (1.1624002625911123, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13070/20000], Loss: (1.1624004036181275, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13080/20000], Loss: (1.1624003246696262, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13090/20000], Loss: (1.162400365964471, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13100/20000], Loss: (1.162400207800061, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13110/20000], Loss: (1.1624002941194091, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13120/20000], Loss: (1.1624001684725234, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13130/20000], Loss: (1.16240031190886, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13140/20000], Loss: (1.162400355478597, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13150/20000], Loss: (1.1624003608500275, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13160/20000], Loss: (1.1624003325055317, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13170/20000], Loss: (1.1624002795898933, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13180/20000], Loss: (1.1624002796977022, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13190/20000], Loss: (1.1624003196729773, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13200/20000], Loss: (1.1624003186652756, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13210/20000], Loss: (1.1624004449538763, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13220/20000], Loss: (1.162400254918402, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13230/20000], Loss: (1.162400326411264, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13240/20000], Loss: (1.16240016113191, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13250/20000], Loss: (1.162400229181325, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13260/20000], Loss: (1.162400253513495, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13270/20000], Loss: (1.162400429549714, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13280/20000], Loss: (1.1624002837472818, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13290/20000], Loss: (1.1624003547551367, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13300/20000], Loss: (1.1624001687465457, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13310/20000], Loss: (1.1624003067586135, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13320/20000], Loss: (1.1624001017139134, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13330/20000], Loss: (1.1624004161124508, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13340/20000], Loss: (1.1624002546628924, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13350/20000], Loss: (1.1624003736813173, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13360/20000], Loss: (1.1624002098596367, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13370/20000], Loss: (1.1624002579225592, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13380/20000], Loss: (1.162400335969996, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13390/20000], Loss: (1.1624001924510003, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13400/20000], Loss: (1.1624005431618658, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13410/20000], Loss: (1.1624002460509075, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13420/20000], Loss: (1.162400462127004, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13430/20000], Loss: (1.1624001454148463, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13440/20000], Loss: (1.1624003527907476, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13450/20000], Loss: (1.1624001683763479, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13460/20000], Loss: (1.1624003538315166, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13470/20000], Loss: (1.1624003186528427, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13480/20000], Loss: (1.1624003353511518, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13490/20000], Loss: (1.1624002905023028, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13500/20000], Loss: (1.1624002816030818, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13510/20000], Loss: (1.162400193719672, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13520/20000], Loss: (1.1624003091427444, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13530/20000], Loss: (1.1624002650098786, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13540/20000], Loss: (1.1624003675285228, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13550/20000], Loss: (1.1624002120291954, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13560/20000], Loss: (1.1624002445173107, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13570/20000], Loss: (1.1624002647642853, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13580/20000], Loss: (1.1624001789620244, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13590/20000], Loss: (1.162400328595476, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13600/20000], Loss: (1.1624003769066533, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13610/20000], Loss: (1.1624003722451635, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13620/20000], Loss: (1.1624002759560907, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13630/20000], Loss: (1.1624002942712086, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13640/20000], Loss: (1.162400317794809, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13650/20000], Loss: (1.1624002218199814, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13660/20000], Loss: (1.1624003595855925, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13670/20000], Loss: (1.1624003254990494, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13680/20000], Loss: (1.1624002923525496, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13690/20000], Loss: (1.1624003237690392, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13700/20000], Loss: (1.1624001767067882, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13710/20000], Loss: (1.1624003958996256, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13720/20000], Loss: (1.162400128655736, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13730/20000], Loss: (1.1624005085752738, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13740/20000], Loss: (1.1624001663551544, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13750/20000], Loss: (1.162400392557999, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13760/20000], Loss: (1.1624001608604269, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13770/20000], Loss: (1.1624002791793577, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13780/20000], Loss: (1.1624001199966765, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13790/20000], Loss: (1.1624002851145, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13800/20000], Loss: (1.1624003428718754, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13810/20000], Loss: (1.1624002608195545, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13820/20000], Loss: (1.162400350685352, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13830/20000], Loss: (1.1624003013135158, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13840/20000], Loss: (1.1624002758523193, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13850/20000], Loss: (1.1624002249503287, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13860/20000], Loss: (1.1624003998705041, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13870/20000], Loss: (1.1624002890747385, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13880/20000], Loss: (1.1624003828060236, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13890/20000], Loss: (1.1624001756536495, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13900/20000], Loss: (1.1624004027858579, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13910/20000], Loss: (1.1624001200473595, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13920/20000], Loss: (1.1624003943357126, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13930/20000], Loss: (1.1624003101836284, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13940/20000], Loss: (1.162400380017525, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13950/20000], Loss: (1.162400278238171, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13960/20000], Loss: (1.1624002658958696, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13970/20000], Loss: (1.162400243552599, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13980/20000], Loss: (1.1624001710392975, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [13990/20000], Loss: (1.1624003015292963, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14000/20000], Loss: (1.1624002570995533, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14010/20000], Loss: (1.1624002492721939, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14020/20000], Loss: (1.1624003402325522, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14030/20000], Loss: (1.162400184833158, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14040/20000], Loss: (1.1624003391363193, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14050/20000], Loss: (1.1624001866754787, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14060/20000], Loss: (1.1624004349397552, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14070/20000], Loss: (1.1624002839558079, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14080/20000], Loss: (1.1624003193170844, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14090/20000], Loss: (1.1624003385521862, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14100/20000], Loss: (1.1624002062660785, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14110/20000], Loss: (1.162400263205505, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14120/20000], Loss: (1.1624002303935985, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14130/20000], Loss: (1.162400449829308, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14140/20000], Loss: (1.162400259053139, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14150/20000], Loss: (1.162400415132814, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14160/20000], Loss: (1.162400232244334, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14170/20000], Loss: (1.1624003013468807, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14180/20000], Loss: (1.162400156260724, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14190/20000], Loss: (1.1624003845657784, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14200/20000], Loss: (1.162400219125487, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14210/20000], Loss: (1.1624003994538483, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14220/20000], Loss: (1.1624001052211774, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14230/20000], Loss: (1.1624003359549364, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14240/20000], Loss: (1.162400078094366, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14250/20000], Loss: (1.1624003188379317, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14260/20000], Loss: (1.1624003322425558, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14270/20000], Loss: (1.1624003094051454, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14280/20000], Loss: (1.1624004073935859, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14290/20000], Loss: (1.162400195132574, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14300/20000], Loss: (1.162400336819559, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14310/20000], Loss: (1.162400091979299, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14320/20000], Loss: (1.1624004270568804, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14330/20000], Loss: (1.1624002347193545, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14340/20000], Loss: (1.1624004002495338, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14350/20000], Loss: (1.1624002912501457, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14360/20000], Loss: (1.1624002954477914, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14370/20000], Loss: (1.1624002624232244, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14380/20000], Loss: (1.1624002468654182, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14390/20000], Loss: (1.16240036587834, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14400/20000], Loss: (1.1624003659320916, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14410/20000], Loss: (1.1624002505358653, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14420/20000], Loss: (1.1624003280580217, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14430/20000], Loss: (1.162400137861071, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14440/20000], Loss: (1.1624002129078534, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14450/20000], Loss: (1.1624001546169036, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14460/20000], Loss: (1.162400370821946, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14470/20000], Loss: (1.1624002928978603, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14480/20000], Loss: (1.1624003405049326, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14490/20000], Loss: (1.162400251767901, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14500/20000], Loss: (1.1624002351093323, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14510/20000], Loss: (1.1624002199821846, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14520/20000], Loss: (1.162400328293555, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14530/20000], Loss: (1.1624003302526416, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14540/20000], Loss: (1.1624003812019, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14550/20000], Loss: (1.1624002546222667, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14560/20000], Loss: (1.1624002661005821, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14570/20000], Loss: (1.1624002157929518, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14580/20000], Loss: (1.162400241320619, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14590/20000], Loss: (1.1624004859089885, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14600/20000], Loss: (1.1624002452581184, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14610/20000], Loss: (1.162400479857886, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14620/20000], Loss: (1.1624001305748486, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14630/20000], Loss: (1.1624003621714096, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14640/20000], Loss: (1.1624000183634957, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14650/20000], Loss: (1.162400399109185, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14660/20000], Loss: (1.1624002377440923, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14670/20000], Loss: (1.162400347237604, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14680/20000], Loss: (1.1624002064401036, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14690/20000], Loss: (1.1624002298950822, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14700/20000], Loss: (1.1624002276795058, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14710/20000], Loss: (1.1624001720919308, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14720/20000], Loss: (1.1624003957017517, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14730/20000], Loss: (1.1624003740857765, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14740/20000], Loss: (1.162400324667363, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14750/20000], Loss: (1.1624002578997024, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14760/20000], Loss: (1.1624002555579094, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14770/20000], Loss: (1.1624001427794832, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14780/20000], Loss: (1.1624003473040778, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14790/20000], Loss: (1.1624002829024418, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14800/20000], Loss: (1.1624004755848403, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14810/20000], Loss: (1.162400257916151, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14820/20000], Loss: (1.1624003562399203, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14830/20000], Loss: (1.1624001712139953, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14840/20000], Loss: (1.1624002619013603, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14850/20000], Loss: (1.1624003182897384, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14860/20000], Loss: (1.1624003417438833, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14870/20000], Loss: (1.1624003174959687, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14880/20000], Loss: (1.1624002282172226, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14890/20000], Loss: (1.1624002020629818, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14900/20000], Loss: (1.1624001346966386, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14910/20000], Loss: (1.1624001698580535, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14920/20000], Loss: (1.1624004674918733, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14930/20000], Loss: (1.16240025875981, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14940/20000], Loss: (1.162400414011329, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14950/20000], Loss: (1.1624001850053367, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14960/20000], Loss: (1.1624002987941382, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14970/20000], Loss: (1.1624001284502712, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14980/20000], Loss: (1.162400319602056, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [14990/20000], Loss: (1.1624004168834738, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15000/20000], Loss: (1.162400285152149, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15010/20000], Loss: (1.162400341959981, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15020/20000], Loss: (1.162400171831172, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15030/20000], Loss: (1.16240033689215, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15040/20000], Loss: (1.162400147871336, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15050/20000], Loss: (1.1624004725621482, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15060/20000], Loss: (1.1624003127382414, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15070/20000], Loss: (1.1624003798173197, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15080/20000], Loss: (1.1624001970019107, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15090/20000], Loss: (1.1624002669950282, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15100/20000], Loss: (1.162400083528413, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15110/20000], Loss: (1.1624003618361434, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15120/20000], Loss: (1.162400188766717, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15130/20000], Loss: (1.1624004176920717, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15140/20000], Loss: (1.162400184247711, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15150/20000], Loss: (1.1624002947340917, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15160/20000], Loss: (1.162400193976916, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15170/20000], Loss: (1.1624001798572865, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15180/20000], Loss: (1.162400411746254, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15190/20000], Loss: (1.162400282456441, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15200/20000], Loss: (1.1624004240447519, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15210/20000], Loss: (1.1624001691805814, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15220/20000], Loss: (1.16240035009452, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15230/20000], Loss: (1.162400079586455, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15240/20000], Loss: (1.1624003307341373, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15250/20000], Loss: (1.1624003731516321, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15260/20000], Loss: (1.1624003802675107, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15270/20000], Loss: (1.1624003522402253, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15280/20000], Loss: (1.1624002648995084, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15290/20000], Loss: (1.1624002368437745, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15300/20000], Loss: (1.1624002176068078, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15310/20000], Loss: (1.1624002314996627, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15320/20000], Loss: (1.1624004146096458, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15330/20000], Loss: (1.1624002245433558, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15340/20000], Loss: (1.162400299759021, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15350/20000], Loss: (1.1624001129024124, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15360/20000], Loss: (1.1624002361647645, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15370/20000], Loss: (1.1624001779765532, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15380/20000], Loss: (1.1624003731904158, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15390/20000], Loss: (1.1624003563171232, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15400/20000], Loss: (1.162400320609135, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15410/20000], Loss: (1.1624002868431753, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15420/20000], Loss: (1.1624002104168212, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15430/20000], Loss: (1.162400211989464, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15440/20000], Loss: (1.162400316179439, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15450/20000], Loss: (1.1624003517168822, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15460/20000], Loss: (1.1624003579397169, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15470/20000], Loss: (1.1624003093135338, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15480/20000], Loss: (1.1624002389994712, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15490/20000], Loss: (1.1624003202251034, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15500/20000], Loss: (1.1624001222246119, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15510/20000], Loss: (1.1624004894494908, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15520/20000], Loss: (1.1624002261924289, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15530/20000], Loss: (1.1624004545272977, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15540/20000], Loss: (1.1624001132088473, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15550/20000], Loss: (1.1624003408537917, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15560/20000], Loss: (1.1624000740306513, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15570/20000], Loss: (1.162400249689315, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15580/20000], Loss: (1.162400259736935, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15590/20000], Loss: (1.1624003211269194, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15600/20000], Loss: (1.1624003324657444, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15610/20000], Loss: (1.16240020624429, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15620/20000], Loss: (1.1624002992266256, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15630/20000], Loss: (1.162400214580482, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15640/20000], Loss: (1.1624003072850915, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15650/20000], Loss: (1.1624003570491062, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15660/20000], Loss: (1.1624003737652997, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15670/20000], Loss: (1.162400242348196, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15680/20000], Loss: (1.1624003006237829, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15690/20000], Loss: (1.162400132048025, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15700/20000], Loss: (1.1624003558630747, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15710/20000], Loss: (1.1624002347509987, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15720/20000], Loss: (1.162400450624088, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15730/20000], Loss: (1.1624002649595275, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15740/20000], Loss: (1.1624003362166204, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15750/20000], Loss: (1.1624002034485958, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15760/20000], Loss: (1.1624002248625163, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15770/20000], Loss: (1.1624002367349138, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15780/20000], Loss: (1.1624002998436653, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15790/20000], Loss: (1.1624003009765784, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15800/20000], Loss: (1.1624002146511279, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15810/20000], Loss: (1.1624002304530525, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15820/20000], Loss: (1.1624002660066044, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15830/20000], Loss: (1.1624001666976358, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15840/20000], Loss: (1.162400390400395, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15850/20000], Loss: (1.162400315647173, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15860/20000], Loss: (1.1624003970958312, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15870/20000], Loss: (1.1624002414913601, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15880/20000], Loss: (1.1624002830715494, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15890/20000], Loss: (1.1624002915009932, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15900/20000], Loss: (1.1624001713229524, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15910/20000], Loss: (1.1624003856698992, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15920/20000], Loss: (1.1624002686193151, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15930/20000], Loss: (1.162400401644431, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15940/20000], Loss: (1.1624001705956821, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15950/20000], Loss: (1.1624003669477938, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15960/20000], Loss: (1.162400198615841, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15970/20000], Loss: (1.1624003199112614, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15980/20000], Loss: (1.1624002708452297, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [15990/20000], Loss: (1.1624003683156945, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16000/20000], Loss: (1.1624001861544135, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16010/20000], Loss: (1.1624003118041066, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16020/20000], Loss: (1.1624000837130903, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16030/20000], Loss: (1.1624002999913792, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16040/20000], Loss: (1.1624001602707863, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16050/20000], Loss: (1.1624003957376368, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16060/20000], Loss: (1.162400312803758, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16070/20000], Loss: (1.1624002800856748, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16080/20000], Loss: (1.1624003466992885, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16090/20000], Loss: (1.162400168177229, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16100/20000], Loss: (1.162400345960659, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16110/20000], Loss: (1.1624002055170357, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16120/20000], Loss: (1.162400484024968, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16130/20000], Loss: (1.1624001595220705, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16140/20000], Loss: (1.1624003819326185, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16150/20000], Loss: (1.1624001995439197, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16160/20000], Loss: (1.1624002672469378, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16170/20000], Loss: (1.1624002439181913, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16180/20000], Loss: (1.1624003497254012, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16190/20000], Loss: (1.1624003420929632, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16200/20000], Loss: (1.1624002737738552, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16210/20000], Loss: (1.1624002299470741, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16220/20000], Loss: (1.162400304888458, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16230/20000], Loss: (1.162400116876928, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16240/20000], Loss: (1.1624002926874752, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16250/20000], Loss: (1.1624002616566886, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16260/20000], Loss: (1.1624002906056818, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16270/20000], Loss: (1.1624002620492468, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16280/20000], Loss: (1.1624002603024965, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16290/20000], Loss: (1.162400285235981, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16300/20000], Loss: (1.1624002150887183, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16310/20000], Loss: (1.1624003645595566, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16320/20000], Loss: (1.162400313633339, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16330/20000], Loss: (1.1624003549495625, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16340/20000], Loss: (1.1624003016449969, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16350/20000], Loss: (1.1624002738746795, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16360/20000], Loss: (1.1624002467749641, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16370/20000], Loss: (1.1624002053630165, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16380/20000], Loss: (1.1624003297581615, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16390/20000], Loss: (1.1624003291318938, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16400/20000], Loss: (1.162400229854282, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16410/20000], Loss: (1.162400424158242, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16420/20000], Loss: (1.1624001164476327, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16430/20000], Loss: (1.162400353291142, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16440/20000], Loss: (1.1624001088693037, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16450/20000], Loss: (1.1624004485859993, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16460/20000], Loss: (1.1624001458353805, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16470/20000], Loss: (1.16240032957467, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16480/20000], Loss: (1.162400219012443, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16490/20000], Loss: (1.1624002135441942, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16500/20000], Loss: (1.1624002053083997, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16510/20000], Loss: (1.1624002593363572, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16520/20000], Loss: (1.162400434508788, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16530/20000], Loss: (1.1624002668616185, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16540/20000], Loss: (1.1624003769641522, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16550/20000], Loss: (1.1624002510422478, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16560/20000], Loss: (1.1624003024044298, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16570/20000], Loss: (1.1624001896298009, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16580/20000], Loss: (1.1624004061811446, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16590/20000], Loss: (1.1624002328898568, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16600/20000], Loss: (1.1624004132845742, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16610/20000], Loss: (1.162400145801375, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16620/20000], Loss: (1.1624003485736065, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16630/20000], Loss: (1.1624001166047748, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16640/20000], Loss: (1.1624003463972787, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16650/20000], Loss: (1.1624002637042083, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16660/20000], Loss: (1.1624003360485387, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16670/20000], Loss: (1.1624002919511487, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16680/20000], Loss: (1.1624002191832552, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16690/20000], Loss: (1.1624002422834316, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16700/20000], Loss: (1.1624001095113983, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16710/20000], Loss: (1.1624003398058138, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16720/20000], Loss: (1.1624002377305749, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16730/20000], Loss: (1.1624003239874627, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16740/20000], Loss: (1.1624003419455065, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16750/20000], Loss: (1.1624002531415965, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16760/20000], Loss: (1.162400296088214, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16770/20000], Loss: (1.162400237783546, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16780/20000], Loss: (1.1624003954653925, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16790/20000], Loss: (1.162400377383687, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16800/20000], Loss: (1.1624002743472717, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16810/20000], Loss: (1.1624003513083927, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16820/20000], Loss: (1.1624001601940679, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16830/20000], Loss: (1.162400235116316, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16840/20000], Loss: (1.1624001613317692, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16850/20000], Loss: (1.1624004031212993, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16860/20000], Loss: (1.1624002647904448, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16870/20000], Loss: (1.1624003796327804, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16880/20000], Loss: (1.1624001975681724, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16890/20000], Loss: (1.162400269139884, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16900/20000], Loss: (1.1624001174030643, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16910/20000], Loss: (1.1624003088546309, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16920/20000], Loss: (1.1624002665434294, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16930/20000], Loss: (1.16240039402354, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16940/20000], Loss: (1.1624001981125192, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16950/20000], Loss: (1.1624002953119386, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16960/20000], Loss: (1.1624001680966953, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16970/20000], Loss: (1.1624002446423483, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16980/20000], Loss: (1.1624004251062732, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [16990/20000], Loss: (1.1624002837491434, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17000/20000], Loss: (1.1624005035627578, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17010/20000], Loss: (1.1624001676127795, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17020/20000], Loss: (1.1624003959691809, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17030/20000], Loss: (1.162400051401544, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17040/20000], Loss: (1.1624003952665833, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17050/20000], Loss: (1.1624002285350825, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17060/20000], Loss: (1.1624003781880943, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17070/20000], Loss: (1.1624002254293535, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17080/20000], Loss: (1.1624002669064548, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17090/20000], Loss: (1.1624002009184211, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17100/20000], Loss: (1.1624001651678253, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17110/20000], Loss: (1.162400300835235, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17120/20000], Loss: (1.162400372695485, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17130/20000], Loss: (1.1624002816109813, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17140/20000], Loss: (1.1624002969279592, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17150/20000], Loss: (1.1624002173905417, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17160/20000], Loss: (1.162400182385151, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17170/20000], Loss: (1.1624002164457, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17180/20000], Loss: (1.1624002871163002, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17190/20000], Loss: (1.1624004512593378, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17200/20000], Loss: (1.1624002573133692, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17210/20000], Loss: (1.1624003772517206, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17220/20000], Loss: (1.1624002170755297, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17230/20000], Loss: (1.1624002858120879, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17240/20000], Loss: (1.16240025156035, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17250/20000], Loss: (1.1624003837076096, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17260/20000], Loss: (1.1624003568527344, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17270/20000], Loss: (1.1624002702320038, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17280/20000], Loss: (1.1624002429724667, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17290/20000], Loss: (1.1624001575709804, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17300/20000], Loss: (1.1624001475798795, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17310/20000], Loss: (1.162400344249383, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17320/20000], Loss: (1.1624002316161615, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17330/20000], Loss: (1.162400449385729, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17340/20000], Loss: (1.162400159704302, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17350/20000], Loss: (1.1624003427814946, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17360/20000], Loss: (1.1624000955442013, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17370/20000], Loss: (1.1624002884755251, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17380/20000], Loss: (1.1624003546939417, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17390/20000], Loss: (1.1624003277090387, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17400/20000], Loss: (1.162400325746916, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17410/20000], Loss: (1.1624002150330908, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17420/20000], Loss: (1.1624002890261969, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17430/20000], Loss: (1.1624001047959147, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17440/20000], Loss: (1.1624004214502552, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17450/20000], Loss: (1.162400360760175, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17460/20000], Loss: (1.1624004277611628, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17470/20000], Loss: (1.1624002463498457, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17480/20000], Loss: (1.1624003168509602, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17490/20000], Loss: (1.1624001317637775, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17500/20000], Loss: (1.1624002648767062, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17510/20000], Loss: (1.1624001947472995, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17520/20000], Loss: (1.1624004657169893, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17530/20000], Loss: (1.1624001307253626, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17540/20000], Loss: (1.1624003506240823, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17550/20000], Loss: (1.1624001260491306, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17560/20000], Loss: (1.1624002298868743, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17570/20000], Loss: (1.1624002622616734, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17580/20000], Loss: (1.1624003257137907, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17590/20000], Loss: (1.162400425273424, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17600/20000], Loss: (1.1624002180013873, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17610/20000], Loss: (1.1624003536120127, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17620/20000], Loss: (1.1624001059453848, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17630/20000], Loss: (1.1624002825304056, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17640/20000], Loss: (1.1624002317112336, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17650/20000], Loss: (1.1624004295933805, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17660/20000], Loss: (1.1624003303098418, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17670/20000], Loss: (1.1624003194187624, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17680/20000], Loss: (1.1624002921465304, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17690/20000], Loss: (1.1624002098156228, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17700/20000], Loss: (1.162400197660142, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17710/20000], Loss: (1.1624004207578105, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17720/20000], Loss: (1.162400277433426, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17730/20000], Loss: (1.1624003546073, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17740/20000], Loss: (1.162400166830091, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17750/20000], Loss: (1.1624002413390067, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17760/20000], Loss: (1.1624000919702828, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17770/20000], Loss: (1.1624002416034913, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17780/20000], Loss: (1.1624003632502111, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17790/20000], Loss: (1.162400369994212, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17800/20000], Loss: (1.1624003001889167, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17810/20000], Loss: (1.162400266442831, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17820/20000], Loss: (1.1624002311544306, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17830/20000], Loss: (1.1624002242103788, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17840/20000], Loss: (1.1624002812430865, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17850/20000], Loss: (1.1624004148362848, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17860/20000], Loss: (1.1624003108863605, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17870/20000], Loss: (1.1624003007972663, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17880/20000], Loss: (1.1624002391900805, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17890/20000], Loss: (1.1624001809528022, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17900/20000], Loss: (1.1624003412045594, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17910/20000], Loss: (1.1624002261522575, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17920/20000], Loss: (1.1624005145071372, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17930/20000], Loss: (1.1624001700417101, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17940/20000], Loss: (1.1624004002634802, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17950/20000], Loss: (1.1624000613288765, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17960/20000], Loss: (1.1624002870665218, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17970/20000], Loss: (1.162400170771226, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17980/20000], Loss: (1.162400388345272, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [17990/20000], Loss: (1.1624002405224039, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18000/20000], Loss: (1.1624002670861293, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18010/20000], Loss: (1.1624002602468113, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18020/20000], Loss: (1.1624002046104518, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18030/20000], Loss: (1.1624002572579233, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18040/20000], Loss: (1.1624003193328827, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18050/20000], Loss: (1.1624004040795954, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18060/20000], Loss: (1.1624003020506932, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18070/20000], Loss: (1.1624003326364243, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18080/20000], Loss: (1.1624001898112601, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18090/20000], Loss: (1.1624003298143264, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18100/20000], Loss: (1.1624000838902444, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18110/20000], Loss: (1.1624004732106363, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18120/20000], Loss: (1.1624002403587406, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18130/20000], Loss: (1.1624004018287049, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18140/20000], Loss: (1.1624002084336882, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18150/20000], Loss: (1.1624002873653458, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18160/20000], Loss: (1.1624002211131628, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18170/20000], Loss: (1.1624002427424671, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18180/20000], Loss: (1.1624003648633767, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18190/20000], Loss: (1.1624002784680891, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18200/20000], Loss: (1.1624002610044994, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18210/20000], Loss: (1.1624001700389004, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18220/20000], Loss: (1.1624002032387037, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18230/20000], Loss: (1.162400236748777, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18240/20000], Loss: (1.1624002517844143, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18250/20000], Loss: (1.1624004306164133, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18260/20000], Loss: (1.1624002857323703, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18270/20000], Loss: (1.1624003490452466, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18280/20000], Loss: (1.162400250013453, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18290/20000], Loss: (1.1624002383391578, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18300/20000], Loss: (1.1624003135128294, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18310/20000], Loss: (1.1624002980145884, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18320/20000], Loss: (1.1624004089795787, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18330/20000], Loss: (1.1624002195446592, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18340/20000], Loss: (1.1624003017194064, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18350/20000], Loss: (1.1624001850163976, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18360/20000], Loss: (1.16240027210454, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18370/20000], Loss: (1.162400211594536, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18380/20000], Loss: (1.1624004408019062, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18390/20000], Loss: (1.1624002552632149, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18400/20000], Loss: (1.1624003253401747, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18410/20000], Loss: (1.1624001416458267, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18420/20000], Loss: (1.162400329661412, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18430/20000], Loss: (1.1624000726966592, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18440/20000], Loss: (1.1624003712681952, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18450/20000], Loss: (1.162400231483037, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18460/20000], Loss: (1.1624003502948452, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18470/20000], Loss: (1.1624002283366555, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18480/20000], Loss: (1.1624002376816671, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18490/20000], Loss: (1.1624003828314515, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18500/20000], Loss: (1.1624001412662444, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18510/20000], Loss: (1.162400502545228, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18520/20000], Loss: (1.162400230133989, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18530/20000], Loss: (1.1624004567045645, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18540/20000], Loss: (1.1624001572891691, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18550/20000], Loss: (1.1624003396207458, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18560/20000], Loss: (1.1624001572632001, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18570/20000], Loss: (1.1624002840963414, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18580/20000], Loss: (1.1624002971987242, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18590/20000], Loss: (1.1624003280607753, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18600/20000], Loss: (1.1624002709776726, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18610/20000], Loss: (1.1624002859398865, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18620/20000], Loss: (1.1624001882860806, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18630/20000], Loss: (1.1624002659910062, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18640/20000], Loss: (1.16240022035704, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18650/20000], Loss: (1.1624003579458508, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18660/20000], Loss: (1.1624002593857132, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18670/20000], Loss: (1.162400242098505, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18680/20000], Loss: (1.1624003096973885, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18690/20000], Loss: (1.1624001412098885, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18700/20000], Loss: (1.1624002893062895, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18710/20000], Loss: (1.1624002803905482, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18720/20000], Loss: (1.1624004217872346, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18730/20000], Loss: (1.1624002763093575, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18740/20000], Loss: (1.1624003464565074, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18750/20000], Loss: (1.1624003189974184, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18760/20000], Loss: (1.1624002276211454, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18770/20000], Loss: (1.1624002578187043, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18780/20000], Loss: (1.1624003247015882, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18790/20000], Loss: (1.1624003030177528, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18800/20000], Loss: (1.1624002575945245, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18810/20000], Loss: (1.1624001897990426, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18820/20000], Loss: (1.1624003288415623, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18830/20000], Loss: (1.1624000782541388, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18840/20000], Loss: (1.1624004219849817, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18850/20000], Loss: (1.1624001921424612, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18860/20000], Loss: (1.1624004032856423, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18870/20000], Loss: (1.1624002159982352, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18880/20000], Loss: (1.1624002875367128, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18890/20000], Loss: (1.1624002216372689, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18900/20000], Loss: (1.1624001861953255, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18910/20000], Loss: (1.1624003165635735, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18920/20000], Loss: (1.1624002770551423, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18930/20000], Loss: (1.1624003398215648, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18940/20000], Loss: (1.1624002821363222, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18950/20000], Loss: (1.1624003498097726, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18960/20000], Loss: (1.1624002109768736, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18970/20000], Loss: (1.1624003415424935, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18980/20000], Loss: (1.1624002980121932, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [18990/20000], Loss: (1.162400382372957, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19000/20000], Loss: (1.1624001935680988, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19010/20000], Loss: (1.1624004185292065, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19020/20000], Loss: (1.162400081644518, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19030/20000], Loss: (1.162400314270433, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19040/20000], Loss: (1.162400096706776, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19050/20000], Loss: (1.162400413495712, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19060/20000], Loss: (1.1624002166999332, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19070/20000], Loss: (1.1624002957821415, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19080/20000], Loss: (1.162400320007044, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19090/20000], Loss: (1.162400180091586, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19100/20000], Loss: (1.1624002647905853, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19110/20000], Loss: (1.1624002236444428, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19120/20000], Loss: (1.1624004034134896, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19130/20000], Loss: (1.1624002413005665, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19140/20000], Loss: (1.1624003315770788, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19150/20000], Loss: (1.1624002179289559, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19160/20000], Loss: (1.162400259590735, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19170/20000], Loss: (1.1624002350605023, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19180/20000], Loss: (1.1624003790837723, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19190/20000], Loss: (1.1624003551511664, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19200/20000], Loss: (1.1624003880605667, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19210/20000], Loss: (1.1624002412977192, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19220/20000], Loss: (1.1624003158554008, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19230/20000], Loss: (1.1624001278505602, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19240/20000], Loss: (1.1624003112941657, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19250/20000], Loss: (1.1624002341784923, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19260/20000], Loss: (1.1624003034225114, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19270/20000], Loss: (1.1624002738507906, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19280/20000], Loss: (1.16240023461726, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19290/20000], Loss: (1.162400192936093, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19300/20000], Loss: (1.1624002128597868, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19310/20000], Loss: (1.1624002846424062, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19320/20000], Loss: (1.162400363026095, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19330/20000], Loss: (1.1624002797777768, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19340/20000], Loss: (1.1624003785795687, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19350/20000], Loss: (1.1624002102128883, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19360/20000], Loss: (1.1624002642017768, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19370/20000], Loss: (1.1624001944895201, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19380/20000], Loss: (1.1624003496866486, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19390/20000], Loss: (1.1624003709104906, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19400/20000], Loss: (1.1624002465979215, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19410/20000], Loss: (1.1624004342305179, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19420/20000], Loss: (1.1624001317791122, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19430/20000], Loss: (1.1624003674415964, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19440/20000], Loss: (1.162400120536814, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19450/20000], Loss: (1.1624004636601728, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19460/20000], Loss: (1.1624002405336884, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19470/20000], Loss: (1.1624003501533058, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19480/20000], Loss: (1.1624001702928959, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19490/20000], Loss: (1.1624002393058703, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19500/20000], Loss: (1.16240006870592, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19510/20000], Loss: (1.1624002706815169, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19520/20000], Loss: (1.1624003137368915, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19530/20000], Loss: (1.1624003788247035, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19540/20000], Loss: (1.1624003104512572, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19550/20000], Loss: (1.1624002665530564, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19560/20000], Loss: (1.162400239814742, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19570/20000], Loss: (1.1624001980744647, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19580/20000], Loss: (1.1624004043278915, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19590/20000], Loss: (1.1624002571491059, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19600/20000], Loss: (1.1624004711675762, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19610/20000], Loss: (1.1624001428343418, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19620/20000], Loss: (1.1624003686625577, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19630/20000], Loss: (1.1624000772740997, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19640/20000], Loss: (1.1624003543493597, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19650/20000], Loss: (1.1624003441158206, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19660/20000], Loss: (1.1624003518506874, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19670/20000], Loss: (1.1624003300017791, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19680/20000], Loss: (1.162400240378296, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19690/20000], Loss: (1.1624002168337948, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19700/20000], Loss: (1.162400130136467, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19710/20000], Loss: (1.1624002549008225, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19720/20000], Loss: (1.1624003486515968, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19730/20000], Loss: (1.1624002559851212, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19740/20000], Loss: (1.1624002965085434, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19750/20000], Loss: (1.1624001889271385, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19760/20000], Loss: (1.1624002642106679, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19770/20000], Loss: (1.1624001695189843, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19780/20000], Loss: (1.162400403290306, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19790/20000], Loss: (1.1624004225063467, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19800/20000], Loss: (1.162400302385607, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19810/20000], Loss: (1.1624003482657184, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19820/20000], Loss: (1.1624001908300272, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19830/20000], Loss: (1.1624002608228647, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19840/20000], Loss: (1.1624002082037836, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19850/20000], Loss: (1.1624003598696837, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19860/20000], Loss: (1.162400331581383, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19870/20000], Loss: (1.162400324552624, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19880/20000], Loss: (1.162400216554816, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19890/20000], Loss: (1.1624002903200727, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19900/20000], Loss: (1.1624001040960303, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19910/20000], Loss: (1.1624003932386644, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19920/20000], Loss: (1.1624002089656431, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19930/20000], Loss: (1.1624004411587183, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19940/20000], Loss: (1.1624001309618766, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19950/20000], Loss: (1.162400318279202, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19960/20000], Loss: (1.1624000672135049, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19970/20000], Loss: (1.1624002435798313, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19980/20000], Loss: (1.1624003628560895, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [19990/20000], Loss: (1.16240030404138, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n",
            "Epoch [20000/20000], Loss: (1.1624004373734167, tensor(0.0985, dtype=torch.float64, grad_fn=<MulBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_sl = sqrtLasso(input_size=x.shape[1])"
      ],
      "metadata": {
        "id": "qkTil80ImnBB"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_sl.fit(x,y, num_epochs = 20000,learning_rate = .009 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ztXGxxvm2zM",
        "outputId": "ee4b1a9c-0272-4d43-f0d0-f69ce68df873"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/20000], Loss: 51.80284647549593\n",
            "Epoch [200/20000], Loss: 46.83016926410336\n",
            "Epoch [300/20000], Loss: 43.69513091237008\n",
            "Epoch [400/20000], Loss: 41.07820357037198\n",
            "Epoch [500/20000], Loss: 39.058964776290175\n",
            "Epoch [600/20000], Loss: 37.51248298943701\n",
            "Epoch [700/20000], Loss: 36.344010497352805\n",
            "Epoch [800/20000], Loss: 35.52069845687445\n",
            "Epoch [900/20000], Loss: 34.981071348522775\n",
            "Epoch [1000/20000], Loss: 34.61557839427149\n",
            "Epoch [1100/20000], Loss: 34.375098434251115\n",
            "Epoch [1200/20000], Loss: 34.221865309711355\n",
            "Epoch [1300/20000], Loss: 34.12757798379472\n",
            "Epoch [1400/20000], Loss: 34.07195825371007\n",
            "Epoch [1500/20000], Loss: 34.04307266099307\n",
            "Epoch [1600/20000], Loss: 34.02874772868155\n",
            "Epoch [1700/20000], Loss: 34.021387128422845\n",
            "Epoch [1800/20000], Loss: 34.01775889830015\n",
            "Epoch [1900/20000], Loss: 34.01604424561523\n",
            "Epoch [2000/20000], Loss: 34.01531030815597\n",
            "Epoch [2100/20000], Loss: 34.01497777824369\n",
            "Epoch [2200/20000], Loss: 34.01484889650445\n",
            "Epoch [2300/20000], Loss: 34.014811240802075\n",
            "Epoch [2400/20000], Loss: 34.01482053280313\n",
            "Epoch [2500/20000], Loss: 34.014807564404265\n",
            "Epoch [2600/20000], Loss: 34.01481883682288\n",
            "Epoch [2700/20000], Loss: 34.014787509530365\n",
            "Epoch [2800/20000], Loss: 34.01480679126447\n",
            "Epoch [2900/20000], Loss: 34.01483006705881\n",
            "Epoch [3000/20000], Loss: 34.01482493094159\n",
            "Epoch [3100/20000], Loss: 34.014883553422784\n",
            "Epoch [3200/20000], Loss: 34.014824794631984\n",
            "Epoch [3300/20000], Loss: 34.01479578143718\n",
            "Epoch [3400/20000], Loss: 34.01489010011961\n",
            "Epoch [3500/20000], Loss: 34.01485508380038\n",
            "Epoch [3600/20000], Loss: 34.01486468126165\n",
            "Epoch [3700/20000], Loss: 34.01485734366106\n",
            "Epoch [3800/20000], Loss: 34.01483790410304\n",
            "Epoch [3900/20000], Loss: 34.01483157014813\n",
            "Epoch [4000/20000], Loss: 34.014938722126494\n",
            "Epoch [4100/20000], Loss: 34.01481746600892\n",
            "Epoch [4200/20000], Loss: 34.0149462190447\n",
            "Epoch [4300/20000], Loss: 34.01485121543586\n",
            "Epoch [4400/20000], Loss: 34.014906619671216\n",
            "Epoch [4500/20000], Loss: 34.01489485241541\n",
            "Epoch [4600/20000], Loss: 34.014860115777864\n",
            "Epoch [4700/20000], Loss: 34.01489465674077\n",
            "Epoch [4800/20000], Loss: 34.01486107896573\n",
            "Epoch [4900/20000], Loss: 34.01483889049941\n",
            "Epoch [5000/20000], Loss: 34.01496309902672\n",
            "Epoch [5100/20000], Loss: 34.01496561241453\n",
            "Epoch [5200/20000], Loss: 34.01484242220092\n",
            "Epoch [5300/20000], Loss: 34.01503430013617\n",
            "Epoch [5400/20000], Loss: 34.01485593001091\n",
            "Epoch [5500/20000], Loss: 34.014993219411856\n",
            "Epoch [5600/20000], Loss: 34.014922398749825\n",
            "Epoch [5700/20000], Loss: 34.015068212406646\n",
            "Epoch [5800/20000], Loss: 34.0149548161124\n",
            "Epoch [5900/20000], Loss: 34.014919091906584\n",
            "Epoch [6000/20000], Loss: 34.014981260549675\n",
            "Epoch [6100/20000], Loss: 34.0148388910988\n",
            "Epoch [6200/20000], Loss: 34.01489341835473\n",
            "Epoch [6300/20000], Loss: 34.014954626930646\n",
            "Epoch [6400/20000], Loss: 34.01501666327226\n",
            "Epoch [6500/20000], Loss: 34.01497032243134\n",
            "Epoch [6600/20000], Loss: 34.014862296126445\n",
            "Epoch [6700/20000], Loss: 34.01505654779214\n",
            "Epoch [6800/20000], Loss: 34.0149711693313\n",
            "Epoch [6900/20000], Loss: 34.01509329033769\n",
            "Epoch [7000/20000], Loss: 34.01513369283772\n",
            "Epoch [7100/20000], Loss: 34.01502480912521\n",
            "Epoch [7200/20000], Loss: 34.015131129661064\n",
            "Epoch [7300/20000], Loss: 34.01501361913354\n",
            "Epoch [7400/20000], Loss: 34.015224933754475\n",
            "Epoch [7500/20000], Loss: 34.01524287747323\n",
            "Epoch [7600/20000], Loss: 34.01509901916839\n",
            "Epoch [7700/20000], Loss: 34.014941426363315\n",
            "Epoch [7800/20000], Loss: 34.0150801221125\n",
            "Epoch [7900/20000], Loss: 34.01517130223878\n",
            "Epoch [8000/20000], Loss: 34.01501698632498\n",
            "Epoch [8100/20000], Loss: 34.015111417880085\n",
            "Epoch [8200/20000], Loss: 34.015083323540374\n",
            "Epoch [8300/20000], Loss: 34.0150844420006\n",
            "Epoch [8400/20000], Loss: 34.014972421388784\n",
            "Epoch [8500/20000], Loss: 34.01514354013493\n",
            "Epoch [8600/20000], Loss: 34.01500300337149\n",
            "Epoch [8700/20000], Loss: 34.01498729590069\n",
            "Epoch [8800/20000], Loss: 34.015156741436776\n",
            "Epoch [8900/20000], Loss: 34.01494413855904\n",
            "Epoch [9000/20000], Loss: 34.01502733791596\n",
            "Epoch [9100/20000], Loss: 34.01503552472855\n",
            "Epoch [9200/20000], Loss: 34.01529186224102\n",
            "Epoch [9300/20000], Loss: 34.015250518889346\n",
            "Epoch [9400/20000], Loss: 34.01518075067164\n",
            "Epoch [9500/20000], Loss: 34.0152524136821\n",
            "Epoch [9600/20000], Loss: 34.01558249104818\n",
            "Epoch [9700/20000], Loss: 34.015373696273656\n",
            "Epoch [9800/20000], Loss: 34.01511447832676\n",
            "Epoch [9900/20000], Loss: 34.01505824843264\n",
            "Epoch [10000/20000], Loss: 34.0153473426458\n",
            "Epoch [10100/20000], Loss: 34.01515426604824\n",
            "Epoch [10200/20000], Loss: 34.01504666626481\n",
            "Epoch [10300/20000], Loss: 34.01561419922076\n",
            "Epoch [10400/20000], Loss: 34.01539118024091\n",
            "Epoch [10500/20000], Loss: 34.01537769417631\n",
            "Epoch [10600/20000], Loss: 34.01515500241865\n",
            "Epoch [10700/20000], Loss: 34.015358356332754\n",
            "Epoch [10800/20000], Loss: 34.01521935887089\n",
            "Epoch [10900/20000], Loss: 34.01523602874048\n",
            "Epoch [11000/20000], Loss: 34.01553817590561\n",
            "Epoch [11100/20000], Loss: 34.01515442754619\n",
            "Epoch [11200/20000], Loss: 34.01527490251041\n",
            "Epoch [11300/20000], Loss: 34.0149540426892\n",
            "Epoch [11400/20000], Loss: 34.01521068669162\n",
            "Epoch [11500/20000], Loss: 34.01539189199262\n",
            "Epoch [11600/20000], Loss: 34.01510267244728\n",
            "Epoch [11700/20000], Loss: 34.015176655657626\n",
            "Epoch [11800/20000], Loss: 34.01521226083652\n",
            "Epoch [11900/20000], Loss: 34.01557742802152\n",
            "Epoch [12000/20000], Loss: 34.01519400808657\n",
            "Epoch [12100/20000], Loss: 34.01523892709389\n",
            "Epoch [12200/20000], Loss: 34.01504747323586\n",
            "Epoch [12300/20000], Loss: 34.01497562748315\n",
            "Epoch [12400/20000], Loss: 34.014958700234786\n",
            "Epoch [12500/20000], Loss: 34.015119860992016\n",
            "Epoch [12600/20000], Loss: 34.01539879350193\n",
            "Epoch [12700/20000], Loss: 34.01526345166645\n",
            "Epoch [12800/20000], Loss: 34.015364769803284\n",
            "Epoch [12900/20000], Loss: 34.0151896416095\n",
            "Epoch [13000/20000], Loss: 34.015288062762316\n",
            "Epoch [13100/20000], Loss: 34.015237060335274\n",
            "Epoch [13200/20000], Loss: 34.015463288525545\n",
            "Epoch [13300/20000], Loss: 34.015218491319445\n",
            "Epoch [13400/20000], Loss: 34.01505858080579\n",
            "Epoch [13500/20000], Loss: 34.01525988079349\n",
            "Epoch [13600/20000], Loss: 34.01581125866931\n",
            "Epoch [13700/20000], Loss: 34.01522996341916\n",
            "Epoch [13800/20000], Loss: 34.015049531712116\n",
            "Epoch [13900/20000], Loss: 34.01506882199844\n",
            "Epoch [14000/20000], Loss: 34.01519602842202\n",
            "Epoch [14100/20000], Loss: 34.015360748196535\n",
            "Epoch [14200/20000], Loss: 34.01509861485605\n",
            "Epoch [14300/20000], Loss: 34.01522734178765\n",
            "Epoch [14400/20000], Loss: 34.01509968595462\n",
            "Epoch [14500/20000], Loss: 34.01517157086028\n",
            "Epoch [14600/20000], Loss: 34.01499456758115\n",
            "Epoch [14700/20000], Loss: 34.01532691106435\n",
            "Epoch [14800/20000], Loss: 34.015184448564966\n",
            "Epoch [14900/20000], Loss: 34.01544130981831\n",
            "Epoch [15000/20000], Loss: 34.0152090259784\n",
            "Epoch [15100/20000], Loss: 34.015456588461596\n",
            "Epoch [15200/20000], Loss: 34.01522818495429\n",
            "Epoch [15300/20000], Loss: 34.01518639002669\n",
            "Epoch [15400/20000], Loss: 34.015217014414226\n",
            "Epoch [15500/20000], Loss: 34.01516318912613\n",
            "Epoch [15600/20000], Loss: 34.015231243032346\n",
            "Epoch [15700/20000], Loss: 34.01522346682671\n",
            "Epoch [15800/20000], Loss: 34.015243770576085\n",
            "Epoch [15900/20000], Loss: 34.015428087493376\n",
            "Epoch [16000/20000], Loss: 34.01541995954545\n",
            "Epoch [16100/20000], Loss: 34.01520157278957\n",
            "Epoch [16200/20000], Loss: 34.01545634193463\n",
            "Epoch [16300/20000], Loss: 34.01520080889144\n",
            "Epoch [16400/20000], Loss: 34.015118483754534\n",
            "Epoch [16500/20000], Loss: 34.01554678064193\n",
            "Epoch [16600/20000], Loss: 34.01525666202557\n",
            "Epoch [16700/20000], Loss: 34.015351382120194\n",
            "Epoch [16800/20000], Loss: 34.0151142311396\n",
            "Epoch [16900/20000], Loss: 34.01521353638932\n",
            "Epoch [17000/20000], Loss: 34.01511846930033\n",
            "Epoch [17100/20000], Loss: 34.015193953766634\n",
            "Epoch [17200/20000], Loss: 34.015466799944484\n",
            "Epoch [17300/20000], Loss: 34.01507806490849\n",
            "Epoch [17400/20000], Loss: 34.015013812602604\n",
            "Epoch [17500/20000], Loss: 34.01495552889101\n",
            "Epoch [17600/20000], Loss: 34.015199664349886\n",
            "Epoch [17700/20000], Loss: 34.015282775692356\n",
            "Epoch [17800/20000], Loss: 34.01507821402935\n",
            "Epoch [17900/20000], Loss: 34.015194349122\n",
            "Epoch [18000/20000], Loss: 34.01526213056882\n",
            "Epoch [18100/20000], Loss: 34.01536573844307\n",
            "Epoch [18200/20000], Loss: 34.01518114024322\n",
            "Epoch [18300/20000], Loss: 34.015173869843224\n",
            "Epoch [18400/20000], Loss: 34.01508524253464\n",
            "Epoch [18500/20000], Loss: 34.015083741475394\n",
            "Epoch [18600/20000], Loss: 34.01507021279404\n",
            "Epoch [18700/20000], Loss: 34.01525376151764\n",
            "Epoch [18800/20000], Loss: 34.01522371488873\n",
            "Epoch [18900/20000], Loss: 34.01496687359179\n",
            "Epoch [19000/20000], Loss: 34.015198149203165\n",
            "Epoch [19100/20000], Loss: 34.015224772286174\n",
            "Epoch [19200/20000], Loss: 34.01513484001472\n",
            "Epoch [19300/20000], Loss: 34.01559475085997\n",
            "Epoch [19400/20000], Loss: 34.01498418509841\n",
            "Epoch [19500/20000], Loss: 34.015228724254754\n",
            "Epoch [19600/20000], Loss: 34.01512568948516\n",
            "Epoch [19700/20000], Loss: 34.01508048169446\n",
            "Epoch [19800/20000], Loss: 34.01531468603131\n",
            "Epoch [19900/20000], Loss: 34.015098730636936\n",
            "Epoch [20000/20000], Loss: 34.015136574819614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## it looks like in this case that the elastic net performs slightly better than the SCAD, with the sqrtLasso coming in a distance third. However, the difference in the size of the penalty term is where elastic net and scad differ, with the penalty term of SCAD being twice as large as that of Elastic Net, which may lead to SCAD being less likely to be overfit when looking at training and testing data. This is backed up when looking at the results from part one, although more testing is needed\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9PTQG5Q0q-kv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dMSj0PhLscX6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}