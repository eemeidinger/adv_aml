<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AAML Paper</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1 id="generating-deep-fakes-of-paleographic-writing-using-a-dcgan">Generating Deep Fakes of Paleographic Writing Using a DCGAN</h1>
<h2 id="abstract">Abstract</h2>
<p>Paleography, the study of ancient writing systems poses significant challenges for transcription due to it being extremely time intensive to work with. To help combat this issue, historians and other researchers working with paleography have looked to machine learning in hopes of expediting this process. The problem is that machine learning models often require a large amount of data to be properly trained on, and paleographic data sources are often scarce. Generative Adversarial Networks (GANS) can help circumvent this problem by generating synthetic text data. This study leverages a Deep Convolutional GAN (DCGAN) to generate deep fakes of paleographic text. This method uses both user imputed letters and an open source PyTorch dataset in order to augment the dataset provided by the users, hoping to provide more instances of paleographic text that can properly represent that which is seen in the manuscripts the user is working with. There are two parts of the DCGAN, known as the discriminator and the generator. The discriminator works to distinguish real data from synthetic data, and the generator works to fool the discriminator by creating better fake data. This process is performed iteratively until a set number of epochs, after which the generated letters are displayed and evaluated by the user to see if they properly resemble the input images. Overall, this approach showcases the potential of DCGANs in generating synthetic paleographic text, offering a promising avenue to create more data to be used in other machine learning models for transcription.</p>
<h2 id="introduction">Introduction</h2>
<p>Paleography refers to the study of ancient writing systems and the history behind them. Transcription of paleographic text, is a process that can be extremely time consuming, and can be infeasible at points due to a large amount of data and limited manpower. It is for this reason that those in the digital humanities have started to look towards machine learning models as a way to help speed up this process.</p>
<p>Historians are often attempting to look for specific keywords that are important to their research. There are multiple different avenues that digital humanists have looked to accomplish this goal. These range from simple probability search engines to comprehensive optical character recognition models. While these models have varying degrees of success for different tasks, it is typical that the most successful models are expensive in terms of both computational power and data needed. For transcription tasks, it would be ideal to have a comprehensive OCR model that you can tune for either greater precision or greater recall.</p>
<p>One of the long standing problems with using machine learning models for paleographic texts is the lack of sufficient data to train on. This is a problem for a multitude of reasons. If you want a more accurate model, you want multiple instances of very similar writing to include in your training dataset to give you a precise image of what exactly you want your word to be. This is useful for paleographic material written by one author. If you want a model with higher recall, you want a multitude of writing samples from a variety of different authors. Both of these approaches would like a good amount of training data to work with. This is where a generative adversarial network can help.</p>
<p>A generative adversarial network, or GAN, is a machine learning model that consists of two smaller models, which are the generator and the discriminator. These two models are trained simultaneously, and in this case uses binary cross-entropy as the loss function. The generator take in a single input, which is a combination of a randomly generated noise vector and a sample image of what you want to generate the deep fakes of. This input is then passed through a series of layers which transform it into replications of the original dataset. The discriminator then attempts to distinguish this generated data from the original data, and following backpropagation, the process is repeated over a set number of epochs or the model is stopped otherwise.</p>
<p>This paper means to show the process of using this form of machine learning to generate new copies of paleographic text for use in optical character recognition models.</p>
<h2 id="data">Data</h2>
<p><img src="https://github.com/eemeidinger/loopy_copy/blob/main/first_image/b%20image.png?raw=true" alt="Paleographic B"></p>
<p>For this process, we would have two different types of data, one being the paleographic text we wish to replicate, and the other being an open source dataset containing text of the same language. For this project, the paleographic text is that from paleography manual for 1770s-1820s Spanish text. The reasoning for using text from the manual is because it is easy to crop and easy to distinguish from the background text. Many instances of cursive Spanish text is slanted to the right, making it harder to get a good crop. In most of the manuscript material, there is also a wide variety of background noise related to the process that scribes went through when writing. Most of the ink was made with iron, and created a situation where text from the back of the page could bleed into the page being read. This creates issues when reading in these images directly, and therefore requires some preprocessing. The instance we will use is the letter B. While this paper is only going to show the one instance of paleographic text, it is able to be used on any version of the letter chosen. The dataset we will use to help train the model is the EMNIST dataset that can be accessed via PyTorch.[<sup>1</sup>] This dataset also has a wide varieties of handwritten forms for each letter, making it able to generate copies of letters of sufficient variety. This is ideal when working with text with multiple authors, as it is more likely that one of the generated samples will match the one being looked for in the text.</p>
<h2 id="methods">Methods</h2>
<p>Firstly, we have to apply some basic preprocessing to both the EMNIST dataset and the paleographic text. Because the EMNIST dataset is a dataset in the PyTorch library, we import the dataset and apply a set of transformations on it to make it compatible with GAN. We do this by resizing the images to 64 by 64 pixels, setting each image to a tensor, normalizing the pixel intensities, and performing anti-aliasing to smooth out the edges of each letter. Following this, we subset the data to only include the labels from the target class, which is the one we want to produce the deep fakes of. The paleographic text requires a slightly different method of preprocessing, but can still be done mostly using the torch library. To start, otsu thresholding is applied to the image as a basic form of noise removal. Following this, a linear interpolation of the text is applied to get an estimation of the letter if it was to be represented in a 64 by 64 space. Following this, we use a function from the PyTorch library called ‘<strong>to_pil_image</strong>’ to convert the tensor into a basic Python Imaging Library object (Pillow). This allows us to use the same set of transformations done to the EMNIST dataset on our input letter. Following this, we create a randomly generated noise vector to combine with the input letter, and then the model is ready to be run.</p>
<h3 id="model-architecture">Model Architecture</h3>
<p>Figure 1[<sup>2</sup>]<br>
<img src="https://miro.medium.com/v2/resize:fit:1400/1*SZo1GcSEnm4M5FQZZ2r_rA.png" alt="enter image description here"><br>
First the model randomly initializes a set of weights from a normal distribution, in this case having a mean of zero and a standard deviation of .2. After this, both the discriminator and the generator are initialized, with the generator having a hyperparameter of a latent dimension. The generator consists of layers of deconvolutional blocks followed by a final convolutional block at the end. Each deconvolutional block  consists of a transposed convolutional layer, batch normalization, and a set of activation functions. The forward propagation of the generator is where the input tensor is passed through the model to generate an output image. The discriminator has a set of convolutional blocks with a sigmoid function at the end. The forward propagation for the discriminator is where an image is passed through the model to produce a probability score indicating whether the input is real or fake. Following the initialization of these models, we move them to the GPU.</p>
<h3 id="training">Training</h3>
<p>Firstly the discriminator is trained using images from the real dataset. Real images are all labeled with the number one, and fake images are labeled with the number zero. After this the generator then creates the first set of fake images, which are then used to train the discriminator once again. The total loss of the discriminator is then calculated as: <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>l</mi><mi>o</mi><mi>s</mi><msub><mi>s</mi><mi>d</mi></msub><mo>=</mo><mi>l</mi><mi>o</mi><mi>s</mi><msub><mi>s</mi><mi>r</mi></msub><mo>+</mo><mi>l</mi><mi>o</mi><mi>s</mi><msub><mi>s</mi><mi>f</mi></msub></mrow><annotation encoding="application/x-tex">loss_d = loss_r + loss_f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord mathnormal" style="margin-right: 0.01968em;">l</span><span class="mord mathnormal">os</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord mathnormal" style="margin-right: 0.01968em;">l</span><span class="mord mathnormal">os</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.980548em; vertical-align: -0.286108em;"></span><span class="mord mathnormal" style="margin-right: 0.01968em;">l</span><span class="mord mathnormal">os</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span><br>
The gradients are then backpropagated and the discriminator’s parameters are updated. Following this process, the generator is trained to attempt to fool the discriminator.  The generator is zeroed out, and new coefficients are given via backpropagations. The loss for the generator is calculated based on the discriminator’s output for both the real and the fake data. After this the gradients are backpropagated and the generator’s parameters are updated. Because there is no stopping criteria, the process continues to run until the set number of epochs is completed.</p>
<h2 id="results-and-conclusion">Results and Conclusion</h2>
<p><img src="https://github.com/eemeidinger/loopy_copy/blob/main/first_image/Screenshot%20%2891%29.png?raw=true" alt="Grid of Example Bs that were generated from the DCGAN"><br>
Following the training process, the model is able to generate instances of paleographic text that look similar to the input image, but different enough to be considered new versions of each letter. However it is clear that a few instance of the generated letters do not have the second loop found in the capital B, which is caused by an issue with the initial input letter. Because of the flourish of the initial input letter which extends outward to the left, the model tries to create instances which emphasize this letter to account for variability in it’s form, not accounting for the fact that this flourish could be an edge case by itself. While this is not an inherent problem of the model, it is important to understand how the segments of the input can affect the output generated from the DCGAN. Despite this, the model has successfully generated a diverse array of the letter B, which can serve as valuable training data for optical character recognition systems. This underscores the potential of our approach in not only generating synthetic paleographic text but also in facilitating advancements in OCR technology within the digital humanities.</p>
<h2 id="references">References</h2>
<ol>
<li><a href="https://pytorch.org/vision/main/generated/torchvision.datasets.EMNIST.html">EMNIST</a></li>
<li><a href="https://towardsdatascience.com/deep-convolutional-gan-how-to-use-a-dcgan-to-generate-images-in-python-b08afd4d124e">DCGAN Flowchart</a></li>
</ol>
</div>
</body>

</html>
